{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from numpy import *\n",
    "\n",
    "from utils.sampling import mnist_iid_cluster, mnist_noniid_cluster, cifar_iid_cluster,cifar_noniid_cluster\n",
    "from models_v4.Update import LocalTrain,ClusterDetect\n",
    "from models_v4.Nets import CNNMnist, CNNCifar, CNNMnist2\n",
    "from models_v4.Fed import FedAvg_vectorization\n",
    "from models_v4.test import test_acc\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "class my_argument:    \n",
    "    epochs = 10   #\"rounds of training\"\n",
    "    num_users = 5 # \"number of users: K\"\n",
    "    frac = 0.1 #\"the fraction of clients: C\"\n",
    "    local_ep=5 #\"the number of local epochs: E\"\n",
    "    local_bs=50 #\"local batch size: B\"\n",
    "    bs=128 #\"test batch size\"\n",
    "    lr=0.0001 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    \n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=0\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=1#, help=\"number of channels of images\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    cluster=5\n",
    "    opt='ADAM'\n",
    "args = my_argument()\n",
    "#args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "print(args.epochs)\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "6\n",
      "(5000, 1000)\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "encoder1=[]\n",
    "encoder2=[]\n",
    "decoder1=[]\n",
    "decoder2=[]\n",
    "A_bar1=[]\n",
    "A_bar2=[]\n",
    "U1=[]\n",
    "U2=[]\n",
    "mu=0\n",
    "sigma=1\n",
    "#alpha=0.001\n",
    "for i in range(args.cluster):\n",
    "    encoder1.append([])\n",
    "    decoder1.append([])\n",
    "    encoder2.append([])\n",
    "    decoder2.append([])\n",
    "    A_bar1.append([])\n",
    "    A_bar2.append([])\n",
    "    U1.append([])\n",
    "    U2.append([])\n",
    "    decoder1.append([])\n",
    "    decoder2.append([])\n",
    "part=1000; # the size of vector that we want to send each time\n",
    "total_size=round(62006*1) #total length of the vector\n",
    "d1=part\n",
    "slot=math.ceil(total_size/part) # the no. of slots we need to send vectors\n",
    "d2=total_size%part\n",
    "slot1=math.ceil(total_size/d1)-1\n",
    "slot2=total_size%d1\n",
    "print(slot)\n",
    "print(d2)\n",
    "K=args.cluster\n",
    "for i in range(args.cluster):\n",
    "    num1=np.random.normal(mu, sigma, K*d1*d1)\n",
    "    #num1=np.random.rand(K*d1*d1)\n",
    "    num2=np.random.normal(mu, sigma, K*d2*d2)\n",
    "    #num2=np.random.rand(K*d2*d2)\n",
    "    J1=np.random.rand(d1,d1)\n",
    "    for j in range(K):\n",
    "        if (j!=i):\n",
    "            encoder=np.zeros((d1,d1))\n",
    "            J1=np.vstack([J1,encoder])\n",
    "        else:\n",
    "            encoder=np.identity(d1)\n",
    "            J1=np.vstack([J1,encoder])\n",
    "    J1 = np.delete(J1,np.s_[0:d1], axis=0)\n",
    "    encoder1[i]=J1\n",
    "    #encoder2[i]=num2.reshape(K*d2,d2)\n",
    "    #divide the vector into 15 parts of size 10000 each and one part of size 9010\n",
    "#define encoder[0],encoder[1],encoder[2],encoder[3]\n",
    "K=args.cluster\n",
    "print(encoder1[1].shape)\n",
    "#N_R=5#no of receiver antenna\n",
    "for i in range(K): # K= no. of clusters\n",
    "    J1=np.random.rand(K*d1,d1)\n",
    "    #J2=np.random.rand(K*d2,d2)\n",
    "    #print(type(J2))\n",
    "    print(type(encoder1[i]))\n",
    "    for j in range(args.cluster):\n",
    "        if(j!=i):\n",
    "            J1=np.hstack([J1,encoder1[j]]) #concatenating different encoder matrices\n",
    "            #J1.append(encoder1[j])\n",
    "            #J2=np.hstack([J2,encoder2[j]])\n",
    "    print(J1.shape)\n",
    "    J1 = np.delete(J1,np.s_[0:d1], axis=1)# delete the initial d rows in J\n",
    "    print(J1.shape)\n",
    "    #J2 = np.delete(J2,np.s_[0:d2], axis=1)\n",
    "    #print(J2.shape)\n",
    "    A_bar1[i]=J1\n",
    "    #A_bar2[i]=J2\n",
    "    print(A_bar1[i].shape)\n",
    "    #print(A_bar2[i].shape)\n",
    "    print(\"printing type of A_bar\")\n",
    "    print(type(A_bar1[i]))\n",
    "    U1[i]=null_space((A_bar1[i].transpose()))\n",
    "    print(U1[i].shape)\n",
    "    #U2[i]=null_space((A_bar2[i].transpose()))\n",
    "    #print(U2[i].shape)\n",
    "    decoder1[i]=(np.linalg.inv((U1[i].transpose())@(encoder1[i])) @(U1[i].transpose()))\n",
    "    #decoder2[i]=(np.linalg.inv((U2[i].transpose())@(encoder2[i])) @(U2[i].transpose()))\n",
    "    #decoder1[i]=\n",
    "    print(\"printing size of decoder1\")\n",
    "    print(decoder1[0].shape)\n",
    "    print(type(decoder1))\n",
    "    #print(\"printing size of decoder2\")\n",
    "    #print(decoder2[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n",
      "5\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# load dataset and split users\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    count=0\n",
    "    #print(len(dataset_train))\n",
    "    dict_users=[] #2D array in each row, users of a particular cluster\n",
    "    train_data=[]\n",
    "    test_data=[]\n",
    "    for j in range(args.cluster):\n",
    "        train_data.append([])\n",
    "        test_data.append([])\n",
    "        dict_users.append([])\n",
    "    for j in range(len(dataset_train)):\n",
    "        data,label=j\n",
    "        if (label==0) | (label==1):\n",
    "            train_data[0].append(dataset_train[j])\n",
    "        elif (label==2) | (label==3):\n",
    "            train_data[1].append(dataset_train[j])\n",
    "        elif (label==4) | (label==5):\n",
    "            train_data[2].append(dataset_train[j])\n",
    "        elif (label==6) | (label==7):\n",
    "            train_data[3].append(dataset_train[j])\n",
    "        elif (label==8) | (label==9):\n",
    "            train_data[4].append(dataset_train[j])\n",
    "    for j in range(len(dataset_test)):\n",
    "        data,label=j\n",
    "        if (label==0) | (label==1):\n",
    "            test_data[0].append(dataset_test[j])\n",
    "        elif (label==2) | (label==3):\n",
    "            test_data[1].append(dataset_test[j])\n",
    "        elif (label==4) | (label==5):\n",
    "            test_data[2].append(dataset_test[j])\n",
    "        elif (label==6) | (label==7):\n",
    "            test_data[3].append(dataset_test[j])\n",
    "        elif (label==8) | (label==9):\n",
    "            test_data[4].append(dataset_test[j])\n",
    "    \n",
    "#defining 5 different types of datasets for 5 different clusters\n",
    "    \n",
    "    if args.iid:\n",
    "        for cluster_no in range(args.cluster):\n",
    "            dict_users[cluster_no] = mnist_iid_cluster(train_data[cluster_no], args.num_users)\n",
    "    else:\n",
    "        for cluster_no in range(args.cluster):\n",
    "            dict_users[cluster_no] = mnist_noniid_cluster(train_data[cluster_no], args.num_users)\n",
    "elif args.dataset == 'cifar':\n",
    "    trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    dataset_train = datasets.CIFAR10('./data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "    dataset_test = datasets.CIFAR10('./data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "#defining 5 different types of datasets for 5 different clusters\n",
    "    count=0\n",
    "    #print(len(dataset_train))\n",
    "    dict_users=[] #2D array in each row, users of a particular cluster\n",
    "    train_data=[]\n",
    "    test_data=[]\n",
    "    for j in range(args.cluster):\n",
    "        train_data.append([])\n",
    "        test_data.append([])\n",
    "        dict_users.append([])\n",
    "    for j in range(len(dataset_train)):\n",
    "        data,label=dataset_train[j]\n",
    "        if (label==0) | (label==1):\n",
    "            train_data[0].append(dataset_train[j])\n",
    "        elif (label==2) | (label==3):\n",
    "            train_data[1].append(dataset_train[j])\n",
    "        elif (label==4) | (label==5):\n",
    "            train_data[2].append(dataset_train[j])\n",
    "        elif (label==6) | (label==7):\n",
    "            train_data[3].append(dataset_train[j])\n",
    "        elif (label==8) | (label==9):\n",
    "            train_data[4].append(dataset_train[j])\n",
    "    for j in range(len(dataset_test)):\n",
    "        data,label=dataset_test[j]\n",
    "        if (label==0) | (label==1):\n",
    "            test_data[0].append(dataset_test[j])\n",
    "        elif (label==2) | (label==3):\n",
    "            test_data[1].append(dataset_test[j])\n",
    "        elif (label==4) | (label==5):\n",
    "            test_data[2].append(dataset_test[j])\n",
    "        elif (label==6) | (label==7):\n",
    "            test_data[3].append(dataset_test[j])\n",
    "        elif (label==8) | (label==9):\n",
    "            test_data[4].append(dataset_test[j])\n",
    "\n",
    "    if args.iid:\n",
    "        for cluster_no in range(args.cluster):\n",
    "            dict_users[cluster_no] = cifar_iid_cluster(train_data[cluster_no], args.num_users)\n",
    "    else:\n",
    "        for cluster_no in range(args.cluster):\n",
    "            dict_users[cluster_no] = cifar_noniid_cluster(train_data[cluster_no], args.num_users)\n",
    "else:\n",
    "    exit('Error: dataset not found')\n",
    "img_size = dataset_train[0][0].shape\n",
    "#print(dict_users[0])\n",
    "#print((dict_users[0][4]))\n",
    "print(len(dataset_train))\n",
    "print(len(dict_users))\n",
    "print(len(train_data[0]))\n",
    "#print(train_data[0])\n",
    "#idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "#print(idxs_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "#print(use_cuda)\n",
    "args.device = torch.device(\"cuda:3\" if use_cuda else \"cpu\")\n",
    "#args.device=torch.device(\"cpu\")\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[], [], [], [], []], [[], [], [], [], []], [[], [], [], [], []], [[], [], [], [], []], [[], [], [], [], []]]\n"
     ]
    }
   ],
   "source": [
    "acc_test=[]\n",
    "acc_test_arr=[]\n",
    "loss_test=[]\n",
    "loss_test_arr=[]\n",
    "for cluster_no in range(args.cluster):\n",
    "    acc_test.append([])\n",
    "    loss_test.append([])\n",
    "    acc_test_arr.append([])\n",
    "    loss_test_arr.append([])\n",
    "for cluster_no in range(args.cluster):\n",
    "    for i in range(args.cluster):\n",
    "        acc_test_arr[cluster_no].append([])\n",
    "        loss_test_arr[cluster_no].append([])\n",
    "print(acc_test_arr)\n",
    "noise_acc=[]\n",
    "for user in range(25):\n",
    "    noise_acc.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62006\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "from models_v4.Fed import weight_vectorization,FedSubstract,FedAvg_gradient,FedAdd\n",
    "from models_v4.Fed import FedAdd,FedSubstract,weight_vectorization_gen,FedAvg_gradient\n",
    "import numpy as np\n",
    "import copy\n",
    "if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "    net_glob=[]\n",
    "    for i in range(args.cluster):\n",
    "        net_glob.append(CNNCifar(args=args).to(args.device))\n",
    "elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "    net_glob=[]\n",
    "    for i in range(args.cluster):\n",
    "        net_glob.append(CNNMnist2(args=args).to(args.device))\n",
    "\n",
    "else:\n",
    "    exit('Error: model not found')\n",
    "#print(net_glob)\n",
    "w_glob=[]\n",
    "# copy weights\n",
    "for i in range(args.cluster):\n",
    "    net_glob[i].train()\n",
    "    w_glob.append(net_glob[i].state_dict())\n",
    "abs_vect,layer_size=weight_vectorization_gen(w_glob[2])\n",
    "net_glob_in=copy.deepcopy(net_glob)\n",
    "w_glob_in=copy.deepcopy(w_glob)\n",
    "print(len(abs_vect))\n",
    "#print(\"weights\")\n",
    "#print(w_glob)\n",
    "#print(w_glob.shape)\n",
    "#print(w_glob[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_glob_in[0]=torch.load(\"net1_2.pt\")\n",
    "w_glob_in[1]=torch.load(\"net2_2.pt\")\n",
    "w_glob_in[2]=torch.load(\"net3_2.pt\")\n",
    "w_glob_in[3]=torch.load(\"net4_2.pt\")\n",
    "w_glob_in[4]=torch.load(\"net5_2.pt\")\n",
    "net_glob_in[0].load_state_dict(w_glob_in[0])\n",
    "net_glob_in[1].load_state_dict(w_glob_in[1])\n",
    "net_glob_in[2].load_state_dict(w_glob_in[2])\n",
    "net_glob_in[3].load_state_dict(w_glob_in[3])\n",
    "net_glob_in[4].load_state_dict(w_glob_in[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4, 14]\n",
      "[10, 11, 12, 13]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "tensor(50.0500)\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "iteration number 1\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(50.)\n",
      "tensor(70.7000)\n",
      "tensor(52.0500)\n",
      "tensor(50.)\n",
      "tensor(50.1500)\n",
      "iteration number 2\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(50.)\n",
      "tensor(71.7000)\n",
      "tensor(70.1000)\n",
      "tensor(54.5500)\n",
      "tensor(68.)\n",
      "iteration number 3\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(53.0500)\n",
      "tensor(73.7500)\n",
      "tensor(77.7500)\n",
      "tensor(64.)\n",
      "tensor(68.7500)\n",
      "iteration number 4\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(53.)\n",
      "tensor(75.4000)\n",
      "tensor(77.6500)\n",
      "tensor(65.0500)\n",
      "tensor(68.6000)\n",
      "iteration number 5\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(56.3500)\n",
      "tensor(76.1500)\n",
      "tensor(77.9000)\n",
      "tensor(65.5000)\n",
      "tensor(70.1500)\n",
      "iteration number 6\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(58.6000)\n",
      "tensor(77.2000)\n",
      "tensor(78.5500)\n",
      "tensor(66.5500)\n",
      "tensor(69.6500)\n",
      "iteration number 7\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(59.8500)\n",
      "tensor(77.7000)\n",
      "tensor(78.8500)\n",
      "tensor(67.)\n",
      "tensor(71.)\n",
      "iteration number 8\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(58.)\n",
      "tensor(78.2000)\n",
      "tensor(79.6500)\n",
      "tensor(66.8500)\n",
      "tensor(72.0500)\n",
      "iteration number 9\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(59.0500)\n",
      "tensor(78.7000)\n",
      "tensor(81.0500)\n",
      "tensor(68.4000)\n",
      "tensor(73.1000)\n",
      "iteration number 10\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(58.5000)\n",
      "tensor(79.6000)\n",
      "tensor(81.3000)\n",
      "tensor(67.9000)\n",
      "tensor(72.8000)\n",
      "iteration number 11\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(61.3500)\n",
      "tensor(80.3000)\n",
      "tensor(81.9000)\n",
      "tensor(68.3000)\n",
      "tensor(73.6500)\n",
      "iteration number 12\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(60.7500)\n",
      "tensor(80.6500)\n",
      "tensor(82.2500)\n",
      "tensor(68.9500)\n",
      "tensor(74.)\n",
      "iteration number 13\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(62.2500)\n",
      "tensor(81.)\n",
      "tensor(82.8500)\n",
      "tensor(69.9500)\n",
      "tensor(74.6500)\n",
      "iteration number 14\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(61.7500)\n",
      "tensor(81.8500)\n",
      "tensor(82.8000)\n",
      "tensor(70.3500)\n",
      "tensor(75.2500)\n",
      "iteration number 15\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(63.8500)\n",
      "tensor(82.1500)\n",
      "tensor(83.5000)\n",
      "tensor(71.4000)\n",
      "tensor(75.4500)\n",
      "iteration number 16\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(64.0500)\n",
      "tensor(82.9000)\n",
      "tensor(83.)\n",
      "tensor(71.7000)\n",
      "tensor(75.5500)\n",
      "iteration number 17\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(66.2000)\n",
      "tensor(82.9500)\n",
      "tensor(83.9500)\n",
      "tensor(72.0500)\n",
      "tensor(75.6000)\n",
      "iteration number 18\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(65.8000)\n",
      "tensor(84.0500)\n",
      "tensor(84.1000)\n",
      "tensor(72.5500)\n",
      "tensor(75.9500)\n",
      "iteration number 19\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(68.1000)\n",
      "tensor(84.1500)\n",
      "tensor(83.5500)\n",
      "tensor(73.1500)\n",
      "tensor(76.)\n",
      "iteration number 20\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(69.2500)\n",
      "tensor(84.6000)\n",
      "tensor(84.)\n",
      "tensor(73.3000)\n",
      "tensor(76.7000)\n",
      "iteration number 21\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(70.9000)\n",
      "tensor(84.5000)\n",
      "tensor(84.7500)\n",
      "tensor(74.8000)\n",
      "tensor(77.0500)\n",
      "iteration number 22\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(70.0500)\n",
      "tensor(84.8500)\n",
      "tensor(84.4000)\n",
      "tensor(74.9500)\n",
      "tensor(77.0500)\n",
      "iteration number 23\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(71.9000)\n",
      "tensor(85.6500)\n",
      "tensor(85.2000)\n",
      "tensor(75.2000)\n",
      "tensor(78.)\n",
      "iteration number 24\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(72.1000)\n",
      "tensor(85.8500)\n",
      "tensor(84.4500)\n",
      "tensor(75.5500)\n",
      "tensor(78.1000)\n",
      "iteration number 25\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(73.1500)\n",
      "tensor(85.8000)\n",
      "tensor(84.5500)\n",
      "tensor(77.0500)\n",
      "tensor(78.3500)\n",
      "iteration number 26\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(72.5000)\n",
      "tensor(85.9500)\n",
      "tensor(85.2500)\n",
      "tensor(76.2000)\n",
      "tensor(78.4000)\n",
      "iteration number 27\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(73.5000)\n",
      "tensor(86.2000)\n",
      "tensor(85.7000)\n",
      "tensor(77.1000)\n",
      "tensor(79.1500)\n",
      "iteration number 28\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(73.9000)\n",
      "tensor(86.2000)\n",
      "tensor(85.3500)\n",
      "tensor(77.0500)\n",
      "tensor(78.5500)\n",
      "iteration number 29\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(72.8000)\n",
      "tensor(86.3000)\n",
      "tensor(85.5500)\n",
      "tensor(77.4500)\n",
      "tensor(79.2500)\n",
      "iteration number 30\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(73.7000)\n",
      "tensor(86.8000)\n",
      "tensor(85.2500)\n",
      "tensor(77.8000)\n",
      "tensor(79.6000)\n",
      "iteration number 31\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.6000)\n",
      "tensor(87.)\n",
      "tensor(85.6500)\n",
      "tensor(78.3500)\n",
      "tensor(80.1500)\n",
      "iteration number 32\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.8000)\n",
      "tensor(86.8500)\n",
      "tensor(85.5000)\n",
      "tensor(77.4500)\n",
      "tensor(80.0500)\n",
      "iteration number 33\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.7500)\n",
      "tensor(87.0500)\n",
      "tensor(85.5500)\n",
      "tensor(78.2000)\n",
      "tensor(80.1000)\n",
      "iteration number 34\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(73.7000)\n",
      "tensor(87.0500)\n",
      "tensor(85.9000)\n",
      "tensor(78.1500)\n",
      "tensor(80.8500)\n",
      "iteration number 35\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.1500)\n",
      "tensor(87.4000)\n",
      "tensor(85.6000)\n",
      "tensor(79.2500)\n",
      "tensor(81.4000)\n",
      "iteration number 36\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.4500)\n",
      "tensor(86.8000)\n",
      "tensor(86.4000)\n",
      "tensor(77.7000)\n",
      "tensor(80.9500)\n",
      "iteration number 37\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.5500)\n",
      "tensor(87.3500)\n",
      "tensor(85.6000)\n",
      "tensor(78.4000)\n",
      "tensor(80.4500)\n",
      "iteration number 38\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.8500)\n",
      "tensor(87.7000)\n",
      "tensor(86.2500)\n",
      "tensor(79.0500)\n",
      "tensor(82.2000)\n",
      "iteration number 39\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.9500)\n",
      "tensor(87.6000)\n",
      "tensor(86.3500)\n",
      "tensor(79.0500)\n",
      "tensor(82.)\n",
      "iteration number 40\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.6000)\n",
      "tensor(87.7500)\n",
      "tensor(86.8000)\n",
      "tensor(77.9000)\n",
      "tensor(81.5500)\n",
      "iteration number 41\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.3500)\n",
      "tensor(87.2000)\n",
      "tensor(86.3500)\n",
      "tensor(78.5500)\n",
      "tensor(81.3000)\n",
      "iteration number 42\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.6500)\n",
      "tensor(87.8500)\n",
      "tensor(86.6000)\n",
      "tensor(78.9000)\n",
      "tensor(82.7000)\n",
      "iteration number 43\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(73.2500)\n",
      "tensor(87.9500)\n",
      "tensor(86.4000)\n",
      "tensor(79.3000)\n",
      "tensor(82.5500)\n",
      "iteration number 44\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.1500)\n",
      "tensor(87.9500)\n",
      "tensor(86.5500)\n",
      "tensor(77.8500)\n",
      "tensor(82.1500)\n",
      "iteration number 45\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.5000)\n",
      "tensor(87.9000)\n",
      "tensor(86.6500)\n",
      "tensor(79.1000)\n",
      "tensor(82.6500)\n",
      "iteration number 46\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.9500)\n",
      "tensor(87.5500)\n",
      "tensor(86.6000)\n",
      "tensor(79.1500)\n",
      "tensor(83.2000)\n",
      "iteration number 47\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.7000)\n",
      "tensor(88.)\n",
      "tensor(86.7000)\n",
      "tensor(79.1500)\n",
      "tensor(82.9500)\n",
      "iteration number 48\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.1500)\n",
      "tensor(88.)\n",
      "tensor(86.7500)\n",
      "tensor(79.6000)\n",
      "tensor(82.8500)\n",
      "iteration number 49\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.5500)\n",
      "tensor(87.9000)\n",
      "tensor(86.8500)\n",
      "tensor(79.0500)\n",
      "tensor(83.2500)\n",
      "iteration number 50\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.0500)\n",
      "tensor(88.2500)\n",
      "tensor(86.7000)\n",
      "tensor(79.5500)\n",
      "tensor(83.6000)\n",
      "iteration number 51\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.5000)\n",
      "tensor(88.3000)\n",
      "tensor(87.0500)\n",
      "tensor(79.7000)\n",
      "tensor(83.3500)\n",
      "iteration number 52\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.5500)\n",
      "tensor(87.7500)\n",
      "tensor(87.4000)\n",
      "tensor(79.5500)\n",
      "tensor(83.8500)\n",
      "iteration number 53\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.9000)\n",
      "tensor(87.8500)\n",
      "tensor(86.9000)\n",
      "tensor(79.3500)\n",
      "tensor(83.8500)\n",
      "iteration number 54\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.3500)\n",
      "tensor(88.3000)\n",
      "tensor(87.2000)\n",
      "tensor(79.8000)\n",
      "tensor(84.)\n",
      "iteration number 55\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.6000)\n",
      "tensor(88.2500)\n",
      "tensor(87.5000)\n",
      "tensor(80.1000)\n",
      "tensor(83.9000)\n",
      "iteration number 56\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.7500)\n",
      "tensor(88.2000)\n",
      "tensor(87.4500)\n",
      "tensor(79.6000)\n",
      "tensor(83.7500)\n",
      "iteration number 57\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.3000)\n",
      "tensor(88.1500)\n",
      "tensor(87.4000)\n",
      "tensor(79.6500)\n",
      "tensor(84.5500)\n",
      "iteration number 58\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.1500)\n",
      "tensor(88.7000)\n",
      "tensor(87.5500)\n",
      "tensor(80.0500)\n",
      "tensor(84.3500)\n",
      "iteration number 59\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.7500)\n",
      "tensor(88.7000)\n",
      "tensor(87.8500)\n",
      "tensor(80.0500)\n",
      "tensor(84.)\n",
      "iteration number 60\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.6000)\n",
      "tensor(88.3500)\n",
      "tensor(87.8500)\n",
      "tensor(79.1000)\n",
      "tensor(84.4000)\n",
      "iteration number 61\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.3500)\n",
      "tensor(88.3000)\n",
      "tensor(87.2000)\n",
      "tensor(80.0500)\n",
      "tensor(84.6000)\n",
      "iteration number 62\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.9500)\n",
      "tensor(89.0500)\n",
      "tensor(87.7500)\n",
      "tensor(80.1000)\n",
      "tensor(84.5000)\n",
      "iteration number 63\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(74.4000)\n",
      "tensor(88.9500)\n",
      "tensor(88.0500)\n",
      "tensor(80.2500)\n",
      "tensor(84.2000)\n",
      "iteration number 64\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.2000)\n",
      "tensor(88.8000)\n",
      "tensor(88.0500)\n",
      "tensor(79.7000)\n",
      "tensor(84.2000)\n",
      "iteration number 65\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.1000)\n",
      "tensor(88.4500)\n",
      "tensor(87.6000)\n",
      "tensor(79.8000)\n",
      "tensor(84.8500)\n",
      "iteration number 66\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.3500)\n",
      "tensor(88.9500)\n",
      "tensor(87.9000)\n",
      "tensor(80.5000)\n",
      "tensor(85.1000)\n",
      "iteration number 67\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.1500)\n",
      "tensor(89.0500)\n",
      "tensor(88.4000)\n",
      "tensor(80.6000)\n",
      "tensor(84.8500)\n",
      "iteration number 68\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.8500)\n",
      "tensor(88.8000)\n",
      "tensor(88.3000)\n",
      "tensor(80.1000)\n",
      "tensor(85.)\n",
      "iteration number 69\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.3500)\n",
      "tensor(88.6000)\n",
      "tensor(87.9500)\n",
      "tensor(80.2000)\n",
      "tensor(85.2000)\n",
      "iteration number 70\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.7000)\n",
      "tensor(89.2000)\n",
      "tensor(88.2000)\n",
      "tensor(80.4000)\n",
      "tensor(85.5000)\n",
      "iteration number 71\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.)\n",
      "tensor(89.3500)\n",
      "tensor(88.7500)\n",
      "tensor(80.5000)\n",
      "tensor(85.1000)\n",
      "iteration number 72\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(76.0500)\n",
      "tensor(89.2000)\n",
      "tensor(88.5000)\n",
      "tensor(80.3500)\n",
      "tensor(85.2000)\n",
      "iteration number 73\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.9000)\n",
      "tensor(89.1500)\n",
      "tensor(88.4000)\n",
      "tensor(80.3500)\n",
      "tensor(85.3000)\n",
      "iteration number 74\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.3000)\n",
      "tensor(89.7500)\n",
      "tensor(88.3000)\n",
      "tensor(80.9500)\n",
      "tensor(85.6000)\n",
      "iteration number 75\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.)\n",
      "tensor(89.3500)\n",
      "tensor(88.9500)\n",
      "tensor(80.8000)\n",
      "tensor(85.2500)\n",
      "iteration number 76\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(76.2000)\n",
      "tensor(89.1000)\n",
      "tensor(88.6000)\n",
      "tensor(80.4000)\n",
      "tensor(85.7000)\n",
      "iteration number 77\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.5500)\n",
      "tensor(89.5000)\n",
      "tensor(88.2500)\n",
      "tensor(80.3500)\n",
      "tensor(85.8500)\n",
      "iteration number 78\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.5500)\n",
      "tensor(89.7500)\n",
      "tensor(88.5500)\n",
      "tensor(80.8500)\n",
      "tensor(86.5500)\n",
      "iteration number 79\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.9000)\n",
      "tensor(89.3000)\n",
      "tensor(88.9500)\n",
      "tensor(80.9500)\n",
      "tensor(85.8000)\n",
      "iteration number 80\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(76.2500)\n",
      "tensor(89.2000)\n",
      "tensor(88.9000)\n",
      "tensor(80.9500)\n",
      "tensor(86.)\n",
      "iteration number 81\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.9000)\n",
      "tensor(89.6000)\n",
      "tensor(88.6000)\n",
      "tensor(80.4000)\n",
      "tensor(86.3000)\n",
      "iteration number 82\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.9000)\n",
      "tensor(89.9000)\n",
      "tensor(88.7500)\n",
      "tensor(81.2500)\n",
      "tensor(86.2500)\n",
      "iteration number 83\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.3500)\n",
      "tensor(89.8000)\n",
      "tensor(89.)\n",
      "tensor(81.2500)\n",
      "tensor(86.)\n",
      "iteration number 84\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.9500)\n",
      "tensor(89.6000)\n",
      "tensor(89.4000)\n",
      "tensor(80.9500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(86.4500)\n",
      "iteration number 85\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(76.3000)\n",
      "tensor(89.8000)\n",
      "tensor(88.8000)\n",
      "tensor(80.6000)\n",
      "tensor(86.4000)\n",
      "iteration number 86\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(76.5000)\n",
      "tensor(90.2000)\n",
      "tensor(88.6500)\n",
      "tensor(81.0500)\n",
      "tensor(86.9000)\n",
      "iteration number 87\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.4000)\n",
      "tensor(90.2000)\n",
      "tensor(89.1000)\n",
      "tensor(81.2500)\n",
      "tensor(86.4500)\n",
      "iteration number 88\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(76.4500)\n",
      "tensor(89.9500)\n",
      "tensor(89.5000)\n",
      "tensor(80.6500)\n",
      "tensor(86.8500)\n",
      "iteration number 89\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "tensor(75.8500)\n",
      "tensor(89.8500)\n",
      "tensor(88.8000)\n",
      "tensor(80.5500)\n",
      "tensor(86.5000)\n",
      "iteration number 90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-18ba809bfd12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m#local2 = ClusterDetect(args=args, dataset= sorted_train_data[user], idxs=idx_users[user][0:599])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# using 1st 600 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_glob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mcluster_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m#print(\"printing user\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gulerlab/Documents/Hasin/Github/Over-the-Air-Clustered-FL-main/models_v4/Update.py\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(self, net)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldr_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# build model\n",
    "\n",
    "# torch.save(net_glob_in[0].state_dict(),  \"Model_cluster0.pt\")\n",
    "# torch.save(net_glob_in[1].state_dict(),  \"Model_cluster1.pt\")\n",
    "# torch.save(net_glob_in[2].state_dict(),  \"Model_cluster2.pt\")\n",
    "# torch.save(net_glob_in[3].state_dict(),  \"Model_cluster3.pt\")\n",
    "# torch.save(net_glob_in[4].state_dict(),  \"Model_cluster4.pt\")\n",
    "d=62006\n",
    "args.lr=0.0002\n",
    "from models_v4.Fed import weight_vectorization_cifar,FedAdd,FedSubstract,FedAvg_gradient\n",
    "import numpy as np\n",
    "import copy\n",
    "#slot=math.ceil(d/d1) # the no. of slots we need to send vectors\n",
    "\n",
    "# training\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "num=[]\n",
    "import random\n",
    "part=1000\n",
    "b=part\n",
    "D=part #dimension of noise vector = Nr= Kd\n",
    "mu=0\n",
    "sigma=1\n",
    "K=args.cluster\n",
    "K_global=round(1*d)\n",
    "error=[]\n",
    "w_glob=copy.deepcopy(w_glob_in)\n",
    "net_glob=copy.deepcopy(net_glob_in)\n",
    "#hist_ = np.zeros(10,dtype=int)\n",
    "sample=0 # fro the purpose of using fresh samples in each iteration\n",
    "n=K*b*K*b\n",
    "H = np.random.normal(loc=0, scale=np.sqrt(2)/2, size=(n, 2)).view(np.complex128)\n",
    "H=real(H.reshape(K*b,K*b))\n",
    "for iter in range(1000): #args.epochs\n",
    "    print(\"iteration number\",iter)\n",
    "    if(iter%4==0):\n",
    "        sample=0\n",
    "    encoded=[]\n",
    "    decoded=[]\n",
    "    num=[]\n",
    "    noise=[] #noise vector\n",
    "  \n",
    "    num=np.random.normal(mu, sigma, K*D)\n",
    "    num=np.transpose(num.reshape(1,len(num)))\n",
    "    #m = 10\n",
    "    loss_train=[]\n",
    "    cluster_block=[]\n",
    "    noise_dec=[] #decoded noise vector\n",
    "    recovered=[]\n",
    "    for cluster_no in range(args.cluster):\n",
    "        loss_train.append([])\n",
    "        cluster_block.append([])\n",
    "        decoded.append([])\n",
    "        recovered.append([])\n",
    "        noise_dec.append([])\n",
    "    Power=[]\n",
    "    Power2=[]\n",
    "    alpha=[]\n",
    "    feature_vector2=[]\n",
    "    for i in range(args.cluster):\n",
    "        Power.append([])\n",
    "        Power2.append([])\n",
    "        alpha.append([])\n",
    "        feature_vector2.append(np.zeros((d,1)))\n",
    "        \n",
    "    #m_global=np.zeros((d,1))\n",
    "    #location_global=np.random.choice(range(d),K_global,replace=False)\n",
    "    #for i in location_global:\n",
    "        #m_global[i]=1\n",
    "                \n",
    "    #idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    idx_users=[]\n",
    "    sorted_train_data=[]\n",
    "    sorted_test_data=[]\n",
    "    for cluster_no in range(args.cluster):\n",
    "        for index in dict_users[cluster_no]:\n",
    "            idx_users.append(index) # putting the data indices of users in this list\n",
    "            sorted_train_data.append(train_data[cluster_no])#putting the corresponding training data in this array\n",
    "            sorted_test_data.append(test_data[cluster_no])\n",
    "    for user in range(len(idx_users)): # no. of loop= no. of users\n",
    "        cluster_loss=[]\n",
    "        for i in range(args.cluster):\n",
    "             # each type of dataset belong to 5 users\n",
    "            local2 = ClusterDetect(args=args, dataset= sorted_train_data[user], idxs=idx_users[user][(sample*250):(sample+1)*250])\n",
    "            #local2 = ClusterDetect(args=args, dataset= sorted_train_data[user], idxs=idx_users[user][0:599])\n",
    "            # using 1st 600 data\n",
    "            w2, loss2 = local2.train2(net=copy.deepcopy(net_glob[i]).to(args.device))\n",
    "            cluster_loss.append(loss2)\n",
    "        #print(\"printing user\")\n",
    "        #print(user)\n",
    "        #print(cluster_loss)\n",
    "        minimum=min(cluster_loss)\n",
    "        index_of_min=cluster_loss.index(minimum)\n",
    "        cluster_block[index_of_min].append(user)\n",
    "    print(cluster_block[0])\n",
    "    print(cluster_block[1])\n",
    "    print(cluster_block[2])\n",
    "    print(cluster_block[3])\n",
    "    print(cluster_block[4])\n",
    "    #K_global=part\n",
    "    #m_global=np.zeros((d,1))\n",
    "#     location_global=np.random.choice(range(d),K_global,replace=False)\n",
    "#     for i in location_global:\n",
    "#         m_global[i]=1\n",
    "    updated=[]\n",
    "    model_diff=[]\n",
    "    grad_vect=[]\n",
    "    prev=[]\n",
    "    error=[]\n",
    "    grad_vect_send=[]\n",
    "    store_grad=[]\n",
    "    sparse_vector=[]\n",
    "    for i in range(25):\n",
    "        updated.append([])\n",
    "        model_diff.append([])\n",
    "        grad_vect.append([])\n",
    "        prev.append([])\n",
    "        #error.append(np.zeros((d,1)))\n",
    "        grad_vect_send.append([])\n",
    "        store_grad.append([])\n",
    "        sparse_vector.append([])\n",
    "    b=part  \n",
    "    sigma_R=1/math.sqrt(b)\n",
    "    mu_R=0\n",
    "    #R=np.random.normal(mu_R, sigma_R, (b,total_size))\n",
    "    superposition=0\n",
    "    #location_global=np.random.choice(range(d),K_global,replace=False) # storing top- K_local locations for each user\n",
    "    #location_global=np.sort(location_global)\n",
    "#     for i in location_global:\n",
    "#         m_global[i]=1\n",
    "    cluster_gradient=[]\n",
    "    for cluster_no in range(args.cluster):\n",
    "        w_locals, loss_locals,grad_locals,diff_locals= [],[],[],[]\n",
    "        cluster_gradient.append([])\n",
    "        if(cluster_block[cluster_no]==[]):\n",
    "            continue\n",
    "        norm=[]\n",
    "        for user2 in cluster_block[cluster_no]:\n",
    "            trans=np.array([])\n",
    "            #local = LocalUpdate(args=args, dataset=sorted_train_data[user], idxs=idx_users[user][(sample+1)*600:(sample+2)*600])\n",
    "            #local = LocalUpdate(args=args, dataset=sorted_train_data[user], idxs=idx_users[user][600:1199])\n",
    "            #print(user2)\n",
    "            total_P=0\n",
    "            updated[user2]=copy.deepcopy(w_glob[cluster_no])\n",
    "            local = LocalTrain(args=args, dataset=sorted_train_data[user2], idxs=idx_users[user2][(sample+1)*250:(sample+2)*250])\n",
    "            #using 2nd half data\n",
    "            w, loss = local.train(net=copy.deepcopy(net_glob[cluster_no]).to(args.device))\n",
    "            w_locals.append(copy.deepcopy(w))\n",
    "            loss_locals.append(copy.deepcopy(loss))\n",
    "            prev[user2]=updated[user2]\n",
    "            model_diff[user2]=FedSubstract(w,prev[user2])\n",
    "            grad_vect[user2],layer_size=weight_vectorization_gen(model_diff[user2])\n",
    "            #grad_vect[user2]=grad_vect[user2]+error[user2]\n",
    "            #for i in location_global:\n",
    "                #sparse_vector[user2].append(grad_vect[user2][i])\n",
    "            # vectorizing the gradient\n",
    "            #grad_vect[user2]=grad_vect[user2]+error[user2] # error feedback\n",
    "            #array_one=np.ones((d,1))\n",
    "            model_vector,layer_size=weight_vectorization_gen(w)\n",
    "#             M=max(abs(model_vector))\n",
    "#             mask=m_global\n",
    "#             grad_vect_send[user2]=np.multiply(mask,grad_vect[user2])\n",
    "            #error[user2]=grad_vect[user2]-grad_vect_send[user2]\n",
    "           \n",
    "            count=0\n",
    "            grad_vect_send[user2]=grad_vect[user2]\n",
    "            #norm.append(np.linalg.norm(grad_vect_send[user2],))\n",
    "            #error[user2]=grad_vect[user2]-grad_vect_send[user2]\n",
    "            grad_locals.append(grad_vect_send[user2])\n",
    "            #store_grad[user2]=grad_vect_send[user2]\n",
    "            diff_locals.append(copy.deepcopy(model_diff[user2]))\n",
    "#             n=K*b*K*b\n",
    "#             H = np.random.normal(loc=0, scale=np.sqrt(2)/2, size=(n, 2)).view(np.complex128)\n",
    "#             H=real(H.reshape(K*b,K*b))\n",
    "            #U, D, VT = np.linalg.svd(H)\n",
    "            \n",
    "            P_T=10000\n",
    "\n",
    "            P_T=100000\n",
    "            #cluster_gradient[cluster_no].append(H@(E1@grad_vect_send[user2]))\n",
    "            #cluster_gradient[cluster_no].append(np.real(H@E1)@grad_vect_send[user2])\n",
    "            \n",
    "            #SNR=math.log10(M**2)\n",
    "            \n",
    "            Power[cluster_no].append(total_P)\n",
    "        \n",
    "        #Power2[cluster_no]=(sum(Power[cluster_no]))/len(cluster_block[cluster_no])\n",
    "        P_limit=2000000000\n",
    "        #alpha[cluster_no]=math.sqrt(P_limit/Power2[cluster_no])\n",
    "        alpha[cluster_no]=100*100\n",
    "        c=100*100\n",
    "        grad_avg=FedAvg_gradient(grad_locals)\n",
    "        #print(grad_avg)\n",
    "        decoded[cluster_no]=grad_avg\n",
    "        #enc=H@(E1@grad_avg)\n",
    "        \n",
    "        j=w_locals[0]\n",
    "        feature_vector=grad_avg\n",
    "        #print(len(feature_vector))\n",
    "        #print(feature_vector)\n",
    "        #fecture vector is dx1 vector after flattening all types of weight matrices and concatenaitng them\n",
    "        increment=0 #keeps track of feature_vactor index\n",
    "        #superposition=superposition+alpha[cluster_no]*encoder1[cluster_no]@feature_vector+num\n",
    "    #received=superposition\n",
    "    #print(len(superposition))\n",
    "    #superposition=superposition +(1/c)*num*(1/min_norm)\n",
    "    #print(\"total received Power\")\n",
    "    #print(sum((received)**2))\n",
    "    #print(\"received min max\")\n",
    "    #print(max(abs(received)))\n",
    "    #print(min(abs(received)))\n",
    "    #m_global=np.zeros((d,1))\n",
    "    recovered=[] \n",
    "    backup=[]\n",
    "    del model_diff\n",
    "    del grad_vect\n",
    "    del grad_vect_send\n",
    "    del prev\n",
    "    del updated\n",
    "    for cluster_no in range(args.cluster):\n",
    "#         recovered.append(np.zeros((62006,1)))\n",
    "#         backup.append(np.zeros((62006,1)))\n",
    "        #print(w_glob[cluster_no])\n",
    "        if(cluster_block[cluster_no]==[]):\n",
    "            continue\n",
    "        flat=[]\n",
    "        \n",
    "        count=0\n",
    "        for i in range(16): # 4 layers in parameter\n",
    "            flat.append([])\n",
    "        increment=0\n",
    "        count=0\n",
    "        #print(decoded[cluster_no])\n",
    "        #back=decoded[cluster_no]\n",
    "        #recovered=np.zeros((d,1))\n",
    "        index=0\n",
    "        #print(decoded[cluster_no])\n",
    "        w_glob_prev=copy.deepcopy(w_glob[cluster_no])\n",
    "        for i in range(len(w_glob[cluster_no].keys())): # 4 layers in parameter\n",
    "            flat.append([])\n",
    "\n",
    "        for h in w_glob_prev.keys():\n",
    "            s=list(w_glob[cluster_no][h].shape)\n",
    "            if (len(s)==0):\n",
    "                new=np.array(0)\n",
    "                decoded[cluster_no]=np.delete(decoded[cluster_no],np.s_[0])\n",
    "            else:\n",
    "                z=np.prod(list(w_glob[cluster_no][h].shape))\n",
    "                flat[count]=decoded[cluster_no][0:z] # taking out the vector for the specified layer\n",
    "                decoded[cluster_no]=np.delete(decoded[cluster_no],np.s_[0:z])# deleting that vector from decoded after taking out\n",
    "             \n",
    "                new=flat[count].reshape(list(w_glob[cluster_no][h].shape)) #reshaping back to the marix\n",
    "              \n",
    "            w_glob[cluster_no][h]=torch.from_numpy(new) #converting the matrix to a tensor\n",
    "            #print(w_glob[cluster_no][h].shape)\n",
    "            count=count+1\n",
    "    # update global weights\n",
    "        \n",
    "        global_diff = w_glob[cluster_no]\n",
    "        w_glob[cluster_no]=FedAdd(w_glob_prev,global_diff)\n",
    "        net_glob[cluster_no].load_state_dict(w_glob[cluster_no])\n",
    "        #printing loss in each iteration\n",
    "        for i in range(args.cluster):\n",
    "            acc_test[i], loss_test[i] = test_acc(net_glob[cluster_no], test_data[i], args)\n",
    "            acc_test_arr[i][cluster_no].append(acc_test[i])\n",
    "            loss_test_arr[i][cluster_no].append(loss_test[i])\n",
    "        for user in cluster_block[cluster_no]:\n",
    "            acc,loss= test_acc(net_glob[cluster_no], sorted_test_data[user], args)\n",
    "            noise_acc[user].append(acc)\n",
    "            if(user==0)|(user==5)|(user==10)|(user==15)|(user==20):\n",
    "                print(acc)\n",
    "        #print(acc_test[cluster_no])\n",
    "        #print(loss_test[cluster_no])\n",
    "        #if iter % 1 ==0:\n",
    "            #print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg[cluster_no],acc_test[cluster_no]))\n",
    "        #print(hist_)\n",
    "        #print(\"users in cluster\",cluster_no)\n",
    "        #print(cluster_block[cluster_no])\n",
    "        #print(\"Test accuracy of cluster\",cluster_no)\n",
    "        #print(acc_test[cluster_no])\n",
    "    #print(loss_train)\n",
    "    sample=sample+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(num))\n",
    "print(alpha[cluster_no])\n",
    "print(num*(1/min_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[3, 9]\n",
    "print(np.linalg.norm(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(encoder1[cluster_no][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(np.array(loss_locals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_gradient[cluster_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_acc=[]\n",
    "for i in range(25):\n",
    "    user_acc.append([])\n",
    "for i in range(25):\n",
    "    for j in noise_acc[i]:\n",
    "        user_acc[i].append(float(j))\n",
    "print(user_acc[19][60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Avg_acc_cluster0=[]\n",
    "Avg_acc_cluster1=[]\n",
    "Avg_acc_cluster2=[]\n",
    "Avg_acc_cluster3=[]\n",
    "Avg_acc_cluster4=[]\n",
    "for i in range(1000):\n",
    "    Avg_acc_cluster0.append((user_acc[0][i]+user_acc[1][i]+user_acc[2][i]+user_acc[3][i]+user_acc[4][i])/5)\n",
    "    Avg_acc_cluster1.append((user_acc[5][i]+user_acc[6][i]+user_acc[7][i]+user_acc[8][i]+user_acc[9][i])/5)\n",
    "    Avg_acc_cluster2.append((user_acc[10][i]+user_acc[11][i]+user_acc[12][i]+user_acc[13][i]+user_acc[14][i])/5)\n",
    "    Avg_acc_cluster3.append((user_acc[15][i]+user_acc[16][i]+user_acc[17][i]+user_acc[18][i]+user_acc[19][i])/5)\n",
    "    Avg_acc_cluster4.append((user_acc[20][i]+user_acc[21][i]+user_acc[22][i]+user_acc[23][i]+user_acc[24][i])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(Avg_acc_cluster0)), Avg_acc_cluster0)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 0 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()\n",
    "# plt.savefig('./save/fed_{}_{}_{}_C{}_iid{}.png'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))\n",
    "plt.plot(range(len(Avg_acc_cluster1)), Avg_acc_cluster1)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 1 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()\n",
    "plt.plot(range(len(Avg_acc_cluster2)), Avg_acc_cluster2)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 2 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()\n",
    "plt.plot(range(len(Avg_acc_cluster3)), Avg_acc_cluster3)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 3 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()\n",
    "plt.plot(range(len(Avg_acc_cluster4)), Avg_acc_cluster4)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 4 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Avg_acc_cluster0)\n",
    "print(\"gap\")\n",
    "print(Avg_acc_cluster1)\n",
    "print(\"gap\")\n",
    "print(Avg_acc_cluster2)\n",
    "print(\"gap\")\n",
    "print(Avg_acc_cluster3)\n",
    "print(\"gap\")\n",
    "print(Avg_acc_cluster4)\n",
    "print(\"gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5000 antenna\n",
    "Cluster_0_accuracy=[35.05999954342842, 50.0, 50.0, 66.25, 50.45000076293945, 71.19999694824219, 70.5999984741211, 74.0, 75.69999694824219, 75.75, 76.05000305175781, 76.3499984741211, 76.69999694824219, 77.1500015258789, 77.0, 76.80000305175781, 76.1500015258789, 76.0999984741211, 78.9000015258789, 79.05000305175781, 79.30000305175781, 79.69999694824219, 78.55000305175781, 80.5, 79.44999694824219, 80.05000305175781, 81.30000305175781, 81.55000305175781, 81.4000015258789, 82.19999694824219, 81.9000015258789, 82.1500015258789, 82.55000305175781, 83.0, 82.25, 82.94999694824219, 83.0999984741211, 83.55000305175781, 83.44999694824219, 83.69999694824219, 83.80000305175781, 83.80000305175781, 84.1500015258789, 84.55000305175781, 83.80000305175781, 84.19999694824219, 84.30000305175781, 84.5, 84.55000305175781, 85.0999984741211, 84.69999694824219, 84.94999694824219, 85.05000305175781, 84.5999984741211, 85.0999984741211, 85.44999694824219, 85.4000015258789, 84.69999694824219, 85.1500015258789, 85.4000015258789, 85.05000305175781, 85.05000305175781, 85.6500015258789, 85.05000305175781, 85.44999694824219, 85.3499984741211, 85.05000305175781, 85.80000305175781, 85.5999984741211, 85.8499984741211, 85.5999984741211, 85.80000305175781, 85.6500015258789, 85.25, 85.94999694824219, 85.8499984741211, 86.3499984741211, 86.0, 86.19999694824219, 86.19999694824219, 86.3499984741211, 86.30000305175781, 86.25, 86.19999694824219, 86.19999694824219, 86.25, 86.30000305175781, 86.5999984741211, 86.4000015258789, 86.55000305175781, 86.44999694824219, 86.9000015258789, 86.94999694824219, 87.0999984741211, 86.69999694824219, 87.19999694824219, 86.75, 86.8499984741211, 87.4000015258789, 87.3499984741211, 86.8499984741211, 86.8499984741211, 87.6500015258789, 87.25, 87.30000305175781, 87.19999694824219, 87.05000305175781, 87.6500015258789, 87.1500015258789, 87.19999694824219, 87.1500015258789, 87.69999694824219, 87.25, 87.5999984741211, 87.25, 87.80000305175781, 87.44999694824219, 87.5, 87.55000305175781, 87.5999984741211, 87.55000305175781, 87.80000305175781, 87.55000305175781, 87.80000305175781, 88.0, 87.5999984741211, 88.1500015258789, 87.94999694824219, 88.05000305175781, 88.19999694824219, 88.0, 88.05000305175781, 87.80000305175781, 88.0, 88.4000015258789, 88.44999694824219, 88.44999694824219, 88.4000015258789, 88.5, 88.75, 87.94999694824219, 88.6500015258789, 88.5999984741211, 88.3499984741211, 88.55000305175781, 88.80000305175781, 88.80000305175781, 88.69999694824219, 89.05000305175781, 88.6500015258789, 88.8499984741211, 88.69999694824219, 88.4000015258789, 88.5999984741211, 88.75, 89.0, 88.9000015258789, 88.69999694824219, 88.69999694824219, 89.0999984741211, 88.6500015258789, 88.5, 88.9000015258789, 88.5, 88.75, 89.0, 89.0999984741211, 88.8499984741211, 88.6500015258789, 88.9000015258789, 89.1500015258789, 89.30000305175781, 89.3499984741211, 88.80000305175781, 89.0999984741211, 88.94999694824219, 89.05000305175781, 89.05000305175781, 89.5, 89.30000305175781, 89.05000305175781, 87.69999694824219, 89.5, 89.30000305175781, 89.19999694824219, 89.5, 89.1500015258789, 89.0999984741211, 88.8499984741211, 89.05000305175781, 89.30000305175781, 89.30000305175781, 89.5, 89.19999694824219, 89.30000305175781, 89.3499984741211, 89.19999694824219, 89.19999694824219, 88.75, 89.30000305175781]\n",
    "Cluster_1_accuracy=[50.0, 50.0, 50.0, 50.0, 52.599998474121094, 53.099998474121094, 50.349998474121094, 54.29999923706055, 54.25, 60.54999923706055, 54.29999923706055, 61.650001525878906, 61.75, 62.150001525878906, 61.79999923706055, 63.150001525878906, 62.849998474121094, 64.1500015258789, 65.5, 62.099998474121094, 66.0, 67.55000305175781, 66.6500015258789, 68.55000305175781, 66.25, 66.5, 69.05000305175781, 69.8499984741211, 70.05000305175781, 70.6500015258789, 70.05000305175781, 71.05000305175781, 70.8499984741211, 69.05000305175781, 71.6500015258789, 70.94999694824219, 71.8499984741211, 70.5, 70.05000305175781, 71.1500015258789, 72.19999694824219, 71.69999694824219, 71.25, 72.94999694824219, 71.9000015258789, 73.30000305175781, 72.75, 71.9000015258789, 73.30000305175781, 73.3499984741211, 73.1500015258789, 72.94999694824219, 73.55000305175781, 72.30000305175781, 72.75, 72.5999984741211, 73.80000305175781, 72.25, 73.30000305175781, 72.4000015258789, 73.0999984741211, 73.5, 73.55000305175781, 73.5999984741211, 73.6500015258789, 73.5, 73.55000305175781, 73.8499984741211, 73.1500015258789, 73.4000015258789, 74.0999984741211, 73.44999694824219, 73.3499984741211, 73.80000305175781, 73.8499984741211, 73.44999694824219, 73.69999694824219, 73.5999984741211, 73.75, 73.0, 73.94999694824219, 73.80000305175781, 73.8499984741211, 73.80000305175781, 73.9000015258789, 74.05000305175781, 73.6500015258789, 73.0999984741211, 73.94999694824219, 73.94999694824219, 74.30000305175781, 73.80000305175781, 74.05000305175781, 73.6500015258789, 73.80000305175781, 74.0999984741211, 74.19999694824219, 74.0, 74.05000305175781, 74.44999694824219, 74.1500015258789, 74.30000305175781, 73.5, 74.30000305175781, 74.25, 73.80000305175781, 73.94999694824219, 74.1500015258789, 73.94999694824219, 73.8499984741211, 73.75, 74.30000305175781, 74.0, 74.05000305175781, 74.05000305175781, 74.0999984741211, 74.3499984741211, 74.30000305175781, 74.6500015258789, 74.44999694824219, 73.8499984741211, 74.5999984741211, 74.0999984741211, 73.8499984741211, 74.5, 74.55000305175781, 74.6500015258789, 74.19999694824219, 74.9000015258789, 75.1500015258789, 74.9000015258789, 74.05000305175781, 74.3499984741211, 74.1500015258789, 74.75, 74.44999694824219, 74.44999694824219, 75.25, 74.8499984741211, 75.25, 74.05000305175781, 75.5, 74.0, 75.30000305175781, 74.80000305175781, 74.5, 74.6500015258789, 74.9000015258789, 74.44999694824219, 75.3499984741211, 75.0, 74.0999984741211, 74.5, 75.5, 75.0, 75.25, 75.1500015258789, 75.4000015258789, 75.1500015258789, 74.44999694824219, 74.9000015258789, 75.44999694824219, 75.4000015258789, 74.75, 74.5999984741211, 75.0, 75.19999694824219, 74.94999694824219, 75.0999984741211, 75.44999694824219, 74.80000305175781, 75.19999694824219, 75.55000305175781, 75.25, 75.25, 74.9000015258789, 74.8499984741211, 75.44999694824219, 75.5, 74.94999694824219, 75.19999694824219, 74.94999694824219, 75.0999984741211, 75.3499984741211, 75.1500015258789, 75.55000305175781, 75.0999984741211, 75.55000305175781, 75.25, 75.4000015258789, 74.94999694824219, 75.19999694824219, 75.0999984741211, 75.5999984741211, 75.0, 75.44999694824219, 75.0, 75.9000015258789, 75.5999984741211, 75.25]\n",
    "Cluster_2_accuracy=[51.900001525878906, 59.0, 58.099998474121094, 50.0, 55.54999923706055, 50.45000076293945, 56.45000076293945, 62.349998474121094, 66.0999984741211, 60.5, 65.94999694824219, 65.0, 67.0999984741211, 68.0, 67.4000015258789, 68.5, 69.55000305175781, 67.5999984741211, 70.05000305175781, 70.6500015258789, 71.5999984741211, 71.80000305175781, 72.55000305175781, 72.25, 71.5, 73.80000305175781, 74.75, 74.1500015258789, 74.55000305175781, 74.3499984741211, 75.0999984741211, 76.0999984741211, 75.5999984741211, 76.30000305175781, 76.25, 77.5999984741211, 76.05000305175781, 77.75, 77.1500015258789, 77.55000305175781, 76.69999694824219, 78.0999984741211, 77.5, 77.8499984741211, 77.8499984741211, 78.30000305175781, 78.3499984741211, 78.25, 78.1500015258789, 78.19999694824219, 78.80000305175781, 78.3499984741211, 79.05000305175781, 78.44999694824219, 78.44999694824219, 78.80000305175781, 78.3499984741211, 78.30000305175781, 78.80000305175781, 79.1500015258789, 78.9000015258789, 79.0999984741211, 79.05000305175781, 79.0, 79.19999694824219, 78.0, 79.30000305175781, 79.5999984741211, 79.05000305175781, 80.19999694824219, 79.30000305175781, 79.25, 79.25, 79.6500015258789, 80.0, 80.3499984741211, 80.0999984741211, 80.1500015258789, 79.5999984741211, 80.19999694824219, 79.80000305175781, 80.4000015258789, 79.5, 80.6500015258789, 79.75, 79.8499984741211, 80.19999694824219, 81.0, 79.80000305175781, 79.9000015258789, 80.25, 80.55000305175781, 80.55000305175781, 81.0, 80.4000015258789, 80.3499984741211, 80.19999694824219, 80.9000015258789, 80.69999694824219, 81.0, 80.30000305175781, 81.0, 80.69999694824219, 80.3499984741211, 80.19999694824219, 80.75, 80.69999694824219, 80.94999694824219, 80.6500015258789, 80.8499984741211, 80.4000015258789, 80.94999694824219, 80.5999984741211, 81.0, 80.25, 80.94999694824219, 81.0999984741211, 81.1500015258789, 81.0, 80.94999694824219, 80.5999984741211, 81.05000305175781, 80.75, 80.94999694824219, 80.44999694824219, 81.19999694824219, 80.30000305175781, 81.5, 80.69999694824219, 81.0, 80.8499984741211, 81.19999694824219, 80.5, 81.0, 80.19999694824219, 80.9000015258789, 80.44999694824219, 81.4000015258789, 80.5999984741211, 81.0, 80.44999694824219, 80.9000015258789, 80.4000015258789, 80.8499984741211, 80.6500015258789, 80.9000015258789, 80.5999984741211, 81.0999984741211, 80.25, 80.94999694824219, 80.44999694824219, 80.9000015258789, 80.3499984741211, 81.0999984741211, 80.80000305175781, 81.05000305175781, 80.44999694824219, 81.19999694824219, 80.44999694824219, 81.3499984741211, 80.6500015258789, 81.0999984741211, 80.69999694824219, 81.0999984741211, 80.55000305175781, 81.30000305175781, 80.8499984741211, 81.4000015258789, 81.0, 81.44999694824219, 81.05000305175781, 81.1500015258789, 80.69999694824219, 81.5999984741211, 81.05000305175781, 81.4000015258789, 80.69999694824219, 81.44999694824219, 80.75, 81.25, 81.0, 81.30000305175781, 81.19999694824219, 81.4000015258789, 80.6500015258789, 81.5, 81.0999984741211, 82.0999984741211, 80.80000305175781, 81.55000305175781, 80.8499984741211, 81.4000015258789, 81.3499984741211, 81.8499984741211, 81.19999694824219, 81.80000305175781, 81.4000015258789, 82.1500015258789, 81.0999984741211, 82.19999694824219]\n",
    "Cluster_3_accuracy=[50.29999923706055, 50.0, 50.0, 50.0, 53.5, 68.25, 66.3499984741211, 67.75, 70.6500015258789, 71.19999694824219, 74.25, 72.75, 73.25, 73.19999694824219, 76.05000305175781, 76.44999694824219, 75.44999694824219, 75.6500015258789, 78.5999984741211, 74.19999694824219, 78.55000305175781, 78.6500015258789, 80.1500015258789, 79.69999694824219, 80.4000015258789, 80.0999984741211, 80.8499984741211, 80.0, 81.25, 82.1500015258789, 82.69999694824219, 82.75, 82.5999984741211, 82.94999694824219, 83.3499984741211, 83.3499984741211, 83.75, 84.30000305175781, 84.0999984741211, 84.25, 84.5999984741211, 84.55000305175781, 85.55000305175781, 85.0999984741211, 85.6500015258789, 85.5, 86.19999694824219, 86.1500015258789, 86.69999694824219, 86.05000305175781, 86.6500015258789, 86.30000305175781, 87.1500015258789, 86.4000015258789, 87.0999984741211, 86.8499984741211, 87.5, 87.4000015258789, 87.3499984741211, 87.3499984741211, 87.69999694824219, 87.4000015258789, 87.80000305175781, 87.6500015258789, 88.1500015258789, 88.0, 88.1500015258789, 88.0999984741211, 88.25, 88.0999984741211, 88.0999984741211, 88.19999694824219, 88.1500015258789, 88.30000305175781, 88.05000305175781, 88.05000305175781, 88.19999694824219, 88.30000305175781, 88.0999984741211, 88.94999694824219, 88.44999694824219, 88.80000305175781, 88.75, 88.80000305175781, 88.5999984741211, 88.80000305175781, 88.6500015258789, 89.0999984741211, 89.0, 88.75, 89.0, 89.05000305175781, 89.19999694824219, 89.0, 89.4000015258789, 89.05000305175781, 89.19999694824219, 89.19999694824219, 89.0, 89.44999694824219, 89.5999984741211, 89.1500015258789, 89.4000015258789, 89.44999694824219, 89.44999694824219, 89.5999984741211, 89.3499984741211, 89.3499984741211, 89.5, 89.55000305175781, 89.44999694824219, 89.69999694824219, 89.44999694824219, 89.44999694824219, 89.6500015258789, 89.4000015258789, 89.44999694824219, 89.6500015258789, 89.55000305175781, 89.55000305175781, 90.05000305175781, 89.9000015258789, 89.75, 89.75, 89.44999694824219, 89.80000305175781, 89.6500015258789, 89.75, 89.6500015258789, 89.8499984741211, 90.1500015258789, 89.80000305175781, 89.30000305175781, 89.80000305175781, 89.44999694824219, 89.94999694824219, 90.0, 89.8499984741211, 90.0, 90.1500015258789, 89.75, 90.05000305175781, 90.05000305175781, 90.05000305175781, 89.8499984741211, 90.1500015258789, 90.05000305175781, 89.9000015258789, 90.0, 90.5, 89.80000305175781, 90.30000305175781, 90.19999694824219, 90.30000305175781, 90.25, 90.5, 90.0, 90.6500015258789, 90.05000305175781, 90.1500015258789, 89.80000305175781, 90.44999694824219, 90.30000305175781, 90.30000305175781, 90.0999984741211, 90.55000305175781, 90.19999694824219, 90.19999694824219, 89.9000015258789, 90.75, 90.0, 91.0999984741211, 90.0999984741211, 90.8499984741211, 90.19999694824219, 91.0, 90.3499984741211, 91.05000305175781, 90.25, 90.94999694824219, 90.44999694824219, 90.80000305175781, 90.3499984741211, 91.1500015258789, 90.5999984741211, 90.9000015258789, 90.0, 91.1500015258789, 90.0, 91.0999984741211, 90.1500015258789, 90.55000305175781, 90.1500015258789, 91.25, 90.0999984741211, 91.0999984741211, 90.4000015258789, 91.25, 89.94999694824219, 91.19999694824219]\n",
    "Cluster_4_accuracy=[50.0, 50.0, 50.0, 50.0, 50.5, 66.19999694824219, 67.30000305175781, 66.0999984741211, 68.94999694824219, 64.80000305175781, 69.4000015258789, 70.19999694824219, 69.8499984741211, 70.4000015258789, 71.5, 71.8499984741211, 72.25, 72.8499984741211, 72.80000305175781, 73.55000305175781, 73.80000305175781, 74.44999694824219, 74.6500015258789, 75.5, 75.0, 76.44999694824219, 76.30000305175781, 77.30000305175781, 77.44999694824219, 78.30000305175781, 78.25, 78.94999694824219, 78.9000015258789, 79.0, 79.1500015258789, 79.4000015258789, 79.4000015258789, 80.19999694824219, 80.5999984741211, 80.6500015258789, 80.5999984741211, 81.1500015258789, 81.0, 81.0999984741211, 80.94999694824219, 81.4000015258789, 80.5999984741211, 81.3499984741211, 81.6500015258789, 81.69999694824219, 82.0, 82.5999984741211, 82.25, 82.5999984741211, 82.44999694824219, 82.4000015258789, 82.69999694824219, 83.4000015258789, 82.44999694824219, 83.19999694824219, 83.44999694824219, 83.75, 83.75, 83.94999694824219, 83.8499984741211, 84.1500015258789, 84.0999984741211, 84.0999984741211, 84.4000015258789, 84.75, 84.4000015258789, 84.6500015258789, 84.30000305175781, 85.0, 84.55000305175781, 84.8499984741211, 85.0999984741211, 85.05000305175781, 84.94999694824219, 85.25, 85.3499984741211, 85.55000305175781, 85.6500015258789, 85.75, 85.55000305175781, 86.05000305175781, 85.69999694824219, 85.44999694824219, 85.5, 86.19999694824219, 86.1500015258789, 86.0, 86.05000305175781, 86.25, 86.3499984741211, 86.3499984741211, 86.55000305175781, 86.30000305175781, 86.19999694824219, 86.6500015258789, 86.30000305175781, 86.75, 86.6500015258789, 86.3499984741211, 86.44999694824219, 86.5999984741211, 86.0999984741211, 86.75, 86.5, 86.69999694824219, 86.5999984741211, 86.80000305175781, 86.80000305175781, 86.80000305175781, 86.6500015258789, 86.9000015258789, 86.80000305175781, 86.80000305175781, 87.1500015258789, 86.75, 86.8499984741211, 86.94999694824219, 86.9000015258789, 86.9000015258789, 86.75, 87.44999694824219, 87.3499984741211, 87.5, 87.05000305175781, 87.3499984741211, 87.0, 87.25, 87.1500015258789, 87.25, 87.0999984741211, 87.5, 87.0999984741211, 87.25, 87.19999694824219, 87.6500015258789, 87.4000015258789, 87.5, 87.1500015258789, 87.5999984741211, 87.4000015258789, 87.6500015258789, 87.55000305175781, 87.3499984741211, 87.3499984741211, 87.0999984741211, 87.44999694824219, 87.8499984741211, 87.4000015258789, 87.5999984741211, 87.44999694824219, 87.25, 87.55000305175781, 87.69999694824219, 87.5999984741211, 87.55000305175781, 87.1500015258789, 87.75, 87.69999694824219, 87.5999984741211, 87.44999694824219, 87.69999694824219, 87.44999694824219, 87.5999984741211, 87.55000305175781, 88.05000305175781, 87.75, 87.8499984741211, 87.80000305175781, 87.75, 88.05000305175781, 88.05000305175781, 87.80000305175781, 87.80000305175781, 87.80000305175781, 88.25, 88.4000015258789, 88.1500015258789, 88.19999694824219, 87.94999694824219, 88.25, 88.05000305175781, 88.1500015258789, 88.25, 87.94999694824219, 88.30000305175781, 88.0999984741211, 88.0999984741211, 88.4000015258789, 88.3499984741211, 88.3499984741211, 88.3499984741211, 88.44999694824219, 88.55000305175781, 88.55000305175781, 88.6500015258789]\n",
    "\n",
    "#100 antenna lr=0.001\n",
    "Cluster_0_accuracy_100=[50.0, 50.0, 50.099998474121094, 51.79999923706055, 52.150001525878906, 61.79999923706055, 73.0999984741211, 50.20000076293945, 68.4000015258789, 73.75, 74.1500015258789, 76.25, 72.55000305175781, 76.9000015258789, 69.8499984741211, 64.9000015258789, 74.8499984741211, 60.349998474121094, 75.80000305175781, 72.5999984741211, 67.05000305175781, 64.25, 76.44999694824219, 71.5999984741211, 72.44999694824219, 75.25, 64.75, 73.5, 70.1500015258789, 65.9000015258789, 74.6500015258789, 73.5999984741211, 71.55000305175781, 72.55000305175781, 75.44999694824219, 72.94999694824219, 66.25, 64.0999984741211, 72.6500015258789, 67.55000305175781, 74.05000305175781, 74.9000015258789, 74.6500015258789, 75.4000015258789, 75.80000305175781, 74.6500015258789, 76.6500015258789, 67.44999694824219, 70.8499984741211, 72.94999694824219, 77.0999984741211, 77.19999694824219, 76.44999694824219, 77.0999984741211, 74.55000305175781, 71.75, 76.5, 77.19999694824219, 77.05000305175781, 77.30000305175781, 77.5999984741211, 75.05000305175781, 71.4000015258789, 73.44999694824219, 70.80000305175781, 78.4000015258789, 75.05000305175781, 76.55000305175781, 70.8499984741211, 78.19999694824219, 75.0, 66.8499984741211, 67.55000305175781, 77.05000305175781, 76.1500015258789, 78.4000015258789, 71.75, 74.5999984741211, 73.75, 75.75, 72.94999694824219, 73.8499984741211, 77.55000305175781, 70.5999984741211, 67.0, 78.55000305175781, 78.5999984741211, 78.3499984741211, 71.19999694824219, 77.44999694824219, 75.44999694824219, 71.44999694824219, 72.55000305175781, 74.0, 76.19999694824219, 69.9000015258789, 77.5999984741211, 73.3499984741211, 76.3499984741211, 75.19999694824219, 75.25, 73.19999694824219, 69.94999694824219, 71.0, 72.94999694824219, 73.19999694824219, 66.25, 73.94999694824219, 75.19999694824219, 76.44999694824219, 76.8499984741211, 71.5999984741211, 73.6500015258789, 71.80000305175781, 66.5, 74.1500015258789, 74.30000305175781, 74.69999694824219, 71.4000015258789, 70.8499984741211, 74.5, 71.80000305175781, 71.5, 70.05000305175781, 73.0, 73.69999694824219, 74.5, 71.75, 73.4000015258789, 75.25, 72.94999694824219, 72.5999984741211, 74.4000015258789, 72.80000305175781, 73.0999984741211, 71.6500015258789, 73.30000305175781, 72.1500015258789, 74.5999984741211, 72.0999984741211, 74.25, 75.75, 76.05000305175781, 74.3499984741211, 77.05000305175781, 74.0, 74.05000305175781, 74.55000305175781, 71.80000305175781, 73.05000305175781, 71.3499984741211, 74.6500015258789, 72.0, 73.80000305175781, 76.6500015258789, 75.6500015258789, 77.1500015258789, 65.9000015258789, 77.1500015258789, 76.5999984741211, 75.25, 76.19999694824219, 76.75, 75.6500015258789, 76.5999984741211, 73.1500015258789, 75.69999694824219, 76.69999694824219, 72.69999694824219, 75.1500015258789, 75.3499984741211, 74.05000305175781, 73.05000305175781, 73.5, 74.3499984741211, 73.69999694824219, 73.94999694824219, 74.30000305175781, 74.5999984741211, 73.75, 74.69999694824219, 73.4000015258789, 72.19999694824219, 74.19999694824219, 73.5, 75.5999984741211, 70.19999694824219, 75.1500015258789, 74.19999694824219, 73.1500015258789, 74.55000305175781, 73.4000015258789, 73.3499984741211, 76.5999984741211, 77.05000305175781, 75.0999984741211, 74.4000015258789, 77.1500015258789, 77.0999984741211, 75.8499984741211]\n",
    "Cluster_1_accuracy_100=[47.95000076293945, 50.0, 49.45000076293945, 50.650001525878906, 51.150001525878906, 53.75, 58.849998474121094, 61.75, 55.849998474121094, 58.75, 57.20000076293945, 58.900001525878906, 62.150001525878906, 65.55000305175781, 61.70000076293945, 60.400001525878906, 63.29999923706055, 64.5999984741211, 65.0, 65.6500015258789, 64.05000305175781, 67.55000305175781, 62.20000076293945, 61.75, 66.69999694824219, 65.94999694824219, 66.3499984741211, 62.5, 65.9000015258789, 66.25, 67.05000305175781, 64.5999984741211, 62.45000076293945, 63.95000076293945, 64.30000305175781, 63.29999923706055, 62.0, 62.099998474121094, 61.849998474121094, 63.75, 61.599998474121094, 63.150001525878906, 65.05000305175781, 63.70000076293945, 65.5999984741211, 61.54999923706055, 67.8499984741211, 66.5, 65.8499984741211, 67.19999694824219, 67.05000305175781, 56.29999923706055, 62.599998474121094, 61.900001525878906, 63.20000076293945, 57.099998474121094, 53.349998474121094, 53.400001525878906, 60.349998474121094, 57.0, 59.79999923706055, 61.75, 58.0, 63.099998474121094, 60.95000076293945, 61.599998474121094, 58.599998474121094, 63.900001525878906, 63.0, 63.900001525878906, 59.0, 61.04999923706055, 63.79999923706055, 57.900001525878906, 61.349998474121094, 60.75, 64.5999984741211, 64.69999694824219, 58.5, 65.75, 64.80000305175781, 63.5, 62.25, 64.25, 62.900001525878906, 63.900001525878906, 63.400001525878906, 64.6500015258789, 62.400001525878906, 62.75, 65.4000015258789, 64.0999984741211, 65.4000015258789, 58.900001525878906, 67.05000305175781, 67.4000015258789, 67.55000305175781, 64.75, 63.54999923706055, 62.79999923706055, 64.05000305175781, 62.25, 59.5, 62.849998474121094, 64.0999984741211, 61.75, 59.95000076293945, 62.29999923706055, 64.0999984741211, 64.30000305175781, 62.0, 63.099998474121094, 61.45000076293945, 62.5, 66.0, 65.44999694824219, 65.1500015258789, 66.1500015258789, 64.19999694824219, 65.1500015258789, 63.25, 65.05000305175781, 60.70000076293945, 59.45000076293945, 62.099998474121094, 64.5, 63.75, 65.5, 65.25, 63.900001525878906, 64.5, 62.54999923706055, 63.70000076293945, 63.650001525878906, 63.54999923706055, 64.05000305175781, 63.54999923706055, 61.04999923706055, 61.349998474121094, 62.75, 65.5, 65.3499984741211, 63.95000076293945, 65.25, 63.5, 60.04999923706055, 62.70000076293945, 60.849998474121094, 61.0, 56.20000076293945, 59.900001525878906, 61.0, 61.25, 63.099998474121094, 62.150001525878906, 62.849998474121094, 62.349998474121094, 62.70000076293945, 61.849998474121094, 63.400001525878906, 61.25, 60.04999923706055, 63.900001525878906, 61.400001525878906, 57.650001525878906, 65.1500015258789, 62.95000076293945, 62.900001525878906, 62.650001525878906, 62.29999923706055, 62.400001525878906, 61.900001525878906, 58.349998474121094, 61.29999923706055, 62.25, 62.0, 63.5, 63.599998474121094, 59.79999923706055, 61.75, 62.150001525878906, 62.29999923706055, 61.95000076293945, 62.25, 62.400001525878906, 62.349998474121094, 62.099998474121094, 60.0, 61.599998474121094, 60.20000076293945, 58.75, 60.150001525878906, 60.45000076293945, 59.29999923706055, 60.5, 60.849998474121094, 60.900001525878906, 61.04999923706055, 61.20000076293945, 63.29999923706055]\n",
    "Cluster_2_accuracy_100=[21.849999237060548, 50.20000076293945, 48.70000076293945, 51.29999923706055, 50.5, 52.70000076293945, 50.25, 50.349998474121094, 63.45000076293945, 61.79999923706055, 51.599998474121094, 61.5, 51.04999923706055, 67.30000305175781, 50.20000076293945, 58.75, 63.79999923706055, 59.900001525878906, 55.75, 65.94999694824219, 58.900001525878906, 64.55000305175781, 59.400001525878906, 64.75, 65.19999694824219, 66.55000305175781, 57.29999923706055, 54.29999923706055, 62.650001525878906, 53.95000076293945, 60.79999923706055, 65.6500015258789, 59.650001525878906, 67.5, 60.45000076293945, 56.349998474121094, 63.45000076293945, 66.3499984741211, 66.0, 65.44999694824219, 58.29999923706055, 60.650001525878906, 66.1500015258789, 61.04999923706055, 60.099998474121094, 60.75, 63.70000076293945, 56.150001525878906, 51.95000076293945, 57.79999923706055, 61.75, 64.80000305175781, 56.75, 56.5, 59.650001525878906, 52.849998474121094, 51.54999923706055, 60.400001525878906, 62.849998474121094, 59.349998474121094, 58.849998474121094, 61.95000076293945, 62.95000076293945, 54.849998474121094, 58.95000076293945, 64.80000305175781, 59.25, 61.20000076293945, 55.45000076293945, 59.54999923706055, 58.70000076293945, 62.25, 53.900001525878906, 62.95000076293945, 63.400001525878906, 60.650001525878906, 64.5, 65.8499984741211, 60.900001525878906, 66.0, 61.95000076293945, 62.70000076293945, 63.54999923706055, 64.44999694824219, 65.30000305175781, 65.5, 63.20000076293945, 65.0999984741211, 62.099998474121094, 58.20000076293945, 61.29999923706055, 61.150001525878906, 62.150001525878906, 62.75, 64.3499984741211, 62.5, 63.20000076293945, 64.0999984741211, 67.55000305175781, 57.04999923706055, 59.04999923706055, 68.4000015258789, 60.150001525878906, 63.20000076293945, 63.20000076293945, 60.5, 63.79999923706055, 66.69999694824219, 53.349998474121094, 61.349998474121094, 60.900001525878906, 61.70000076293945, 62.0, 63.04999923706055, 64.94999694824219, 62.70000076293945, 62.599998474121094, 63.29999923706055, 64.0, 63.400001525878906, 59.650001525878906, 56.29999923706055, 63.0, 63.54999923706055, 62.20000076293945, 63.400001525878906, 53.25, 58.900001525878906, 56.150001525878906, 58.25, 63.650001525878906, 60.599998474121094, 59.75, 60.25, 57.650001525878906, 62.849998474121094, 57.79999923706055, 59.45000076293945, 60.849998474121094, 62.400001525878906, 60.849998474121094, 61.150001525878906, 63.099998474121094, 62.29999923706055, 57.900001525878906, 62.54999923706055, 60.150001525878906, 61.849998474121094, 62.0, 62.04999923706055, 62.400001525878906, 63.150001525878906, 59.400001525878906, 55.900001525878906, 55.650001525878906, 51.54999923706055, 57.04999923706055, 60.75, 61.29999923706055, 62.04999923706055, 60.099998474121094, 56.849998474121094, 60.400001525878906, 64.75, 63.849998474121094, 60.849998474121094, 62.20000076293945, 59.79999923706055, 58.349998474121094, 60.900001525878906, 63.20000076293945, 62.29999923706055, 58.79999923706055, 61.25, 61.349998474121094, 62.900001525878906, 61.150001525878906, 63.04999923706055, 59.150001525878906, 63.900001525878906, 61.54999923706055, 60.95000076293945, 56.79999923706055, 61.75, 63.400001525878906, 64.44999694824219, 63.349998474121094, 63.45000076293945, 62.099998474121094, 64.05000305175781, 60.349998474121094, 62.5, 59.75, 60.099998474121094, 59.75, 53.79999923706055, 55.650001525878906, 53.45000076293945, 55.70000076293945, 55.70000076293945]\n",
    "Cluster_3_accuracy_100=[31.31999939084053, 49.95000076293945, 51.95000076293945, 64.75, 56.900001525878906, 61.29999923706055, 50.400001525878906, 51.45000076293945, 58.70000076293945, 64.94999694824219, 73.6500015258789, 62.95000076293945, 55.54999923706055, 76.5, 73.75, 62.849998474121094, 72.8499984741211, 55.5, 73.8499984741211, 69.3499984741211, 75.0, 66.69999694824219, 72.5, 76.55000305175781, 79.0, 78.4000015258789, 72.19999694824219, 78.19999694824219, 68.19999694824219, 77.69999694824219, 78.9000015258789, 73.0, 77.25, 78.44999694824219, 62.70000076293945, 65.3499984741211, 77.0999984741211, 74.19999694824219, 76.94999694824219, 78.5999984741211, 66.05000305175781, 72.3499984741211, 76.1500015258789, 75.1500015258789, 67.3499984741211, 53.20000076293945, 67.19999694824219, 70.75, 76.1500015258789, 68.69999694824219, 73.1500015258789, 73.55000305175781, 72.1500015258789, 63.349998474121094, 77.5, 79.6500015258789, 62.04999923706055, 66.05000305175781, 57.04999923706055, 70.0, 66.6500015258789, 53.650001525878906, 76.3499984741211, 60.29999923706055, 71.5, 69.8499984741211, 70.75, 63.400001525878906, 74.0999984741211, 76.0, 72.30000305175781, 71.0999984741211, 72.9000015258789, 75.05000305175781, 71.94999694824219, 70.9000015258789, 71.80000305175781, 71.75, 71.80000305175781, 76.9000015258789, 70.5, 74.19999694824219, 64.44999694824219, 72.80000305175781, 74.55000305175781, 75.0999984741211, 75.8499984741211, 72.4000015258789, 73.80000305175781, 75.5999984741211, 73.44999694824219, 77.8499984741211, 71.69999694824219, 76.0999984741211, 75.3499984741211, 75.8499984741211, 75.1500015258789, 78.5999984741211, 75.30000305175781, 79.0999984741211, 73.94999694824219, 75.5, 76.30000305175781, 76.4000015258789, 74.5, 74.3499984741211, 77.80000305175781, 77.94999694824219, 77.8499984741211, 78.9000015258789, 76.80000305175781, 77.8499984741211, 73.0999984741211, 73.6500015258789, 75.80000305175781, 77.05000305175781, 76.0999984741211, 75.05000305175781, 77.25, 75.75, 76.30000305175781, 76.6500015258789, 76.75, 77.5, 76.05000305175781, 76.05000305175781, 77.75, 71.44999694824219, 76.75, 73.0, 77.94999694824219, 74.25, 73.80000305175781, 71.55000305175781, 76.69999694824219, 76.69999694824219, 78.6500015258789, 77.44999694824219, 76.55000305175781, 78.1500015258789, 74.19999694824219, 76.3499984741211, 65.1500015258789, 70.8499984741211, 75.44999694824219, 74.19999694824219, 74.30000305175781, 72.25, 71.30000305175781, 73.5999984741211, 74.25, 77.69999694824219, 73.8499984741211, 77.4000015258789, 76.44999694824219, 62.25, 72.1500015258789, 76.75, 75.8499984741211, 76.9000015258789, 77.05000305175781, 77.05000305175781, 76.4000015258789, 74.9000015258789, 76.69999694824219, 75.25, 75.5, 70.4000015258789, 73.9000015258789, 72.6500015258789, 77.55000305175781, 76.0999984741211, 74.44999694824219, 68.3499984741211, 74.25, 76.1500015258789, 75.94999694824219, 76.0, 73.80000305175781, 75.9000015258789, 74.05000305175781, 74.55000305175781, 73.1500015258789, 72.30000305175781, 70.3499984741211, 74.25, 75.55000305175781, 75.19999694824219, 75.30000305175781, 69.5999984741211, 73.69999694824219, 75.19999694824219, 73.30000305175781, 74.4000015258789, 70.69999694824219, 71.80000305175781, 72.25, 73.05000305175781, 74.6500015258789, 73.6500015258789]\n",
    "Cluster_4_accuracy_100=[50.0, 50.0, 49.95000076293945, 48.95000076293945, 56.25, 65.5999984741211, 57.20000076293945, 67.9000015258789, 59.95000076293945, 65.94999694824219, 64.0, 53.5, 57.349998474121094, 66.44999694824219, 65.3499984741211, 55.79999923706055, 62.900001525878906, 55.5, 51.29999923706055, 53.5, 65.94999694824219, 52.0, 67.94999694824219, 68.3499984741211, 65.55000305175781, 69.25, 60.099998474121094, 63.849998474121094, 63.25, 68.5, 71.6500015258789, 68.1500015258789, 67.25, 64.30000305175781, 70.4000015258789, 69.55000305175781, 58.900001525878906, 59.650001525878906, 71.55000305175781, 64.44999694824219, 63.349998474121094, 65.1500015258789, 66.0999984741211, 70.05000305175781, 71.1500015258789, 70.69999694824219, 71.55000305175781, 71.0, 71.3499984741211, 67.75, 73.3499984741211, 64.6500015258789, 66.30000305175781, 65.05000305175781, 68.5999984741211, 60.599998474121094, 66.25, 63.04999923706055, 67.3499984741211, 67.55000305175781, 65.80000305175781, 63.25, 67.94999694824219, 63.20000076293945, 67.69999694824219, 62.45000076293945, 65.1500015258789, 65.1500015258789, 57.70000076293945, 57.04999923706055, 63.20000076293945, 66.0999984741211, 58.849998474121094, 67.0, 68.55000305175781, 67.0, 64.80000305175781, 68.75, 64.8499984741211, 64.25, 68.94999694824219, 66.3499984741211, 66.4000015258789, 67.8499984741211, 64.6500015258789, 62.25, 67.44999694824219, 60.75, 66.4000015258789, 63.79999923706055, 67.9000015258789, 65.94999694824219, 68.3499984741211, 65.44999694824219, 68.0999984741211, 68.69999694824219, 66.75, 67.80000305175781, 64.55000305175781, 68.5, 65.44999694824219, 62.150001525878906, 65.25, 66.0999984741211, 68.19999694824219, 66.9000015258789, 59.599998474121094, 64.69999694824219, 67.19999694824219, 59.650001525878906, 64.75, 62.70000076293945, 64.6500015258789, 64.19999694824219, 65.0, 65.25, 65.25, 65.8499984741211, 66.25, 66.25, 66.30000305175781, 64.94999694824219, 64.80000305175781, 66.55000305175781, 68.05000305175781, 66.75, 68.75, 66.5999984741211, 65.69999694824219, 67.1500015258789, 66.0999984741211, 63.849998474121094, 66.75, 66.25, 64.8499984741211, 66.19999694824219, 63.04999923706055, 62.5, 67.05000305175781, 66.3499984741211, 68.05000305175781, 65.8499984741211, 63.599998474121094, 63.45000076293945, 62.25, 66.5999984741211, 65.3499984741211, 63.349998474121094, 62.20000076293945, 62.400001525878906, 62.79999923706055, 61.150001525878906, 64.3499984741211, 63.900001525878906, 66.4000015258789, 68.1500015258789, 64.69999694824219, 63.95000076293945, 67.75, 65.5999984741211, 64.05000305175781, 63.95000076293945, 64.25, 61.900001525878906, 70.8499984741211, 65.05000305175781, 70.55000305175781, 68.3499984741211, 71.05000305175781, 69.4000015258789, 66.69999694824219, 65.94999694824219, 69.75, 66.8499984741211, 69.44999694824219, 67.8499984741211, 66.75, 69.1500015258789, 63.29999923706055, 69.6500015258789, 71.4000015258789, 70.25, 66.5999984741211, 69.69999694824219, 71.5, 71.0999984741211, 70.80000305175781, 69.5, 67.05000305175781, 70.5999984741211, 69.44999694824219, 69.1500015258789, 72.25, 71.1500015258789, 69.94999694824219, 68.1500015258789, 67.1500015258789, 67.25, 67.8499984741211, 60.650001525878906]\n",
    "\n",
    "#50 antenna lr =0.00001\n",
    "Cluster_0_accuracy_50=[44.20000076293945, 44.70000076293945, 48.150001525878906, 49.95000076293945, 50.45000076293945, 51.900001525878906, 56.79999923706055, 53.75, 50.0, 58.150001525878906, 54.25, 59.650001525878906, 54.45000076293945, 58.20000076293945, 63.45000076293945, 57.0, 50.0, 55.5, 52.04999923706055, 53.099998474121094, 50.5, 52.0, 50.04999923706055, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 54.25, 53.900001525878906, 50.099998474121094, 50.349998474121094, 55.29999923706055, 56.75, 66.25, 73.25, 69.5999984741211, 70.80000305175781, 73.75, 72.9000015258789, 69.6500015258789, 75.0999984741211, 70.75, 74.19999694824219, 70.0999984741211, 74.3499984741211, 75.0999984741211, 75.05000305175781, 74.44999694824219, 75.75, 73.9000015258789, 74.44999694824219, 75.75, 74.5999984741211, 74.30000305175781, 75.80000305175781, 74.55000305175781, 74.5, 73.94999694824219, 75.75, 75.0999984741211, 76.5999984741211, 76.55000305175781, 76.0999984741211, 76.75, 75.80000305175781, 75.19999694824219, 76.6500015258789, 76.80000305175781, 76.44999694824219, 76.6500015258789, 76.5999984741211, 75.5, 76.5999984741211, 75.4000015258789, 75.55000305175781, 76.0999984741211, 74.9000015258789, 76.75, 76.8499984741211, 75.19999694824219, 76.30000305175781, 77.1500015258789, 76.69999694824219, 76.44999694824219, 76.75, 76.44999694824219, 76.75, 76.8499984741211, 76.6500015258789, 75.94999694824219, 75.8499984741211, 77.3499984741211, 76.3499984741211, 76.80000305175781, 76.3499984741211, 77.5, 77.3499984741211, 76.9000015258789, 76.75, 78.25, 76.30000305175781, 77.1500015258789, 77.80000305175781, 76.5999984741211, 76.75, 77.19999694824219, 76.75, 77.30000305175781, 77.69999694824219, 78.5999984741211, 78.69999694824219, 78.6500015258789, 78.75, 77.4000015258789, 78.0, 78.9000015258789, 76.9000015258789, 77.0999984741211, 78.4000015258789, 79.4000015258789, 78.30000305175781, 79.3499984741211, 79.05000305175781, 79.3499984741211, 79.19999694824219, 77.4000015258789, 79.25, 78.19999694824219, 78.3499984741211, 78.4000015258789, 79.44999694824219, 79.44999694824219, 77.80000305175781, 77.3499984741211, 78.44999694824219, 77.9000015258789, 77.8499984741211, 78.80000305175781, 78.25, 77.94999694824219, 79.4000015258789, 78.3499984741211, 78.5999984741211, 78.69999694824219, 78.9000015258789, 79.4000015258789, 79.4000015258789, 79.44999694824219, 79.8499984741211, 79.5, 80.69999694824219, 80.05000305175781, 79.5, 80.3499984741211, 80.0, 80.55000305175781, 80.3499984741211, 80.5, 79.80000305175781, 80.5, 79.6500015258789, 81.0, 81.0999984741211, 80.94999694824219, 80.19999694824219, 81.0, 80.80000305175781, 79.75, 80.4000015258789, 80.3499984741211, 80.19999694824219, 80.94999694824219, 80.30000305175781, 81.0999984741211, 80.9000015258789, 81.0999984741211, 80.0, 80.8499984741211, 81.25, 80.80000305175781, 80.94999694824219, 80.69999694824219, 80.4000015258789, 80.6500015258789, 80.55000305175781, 80.44999694824219, 81.0999984741211, 81.30000305175781, 80.8499984741211, 81.0999984741211, 81.25, 80.19999694824219, 81.55000305175781, 81.80000305175781, 81.05000305175781, 81.8499984741211, 80.8499984741211, 82.05000305175781, 82.0999984741211, 81.80000305175781, 81.25, 82.0, 82.05000305175781, 81.25, 81.6500015258789, 81.0999984741211, 81.19999694824219, 80.9000015258789, 81.94999694824219, 81.5, 81.69999694824219, 81.8499984741211, 81.44999694824219, 82.19999694824219, 82.5, 82.69999694824219, 81.69999694824219, 82.9000015258789, 80.80000305175781, 82.19999694824219, 81.94999694824219, 82.19999694824219, 82.5, 82.19999694824219, 82.69999694824219, 82.30000305175781, 82.0999984741211, 82.6500015258789, 82.3499984741211, 82.3499984741211, 82.5999984741211, 82.5, 82.94999694824219, 83.19999694824219, 82.8499984741211, 81.3499984741211, 83.1500015258789, 82.1500015258789, 82.25, 82.4000015258789, 83.1500015258789, 83.0999984741211, 82.80000305175781, 82.80000305175781, 82.8499984741211, 83.19999694824219, 83.0, 82.9000015258789, 82.94999694824219, 82.80000305175781, 82.75, 83.3499984741211, 83.44999694824219, 83.4000015258789, 82.8499984741211, 82.8499984741211, 81.5999984741211, 82.80000305175781, 83.44999694824219, 83.3499984741211, 83.30000305175781, 83.4000015258789, 83.4000015258789, 82.0, 81.5999984741211, 83.25, 83.44999694824219, 83.4000015258789, 83.30000305175781, 83.4000015258789, 82.75, 83.0, 83.05000305175781, 82.44999694824219, 82.94999694824219, 83.30000305175781, 83.9000015258789, 83.5999984741211, 83.9000015258789, 83.8499984741211, 83.4000015258789, 83.75, 83.05000305175781, 83.4000015258789, 83.69999694824219, 83.8499984741211, 83.3499984741211, 82.9000015258789, 83.6500015258789, 83.5, 83.05000305175781, 82.94999694824219, 83.19999694824219, 83.4000015258789, 83.44999694824219, 82.44999694824219, 82.30000305175781, 83.3499984741211, 83.4000015258789, 83.19999694824219, 81.44999694824219, 83.6500015258789, 83.30000305175781, 83.55000305175781, 83.5999984741211, 83.4000015258789, 83.1500015258789, 84.1500015258789, 83.69999694824219, 84.05000305175781, 83.44999694824219, 83.5999984741211, 83.5999984741211, 83.55000305175781, 83.3499984741211, 83.75, 83.8499984741211, 83.69999694824219, 83.5999984741211, 83.80000305175781, 83.94999694824219, 84.0999984741211, 84.19999694824219, 83.75, 84.0, 83.3499984741211, 84.4000015258789, 84.5, 84.19999694824219, 84.8499984741211, 84.55000305175781, 84.3499984741211, 84.5999984741211, 83.4000015258789, 83.80000305175781, 84.5999984741211, 84.25, 84.4000015258789, 84.44999694824219, 84.8499984741211, 82.9000015258789, 84.80000305175781, 84.55000305175781, 84.3499984741211, 84.05000305175781, 84.3499984741211, 82.94999694824219, 84.6500015258789, 82.6500015258789, 84.3499984741211, 84.8499984741211, 85.1500015258789, 84.5999984741211, 84.55000305175781, 84.0999984741211, 84.6500015258789, 84.5999984741211, 84.4000015258789, 84.94999694824219, 84.8499984741211, 81.80000305175781, 84.30000305175781, 84.69999694824219, 83.25, 84.80000305175781, 84.5, 84.94999694824219, 83.9000015258789, 85.0999984741211, 84.75, 84.1500015258789, 84.8499984741211, 84.6500015258789, 83.19999694824219, 84.1500015258789, 83.55000305175781, 84.44999694824219, 84.5, 83.19999694824219, 84.30000305175781, 84.1500015258789, 84.4000015258789, 84.80000305175781, 84.3499984741211, 84.19999694824219, 83.19999694824219, 84.80000305175781, 84.5, 84.5999984741211, 82.4000015258789, 84.8499984741211, 85.0, 84.3499984741211, 85.0999984741211, 84.75, 84.75, 84.5, 84.5999984741211, 84.44999694824219, 84.9000015258789, 84.05000305175781, 84.94999694824219, 83.0999984741211, 82.4000015258789, 82.8499984741211, 84.1500015258789, 84.9000015258789, 84.9000015258789, 84.3499984741211, 85.1500015258789, 83.25, 85.19999694824219, 83.75, 84.55000305175781, 84.30000305175781, 84.1500015258789, 84.25, 82.80000305175781, 84.80000305175781, 84.25, 83.5999984741211, 83.44999694824219, 84.6500015258789, 85.30000305175781, 84.75, 85.1500015258789, 85.44999694824219, 84.4000015258789, 85.0999984741211, 84.25, 85.1500015258789, 84.94999694824219, 84.19999694824219, 83.6500015258789, 85.1500015258789, 85.1500015258789, 84.55000305175781, 84.44999694824219, 85.3499984741211, 84.3499984741211, 83.94999694824219, 85.3499984741211, 85.0999984741211, 85.1500015258789, 84.5, 84.6500015258789, 85.05000305175781, 85.19999694824219, 85.0, 85.0999984741211, 85.5999984741211, 85.30000305175781, 85.25, 85.75, 85.69999694824219, 85.4000015258789, 85.4000015258789, 85.0, 85.0, 85.19999694824219, 85.25, 85.05000305175781, 85.44999694824219, 85.1500015258789, 85.55000305175781, 85.05000305175781, 85.19999694824219, 85.5999984741211, 85.5, 84.8499984741211, 85.80000305175781, 85.19999694824219, 85.30000305175781, 85.1500015258789, 85.44999694824219, 85.30000305175781, 85.0, 85.19999694824219, 85.30000305175781, 85.25, 84.30000305175781, 84.8499984741211, 85.1500015258789, 85.5, 85.05000305175781, 85.1500015258789, 85.3499984741211, 85.25, 85.3499984741211, 85.30000305175781, 85.25, 85.30000305175781, 85.4000015258789, 84.44999694824219, 85.75, 85.80000305175781] \n",
    "Cluster_1_accuracy_50=[50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.900001525878906, 51.29999923706055, 56.150001525878906, 50.20000076293945, 50.25, 50.70000076293945, 50.150001525878906, 50.0, 50.95000076293945, 50.0, 49.900001525878906, 50.54999923706055, 50.599998474121094, 51.349998474121094, 50.900001525878906, 51.25, 51.20000076293945, 50.099998474121094, 50.04999923706055, 45.849998474121094, 46.75, 44.849998474121094, 48.54999923706055, 49.54999923706055, 50.0, 49.400001525878906, 50.25, 50.20000076293945, 50.0, 50.0, 50.0, 50.0, 50.0, 50.04999923706055, 55.29999923706055, 57.79999923706055, 50.0, 50.900001525878906, 51.75, 52.75, 57.75, 56.650001525878906, 58.150001525878906, 50.150001525878906, 51.70000076293945, 59.45000076293945, 60.0, 58.099998474121094, 59.900001525878906, 60.25, 59.45000076293945, 60.54999923706055, 57.150001525878906, 59.70000076293945, 60.5, 59.79999923706055, 58.599998474121094, 59.849998474121094, 59.79999923706055, 57.20000076293945, 54.45000076293945, 58.79999923706055, 61.349998474121094, 54.849998474121094, 58.650001525878906, 62.04999923706055, 62.849998474121094, 62.349998474121094, 59.5, 58.900001525878906, 63.20000076293945, 58.599998474121094, 62.75, 63.70000076293945, 64.3499984741211, 63.29999923706055, 61.45000076293945, 63.400001525878906, 57.900001525878906, 58.599998474121094, 62.900001525878906, 60.95000076293945, 64.0, 65.0999984741211, 63.0, 64.0999984741211, 61.75, 63.70000076293945, 61.5, 57.900001525878906, 57.150001525878906, 62.349998474121094, 61.099998474121094, 63.849998474121094, 64.1500015258789, 64.1500015258789, 64.5999984741211, 64.25, 64.8499984741211, 63.79999923706055, 64.80000305175781, 65.44999694824219, 64.5999984741211, 64.80000305175781, 65.05000305175781, 62.150001525878906, 63.599998474121094, 62.400001525878906, 63.900001525878906, 62.29999923706055, 63.25, 64.55000305175781, 65.8499984741211, 64.8499984741211, 61.150001525878906, 61.20000076293945, 65.69999694824219, 64.69999694824219, 65.5999984741211, 66.25, 63.95000076293945, 65.19999694824219, 64.94999694824219, 65.5, 65.75, 63.79999923706055, 65.80000305175781, 65.55000305175781, 66.05000305175781, 65.6500015258789, 63.95000076293945, 65.94999694824219, 61.75, 64.75, 65.19999694824219, 65.19999694824219, 64.94999694824219, 66.0, 66.1500015258789, 66.0999984741211, 65.8499984741211, 65.30000305175781, 65.69999694824219, 65.9000015258789, 64.0999984741211, 66.1500015258789, 65.75, 62.29999923706055, 66.05000305175781, 66.25, 66.4000015258789, 66.5999984741211, 65.3499984741211, 65.0999984741211, 64.80000305175781, 64.94999694824219, 66.69999694824219, 65.0, 65.19999694824219, 66.6500015258789, 65.69999694824219, 66.69999694824219, 65.69999694824219, 66.25, 66.25, 65.0, 65.75, 65.55000305175781, 66.5, 65.44999694824219, 65.44999694824219, 65.6500015258789, 65.9000015258789, 66.3499984741211, 61.349998474121094, 66.44999694824219, 66.3499984741211, 66.3499984741211, 66.6500015258789, 65.5, 65.5999984741211, 65.5999984741211, 67.30000305175781, 67.0, 66.44999694824219, 66.5, 68.05000305175781, 67.6500015258789, 63.20000076293945, 65.55000305175781, 62.5, 66.3499984741211, 66.5999984741211, 66.30000305175781, 67.6500015258789, 68.4000015258789, 68.1500015258789, 68.0, 67.5999984741211, 68.1500015258789, 68.0, 68.44999694824219, 68.6500015258789, 68.75, 64.0999984741211, 69.0999984741211, 68.8499984741211, 69.3499984741211, 67.5, 67.3499984741211, 67.19999694824219, 69.5, 70.25, 70.6500015258789, 67.30000305175781, 70.69999694824219, 69.5999984741211, 70.8499984741211, 70.30000305175781, 69.94999694824219, 69.94999694824219, 69.55000305175781, 66.94999694824219, 70.0, 69.19999694824219, 65.55000305175781, 69.1500015258789, 70.25, 69.75, 69.8499984741211, 68.55000305175781, 68.4000015258789, 68.30000305175781, 69.4000015258789, 69.05000305175781, 70.44999694824219, 68.30000305175781, 70.6500015258789, 68.75, 71.0, 68.0, 64.3499984741211, 65.94999694824219, 68.80000305175781, 70.5, 69.9000015258789, 69.69999694824219, 70.5999984741211, 68.9000015258789, 70.5, 69.94999694824219, 69.55000305175781, 70.19999694824219, 69.8499984741211, 71.30000305175781, 69.0, 68.80000305175781, 69.5, 69.19999694824219, 70.30000305175781, 68.69999694824219, 69.9000015258789, 70.8499984741211, 70.80000305175781, 68.25, 71.5999984741211, 69.6500015258789, 66.5999984741211, 70.30000305175781, 69.94999694824219, 70.5, 70.19999694824219, 69.0, 68.6500015258789, 70.30000305175781, 70.80000305175781, 70.30000305175781, 69.25, 70.8499984741211, 70.44999694824219, 70.44999694824219, 70.44999694824219, 70.6500015258789, 71.44999694824219, 71.69999694824219, 71.1500015258789, 69.80000305175781, 66.4000015258789, 69.0, 70.55000305175781, 66.44999694824219, 69.4000015258789, 71.19999694824219, 71.25, 71.30000305175781, 71.3499984741211, 72.0999984741211, 71.0999984741211, 71.6500015258789, 72.30000305175781, 72.0, 71.5, 71.5999984741211, 72.1500015258789, 72.0, 72.1500015258789, 71.6500015258789, 71.19999694824219, 69.19999694824219, 71.69999694824219, 70.94999694824219, 71.25, 71.94999694824219, 71.44999694824219, 71.19999694824219, 71.55000305175781, 70.30000305175781, 71.5, 64.05000305175781, 71.1500015258789, 71.30000305175781, 71.05000305175781, 71.8499984741211, 72.0999984741211, 72.05000305175781, 68.1500015258789, 70.69999694824219, 71.69999694824219, 71.6500015258789, 72.44999694824219, 71.25, 71.5999984741211, 71.0999984741211, 71.8499984741211, 70.75, 71.5999984741211, 71.5, 70.6500015258789, 71.05000305175781, 71.80000305175781, 71.6500015258789, 72.0999984741211, 72.05000305175781, 72.05000305175781, 71.80000305175781, 71.8499984741211, 71.94999694824219, 71.9000015258789, 71.19999694824219, 70.80000305175781, 63.400001525878906, 71.25, 71.55000305175781, 67.1500015258789, 71.05000305175781, 71.4000015258789, 72.19999694824219, 71.55000305175781, 69.44999694824219, 71.75, 72.5999984741211, 72.1500015258789, 71.8499984741211, 71.9000015258789, 71.55000305175781, 71.44999694824219, 71.94999694824219, 71.5999984741211, 72.30000305175781, 72.4000015258789, 72.44999694824219, 72.4000015258789, 72.9000015258789, 72.5999984741211, 72.0, 72.30000305175781, 72.55000305175781, 70.1500015258789, 72.3499984741211, 71.69999694824219, 72.1500015258789, 72.0, 72.05000305175781, 71.75, 71.8499984741211, 72.1500015258789, 71.8499984741211, 72.1500015258789, 71.19999694824219, 71.75, 72.0999984741211, 72.05000305175781, 71.1500015258789, 71.6500015258789, 65.3499984741211, 72.0, 68.30000305175781, 72.0, 70.19999694824219, 69.6500015258789, 70.6500015258789, 68.30000305175781, 70.44999694824219, 71.75, 71.0, 71.5, 71.30000305175781, 71.94999694824219, 71.94999694824219, 72.05000305175781, 71.4000015258789, 71.5999984741211, 70.1500015258789, 68.9000015258789, 69.80000305175781, 71.55000305175781, 71.94999694824219, 70.80000305175781, 67.05000305175781, 70.5, 71.80000305175781, 71.80000305175781, 71.9000015258789, 71.05000305175781, 72.19999694824219, 71.94999694824219, 71.5, 69.30000305175781, 72.25, 72.0999984741211, 71.94999694824219, 72.0, 71.25, 71.55000305175781, 72.3499984741211, 72.05000305175781, 71.9000015258789, 71.0999984741211, 71.4000015258789, 70.94999694824219, 71.94999694824219, 70.4000015258789, 72.5999984741211, 70.5, 71.8499984741211, 72.1500015258789, 72.05000305175781, 72.0999984741211, 71.69999694824219, 71.8499984741211, 71.5999984741211, 69.0999984741211, 71.5999984741211, 71.3499984741211, 71.6500015258789, 71.55000305175781, 71.6500015258789, 71.6500015258789, 71.69999694824219, 70.30000305175781, 71.44999694824219, 70.8499984741211, 71.1500015258789, 71.0, 71.0999984741211, 71.94999694824219, 72.25, 71.69999694824219, 71.3499984741211, 70.75, 71.19999694824219, 70.8499984741211, 71.55000305175781, 71.0, 69.44999694824219, 70.25, 71.5, 71.44999694824219, 71.75, 71.4000015258789, 71.4000015258789, 71.69999694824219, 69.8499984741211, 71.69999694824219, 71.8499984741211, 71.5999984741211, 71.44999694824219, 71.6500015258789, 70.5999984741211, 70.05000305175781, 70.94999694824219, 69.0999984741211, 71.19999694824219, 71.0999984741211] \n",
    "Cluster_2_accuracy_50=[2.5, 44.54999923706055, 49.849998474121094, 49.95000076293945, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.04999923706055, 50.04999923706055, 50.04999923706055, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 58.400001525878906, 53.099998474121094, 54.5, 55.25, 50.150001525878906, 53.79999923706055, 51.5, 53.54999923706055, 50.849998474121094, 50.400001525878906, 50.20000076293945, 62.54999923706055, 50.849998474121094, 63.650001525878906, 61.5, 50.150001525878906, 53.45000076293945, 52.75, 65.05000305175781, 50.04999923706055, 51.29999923706055, 62.95000076293945, 66.1500015258789, 65.0999984741211, 66.6500015258789, 64.3499984741211, 61.5, 61.099998474121094, 66.1500015258789, 65.0, 65.3499984741211, 65.4000015258789, 65.5999984741211, 64.05000305175781, 66.55000305175781, 62.75, 63.70000076293945, 63.099998474121094, 64.1500015258789, 60.400001525878906, 66.5999984741211, 66.5999984741211, 65.25, 66.4000015258789, 67.0999984741211, 64.0, 66.1500015258789, 67.0, 66.69999694824219, 67.0999984741211, 67.05000305175781, 67.55000305175781, 67.44999694824219, 66.9000015258789, 67.4000015258789, 67.5, 66.55000305175781, 67.55000305175781, 64.5999984741211, 67.5, 67.8499984741211, 68.0, 67.30000305175781, 67.5, 67.94999694824219, 67.5999984741211, 66.25, 67.44999694824219, 67.9000015258789, 67.1500015258789, 67.69999694824219, 68.1500015258789, 68.30000305175781, 67.5, 67.55000305175781, 67.44999694824219, 67.80000305175781, 68.1500015258789, 67.1500015258789, 68.25, 68.5, 68.3499984741211, 68.5999984741211, 68.44999694824219, 68.9000015258789, 68.55000305175781, 67.05000305175781, 69.3499984741211, 68.9000015258789, 66.05000305175781, 65.69999694824219, 66.8499984741211, 69.05000305175781, 66.69999694824219, 69.55000305175781, 70.1500015258789, 66.9000015258789, 69.0, 70.05000305175781, 68.3499984741211, 67.0999984741211, 70.0999984741211, 68.05000305175781, 70.19999694824219, 70.75, 70.6500015258789, 69.9000015258789, 70.30000305175781, 68.5, 70.30000305175781, 67.1500015258789, 68.8499984741211, 70.1500015258789, 70.30000305175781, 70.69999694824219, 70.55000305175781, 69.9000015258789, 70.80000305175781, 70.5, 70.5999984741211, 68.0999984741211, 71.0, 70.75, 71.05000305175781, 71.44999694824219, 70.19999694824219, 71.8499984741211, 72.0, 69.3499984741211, 69.69999694824219, 71.4000015258789, 71.8499984741211, 70.4000015258789, 71.75, 72.0999984741211, 72.0, 67.94999694824219, 72.69999694824219, 72.5, 70.80000305175781, 72.6500015258789, 72.4000015258789, 71.19999694824219, 72.19999694824219, 73.6500015258789, 73.19999694824219, 73.4000015258789, 73.05000305175781, 70.94999694824219, 73.0, 72.4000015258789, 73.25, 71.25, 73.44999694824219, 73.19999694824219, 73.3499984741211, 72.55000305175781, 72.69999694824219, 72.9000015258789, 72.30000305175781, 71.80000305175781, 73.5, 73.69999694824219, 72.8499984741211, 73.6500015258789, 74.0, 73.4000015258789, 73.8499984741211, 72.80000305175781, 73.6500015258789, 73.30000305175781, 73.5999984741211, 74.0, 74.05000305175781, 73.75, 74.25, 73.69999694824219, 73.5, 74.1500015258789, 74.69999694824219, 74.0999984741211, 74.3499984741211, 73.5999984741211, 73.44999694824219, 74.05000305175781, 74.1500015258789, 74.4000015258789, 74.19999694824219, 73.75, 74.4000015258789, 74.05000305175781, 74.0, 72.9000015258789, 72.94999694824219, 73.69999694824219, 73.94999694824219, 73.55000305175781, 74.25, 74.80000305175781, 74.25, 74.3499984741211, 74.55000305175781, 74.55000305175781, 75.5999984741211, 75.75, 74.94999694824219, 75.44999694824219, 75.75, 74.5999984741211, 75.19999694824219, 74.9000015258789, 75.80000305175781, 73.5, 74.5, 75.1500015258789, 74.55000305175781, 75.4000015258789, 75.30000305175781, 75.0, 74.9000015258789, 75.0, 75.4000015258789, 74.80000305175781, 74.80000305175781, 75.4000015258789, 75.69999694824219, 75.5, 75.4000015258789, 75.80000305175781, 75.30000305175781, 75.6500015258789, 75.25, 75.8499984741211, 75.69999694824219, 75.1500015258789, 75.44999694824219, 75.44999694824219, 75.0999984741211, 75.5, 76.0, 76.44999694824219, 75.05000305175781, 75.94999694824219, 75.44999694824219, 75.4000015258789, 75.9000015258789, 75.8499984741211, 76.25, 74.5999984741211, 75.6500015258789, 76.0999984741211, 75.69999694824219, 75.8499984741211, 75.0999984741211, 75.9000015258789, 75.19999694824219, 75.75, 76.4000015258789, 76.44999694824219, 75.80000305175781, 75.55000305175781, 76.0, 75.94999694824219, 75.5999984741211, 75.55000305175781, 75.6500015258789, 75.0, 74.5999984741211, 75.30000305175781, 74.5999984741211, 75.0, 76.5, 75.0999984741211, 75.3499984741211, 76.5, 75.3499984741211, 75.55000305175781, 74.94999694824219, 75.69999694824219, 74.5, 74.05000305175781, 75.5999984741211, 75.3499984741211, 75.80000305175781, 73.5, 76.0999984741211, 76.3499984741211, 75.3499984741211, 75.80000305175781, 75.05000305175781, 75.19999694824219, 75.30000305175781, 75.3499984741211, 74.8499984741211, 74.80000305175781, 74.75, 74.4000015258789, 74.6500015258789, 75.9000015258789, 76.5, 76.80000305175781, 76.30000305175781, 75.69999694824219, 76.0999984741211, 75.8499984741211, 76.1500015258789, 76.1500015258789, 75.8499984741211, 75.4000015258789, 76.0999984741211, 75.94999694824219, 75.8499984741211, 76.1500015258789, 75.80000305175781, 76.1500015258789, 76.1500015258789, 75.5999984741211, 75.55000305175781, 75.44999694824219, 75.9000015258789, 75.6500015258789, 76.1500015258789, 75.3499984741211, 75.3499984741211, 75.75, 74.80000305175781, 76.0, 75.5999984741211, 75.80000305175781, 75.6500015258789, 75.80000305175781, 76.05000305175781, 76.5999984741211, 76.55000305175781, 76.3499984741211, 75.75, 76.75, 76.0999984741211, 75.75, 76.0, 75.8499984741211, 75.55000305175781, 76.0999984741211, 76.05000305175781, 75.19999694824219, 76.3499984741211, 76.0999984741211, 75.4000015258789, 76.0999984741211, 75.5999984741211, 76.75, 76.30000305175781, 76.6500015258789, 76.3499984741211, 76.75, 76.25, 76.6500015258789, 76.6500015258789, 76.80000305175781, 76.4000015258789, 76.25, 75.94999694824219, 76.25, 77.0, 77.3499984741211, 76.5999984741211, 76.55000305175781, 76.1500015258789, 76.30000305175781, 76.5, 77.1500015258789, 76.44999694824219, 76.5, 76.25, 75.6500015258789, 75.75, 75.94999694824219, 76.1500015258789, 75.9000015258789, 76.1500015258789, 76.5, 76.05000305175781, 75.8499984741211, 75.44999694824219, 76.5999984741211, 76.44999694824219, 76.44999694824219, 75.80000305175781, 76.25, 75.5999984741211, 75.55000305175781, 76.05000305175781, 75.75, 75.8499984741211, 75.6500015258789, 75.3499984741211, 75.75, 74.69999694824219, 75.19999694824219, 75.5, 75.94999694824219, 75.05000305175781, 75.9000015258789, 75.55000305175781, 75.80000305175781, 76.0, 76.0999984741211, 75.94999694824219, 76.05000305175781, 76.19999694824219, 76.05000305175781, 76.0, 76.25, 76.05000305175781, 75.80000305175781, 75.94999694824219, 75.80000305175781, 75.75, 75.5, 76.1500015258789, 75.9000015258789, 75.6500015258789, 75.69999694824219, 76.4000015258789, 76.5, 76.05000305175781, 76.1500015258789, 76.05000305175781, 75.44999694824219, 76.19999694824219, 75.9000015258789, 75.94999694824219, 75.80000305175781, 75.9000015258789, 75.5999984741211, 75.55000305175781, 75.4000015258789, 75.69999694824219, 74.80000305175781, 76.25, 75.30000305175781, 75.05000305175781, 76.0, 75.5999984741211, 76.30000305175781, 76.4000015258789, 75.80000305175781, 76.0, 75.9000015258789, 75.80000305175781, 75.69999694824219, 75.8499984741211, 76.19999694824219, 76.1500015258789, 76.05000305175781, 75.75, 76.5999984741211, 75.6500015258789, 76.30000305175781, 76.3499984741211, 75.9000015258789, 76.25, 76.5, 77.19999694824219, 76.55000305175781, 76.69999694824219, 76.5999984741211, 76.4000015258789] \n",
    "Cluster_3_accuracy_50=[20.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 49.95000076293945, 56.95000076293945, 54.599998474121094, 66.30000305175781, 63.20000076293945, 67.75, 68.5, 65.5999984741211, 67.69999694824219, 68.19999694824219, 56.20000076293945, 56.150001525878906, 57.849998474121094, 68.3499984741211, 70.0999984741211, 66.05000305175781, 62.70000076293945, 58.599998474121094, 65.94999694824219, 57.900001525878906, 70.80000305175781, 70.8499984741211, 70.44999694824219, 70.25, 71.1500015258789, 71.4000015258789, 71.1500015258789, 70.4000015258789, 71.0999984741211, 71.75, 65.55000305175781, 72.44999694824219, 73.05000305175781, 72.8499984741211, 72.80000305175781, 72.3499984741211, 72.69999694824219, 73.4000015258789, 66.69999694824219, 73.4000015258789, 72.6500015258789, 73.44999694824219, 74.55000305175781, 74.9000015258789, 73.94999694824219, 72.1500015258789, 74.69999694824219, 75.55000305175781, 75.0999984741211, 73.94999694824219, 73.30000305175781, 75.55000305175781, 76.0999984741211, 77.0999984741211, 76.75, 76.5999984741211, 76.80000305175781, 77.25, 77.19999694824219, 77.0999984741211, 77.3499984741211, 77.9000015258789, 77.69999694824219, 77.94999694824219, 76.6500015258789, 78.44999694824219, 78.6500015258789, 78.05000305175781, 74.9000015258789, 78.5999984741211, 78.30000305175781, 78.8499984741211, 79.6500015258789, 79.6500015258789, 78.80000305175781, 79.5, 77.0, 78.05000305175781, 78.1500015258789, 79.19999694824219, 79.19999694824219, 79.19999694824219, 79.3499984741211, 78.55000305175781, 79.0999984741211, 79.55000305175781, 79.6500015258789, 79.5, 79.5999984741211, 79.9000015258789, 80.0, 80.0, 79.4000015258789, 76.69999694824219, 79.3499984741211, 80.1500015258789, 80.30000305175781, 81.0999984741211, 80.80000305175781, 80.94999694824219, 80.6500015258789, 80.44999694824219, 81.0999984741211, 80.94999694824219, 80.80000305175781, 78.25, 80.80000305175781, 81.0999984741211, 80.5, 80.80000305175781, 80.8499984741211, 81.1500015258789, 81.0, 81.5, 81.44999694824219, 80.9000015258789, 81.25, 81.4000015258789, 81.25, 81.1500015258789, 81.5999984741211, 80.55000305175781, 81.8499984741211, 81.4000015258789, 82.25, 81.05000305175781, 81.69999694824219, 82.0999984741211, 81.5, 81.25, 81.05000305175781, 81.9000015258789, 81.4000015258789, 82.0, 81.94999694824219, 81.30000305175781, 81.6500015258789, 81.44999694824219, 81.80000305175781, 81.9000015258789, 81.69999694824219, 81.1500015258789, 81.6500015258789, 81.8499984741211, 81.94999694824219, 81.44999694824219, 80.8499984741211, 81.0, 82.0999984741211, 81.44999694824219, 81.8499984741211, 81.94999694824219, 79.80000305175781, 82.0, 81.1500015258789, 81.94999694824219, 82.55000305175781, 80.94999694824219, 82.19999694824219, 82.5999984741211, 81.44999694824219, 82.5999984741211, 82.19999694824219, 82.44999694824219, 81.9000015258789, 82.1500015258789, 80.05000305175781, 82.0, 81.5, 82.3499984741211, 81.5999984741211, 81.80000305175781, 81.4000015258789, 81.44999694824219, 82.19999694824219, 81.9000015258789, 79.19999694824219, 82.3499984741211, 81.4000015258789, 82.44999694824219, 82.80000305175781, 82.8499984741211, 80.5999984741211, 82.8499984741211, 81.94999694824219, 82.9000015258789, 83.25, 82.4000015258789, 81.5999984741211, 82.8499984741211, 83.44999694824219, 82.5999984741211, 82.5999984741211, 83.25, 82.8499984741211, 83.1500015258789, 81.05000305175781, 83.0, 83.44999694824219, 81.94999694824219, 83.25, 83.19999694824219, 82.0999984741211, 82.4000015258789, 83.19999694824219, 83.25, 83.5999984741211, 80.94999694824219, 83.44999694824219, 83.05000305175781, 83.8499984741211, 81.9000015258789, 83.69999694824219, 83.0999984741211, 82.8499984741211, 82.44999694824219, 82.94999694824219, 83.9000015258789, 83.55000305175781, 83.30000305175781, 83.6500015258789, 83.5999984741211, 83.69999694824219, 83.44999694824219, 82.94999694824219, 82.80000305175781, 83.05000305175781, 83.5, 81.1500015258789, 82.80000305175781, 83.0999984741211, 82.9000015258789, 83.8499984741211, 82.9000015258789, 82.5999984741211, 82.94999694824219, 83.9000015258789, 84.0, 84.1500015258789, 82.0999984741211, 83.19999694824219, 83.5, 84.0999984741211, 82.94999694824219, 83.69999694824219, 84.05000305175781, 83.44999694824219, 83.8499984741211, 84.05000305175781, 83.75, 83.25, 82.5999984741211, 83.55000305175781, 83.4000015258789, 82.1500015258789, 83.75, 83.0999984741211, 83.69999694824219, 84.1500015258789, 82.0999984741211, 82.0999984741211, 83.55000305175781, 83.94999694824219, 82.80000305175781, 83.44999694824219, 83.94999694824219, 83.4000015258789, 84.1500015258789, 84.0, 83.5999984741211, 82.80000305175781, 84.44999694824219, 84.30000305175781, 83.75, 83.30000305175781, 83.19999694824219, 83.3499984741211, 84.05000305175781, 82.5999984741211, 83.94999694824219, 82.55000305175781, 83.94999694824219, 84.3499984741211, 84.0, 84.0, 84.3499984741211, 83.05000305175781, 83.94999694824219, 84.4000015258789, 83.19999694824219, 84.1500015258789, 83.5, 83.94999694824219, 84.19999694824219, 80.94999694824219, 84.19999694824219, 84.05000305175781, 83.5, 84.19999694824219, 83.69999694824219, 83.8499984741211, 84.05000305175781, 84.0, 84.4000015258789, 84.4000015258789, 84.5, 84.30000305175781, 84.5999984741211, 84.5, 82.75, 84.44999694824219, 84.25, 83.9000015258789, 84.30000305175781, 82.05000305175781, 79.30000305175781, 84.3499984741211, 84.0999984741211, 84.25, 84.30000305175781, 83.75, 84.30000305175781, 83.5999984741211, 83.75, 84.25, 84.3499984741211, 84.19999694824219, 84.05000305175781, 84.0999984741211, 82.0999984741211, 84.5999984741211, 84.0999984741211, 84.1500015258789, 84.94999694824219, 84.8499984741211, 84.5999984741211, 84.55000305175781, 84.9000015258789, 84.94999694824219, 85.0999984741211, 85.30000305175781, 85.1500015258789, 84.6500015258789, 84.69999694824219, 84.3499984741211, 84.8499984741211, 85.05000305175781, 85.0, 85.05000305175781, 84.75, 85.1500015258789, 85.05000305175781, 84.5999984741211, 85.05000305175781, 84.0999984741211, 84.55000305175781, 84.6500015258789, 84.8499984741211, 84.5999984741211, 84.6500015258789, 84.44999694824219, 84.5, 84.4000015258789, 84.6500015258789, 84.8499984741211, 84.80000305175781, 85.0999984741211, 85.25, 84.69999694824219, 84.3499984741211, 84.9000015258789, 85.25, 84.69999694824219, 85.25, 84.5999984741211, 85.1500015258789, 85.25, 84.94999694824219, 84.9000015258789, 85.19999694824219, 85.0, 85.25, 85.0999984741211, 85.0, 85.1500015258789, 85.05000305175781, 85.1500015258789, 84.80000305175781, 83.69999694824219, 85.44999694824219, 85.75, 85.05000305175781, 84.30000305175781, 84.9000015258789, 85.44999694824219, 85.6500015258789, 84.30000305175781, 85.0999984741211, 85.05000305175781, 84.30000305175781, 85.4000015258789, 85.4000015258789, 85.4000015258789, 85.5999984741211, 85.8499984741211, 83.6500015258789, 82.0999984741211, 85.9000015258789, 84.69999694824219, 85.4000015258789, 83.44999694824219, 84.75, 85.19999694824219, 85.6500015258789, 84.9000015258789, 85.30000305175781, 85.94999694824219, 85.55000305175781, 85.9000015258789, 84.4000015258789, 85.55000305175781, 85.8499984741211, 85.6500015258789, 85.80000305175781, 85.94999694824219, 84.9000015258789, 85.25, 85.25, 85.80000305175781, 85.55000305175781, 86.0999984741211, 85.9000015258789, 85.94999694824219, 86.30000305175781, 85.69999694824219, 85.5999984741211, 85.94999694824219, 85.19999694824219, 85.9000015258789, 85.19999694824219, 85.5999984741211, 85.94999694824219, 84.9000015258789, 86.25, 86.0999984741211, 86.4000015258789, 86.30000305175781, 85.80000305175781, 85.1500015258789, 86.0, 85.9000015258789, 86.05000305175781, 86.19999694824219, 86.30000305175781, 86.4000015258789, 86.55000305175781, 86.4000015258789, 86.25, 85.94999694824219, 86.0999984741211]\n",
    "Cluster_4_accuracy_50=[50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.150001525878906, 50.25, 50.20000076293945, 50.0, 50.0, 50.0, 50.0, 50.04999923706055, 50.099998474121094, 53.04999923706055, 52.599998474121094, 52.849998474121094, 50.099998474121094, 54.04999923706055, 53.849998474121094, 53.349998474121094, 52.04999923706055, 53.25, 57.29999923706055, 53.54999923706055, 65.75, 61.650001525878906, 66.5, 64.5, 63.95000076293945, 64.25, 65.6500015258789, 61.400001525878906, 62.150001525878906, 68.69999694824219, 68.94999694824219, 66.1500015258789, 68.94999694824219, 66.19999694824219, 68.3499984741211, 64.94999694824219, 67.25, 70.05000305175781, 69.44999694824219, 66.25, 69.05000305175781, 70.0999984741211, 66.4000015258789, 67.44999694824219, 68.75, 70.3499984741211, 66.44999694824219, 70.1500015258789, 70.5, 71.30000305175781, 71.25, 67.44999694824219, 69.0, 67.69999694824219, 69.55000305175781, 69.75, 71.75, 70.3499984741211, 71.05000305175781, 68.55000305175781, 72.0999984741211, 71.6500015258789, 71.69999694824219, 71.9000015258789, 71.1500015258789, 70.8499984741211, 71.69999694824219, 71.9000015258789, 68.6500015258789, 71.55000305175781, 67.94999694824219, 72.69999694824219, 70.94999694824219, 72.44999694824219, 67.9000015258789, 70.0999984741211, 72.6500015258789, 72.5999984741211, 72.9000015258789, 71.6500015258789, 70.9000015258789, 73.0999984741211, 73.30000305175781, 71.25, 71.19999694824219, 72.25, 71.25, 68.25, 68.8499984741211, 73.1500015258789, 73.19999694824219, 69.8499984741211, 72.69999694824219, 72.5999984741211, 72.0, 72.69999694824219, 72.25, 73.0, 73.6500015258789, 72.8499984741211, 72.1500015258789, 72.8499984741211, 73.19999694824219, 72.80000305175781, 73.5, 74.0, 74.25, 70.4000015258789, 72.30000305175781, 74.25, 74.19999694824219, 73.94999694824219, 72.94999694824219, 74.1500015258789, 72.9000015258789, 74.44999694824219, 74.5, 74.4000015258789, 73.55000305175781, 73.4000015258789, 74.8499984741211, 75.0, 74.75, 73.19999694824219, 75.1500015258789, 74.94999694824219, 74.69999694824219, 74.5999984741211, 75.1500015258789, 75.44999694824219, 74.55000305175781, 75.4000015258789, 75.44999694824219, 75.19999694824219, 75.5, 74.0, 75.0999984741211, 75.25, 74.9000015258789, 75.3499984741211, 75.44999694824219, 76.4000015258789, 73.80000305175781, 75.9000015258789, 75.94999694824219, 75.80000305175781, 76.05000305175781, 75.75, 76.19999694824219, 75.69999694824219, 75.3499984741211, 75.80000305175781, 75.25, 75.5999984741211, 73.0, 75.9000015258789, 76.30000305175781, 76.3499984741211, 75.3499984741211, 76.44999694824219, 76.19999694824219, 76.0999984741211, 76.5, 76.30000305175781, 76.0, 76.25, 76.0999984741211, 76.44999694824219, 76.19999694824219, 75.94999694824219, 76.44999694824219, 77.05000305175781, 76.6500015258789, 76.6500015258789, 77.0999984741211, 77.05000305175781, 76.94999694824219, 76.94999694824219, 77.1500015258789, 76.80000305175781, 77.6500015258789, 76.94999694824219, 77.55000305175781, 77.9000015258789, 77.30000305175781, 78.1500015258789, 77.3499984741211, 77.75, 77.5, 77.4000015258789, 76.8499984741211, 77.30000305175781, 77.05000305175781, 77.0, 77.44999694824219, 77.19999694824219, 77.9000015258789, 77.25, 77.75, 78.0, 77.75, 77.75, 77.75, 77.5, 78.3499984741211, 77.6500015258789, 77.6500015258789, 78.4000015258789, 78.5, 77.9000015258789, 77.94999694824219, 77.05000305175781, 79.3499984741211, 79.0, 78.55000305175781, 78.5999984741211, 77.19999694824219, 78.55000305175781, 78.9000015258789, 79.1500015258789, 78.8499984741211, 79.0999984741211, 79.0, 79.3499984741211, 77.8499984741211, 79.3499984741211, 78.4000015258789, 79.30000305175781, 79.0999984741211, 79.55000305175781, 79.55000305175781, 79.3499984741211, 79.0, 79.4000015258789, 79.55000305175781, 79.80000305175781, 79.55000305175781, 80.05000305175781, 79.9000015258789, 80.05000305175781, 79.69999694824219, 79.94999694824219, 79.6500015258789, 79.9000015258789, 79.30000305175781, 79.3499984741211, 79.94999694824219, 79.8499984741211, 78.8499984741211, 79.94999694824219, 79.5, 79.8499984741211, 79.5999984741211, 79.55000305175781, 79.5, 79.6500015258789, 79.5, 79.55000305175781, 79.8499984741211, 79.5999984741211, 80.1500015258789, 79.94999694824219, 79.8499984741211, 79.80000305175781, 80.19999694824219, 80.44999694824219, 79.9000015258789, 80.5999984741211, 80.05000305175781, 80.6500015258789, 80.6500015258789, 80.6500015258789, 79.75, 79.4000015258789, 80.44999694824219, 80.3499984741211, 80.30000305175781, 80.9000015258789, 80.6500015258789, 80.55000305175781, 80.3499984741211, 81.0, 81.05000305175781, 80.69999694824219, 79.9000015258789, 80.8499984741211, 80.5999984741211, 81.5, 80.44999694824219, 81.19999694824219, 81.0999984741211, 81.05000305175781, 81.19999694824219, 81.4000015258789, 81.30000305175781, 80.9000015258789, 81.5999984741211, 81.44999694824219, 81.30000305175781, 82.05000305175781, 81.8499984741211, 81.75, 82.0, 81.69999694824219, 81.69999694824219, 80.9000015258789, 81.94999694824219, 81.69999694824219, 81.44999694824219, 81.5999984741211, 80.80000305175781, 81.25, 81.75, 81.5999984741211, 80.80000305175781, 82.0, 81.3499984741211, 81.30000305175781, 81.69999694824219, 82.05000305175781, 81.05000305175781, 77.6500015258789, 81.94999694824219, 81.55000305175781, 81.94999694824219, 82.0999984741211, 81.80000305175781, 81.75, 81.8499984741211, 81.8499984741211, 82.0, 81.6500015258789, 82.0999984741211, 82.0, 81.3499984741211, 81.44999694824219, 81.6500015258789, 82.30000305175781, 81.6500015258789, 82.19999694824219, 81.94999694824219, 80.69999694824219, 82.3499984741211, 82.0999984741211, 82.0, 81.5999984741211, 82.0999984741211, 82.3499984741211, 81.9000015258789, 82.5, 82.05000305175781, 82.5999984741211, 82.5, 82.0999984741211, 82.1500015258789, 82.44999694824219, 82.69999694824219, 82.3499984741211, 81.75, 83.0999984741211, 82.9000015258789, 82.6500015258789, 82.0999984741211, 82.3499984741211, 82.5999984741211, 82.44999694824219, 82.80000305175781, 82.80000305175781, 82.5999984741211, 82.6500015258789, 83.0999984741211, 82.75, 81.69999694824219, 83.0, 82.19999694824219, 82.55000305175781, 82.8499984741211, 80.5999984741211, 83.05000305175781, 83.05000305175781, 82.6500015258789, 82.30000305175781, 82.8499984741211, 82.55000305175781, 82.5, 82.75, 82.80000305175781, 82.75, 82.75, 82.8499984741211, 83.1500015258789, 82.6500015258789, 82.94999694824219, 83.0999984741211, 83.19999694824219, 83.44999694824219, 82.6500015258789, 82.25, 83.19999694824219, 83.05000305175781, 83.30000305175781, 83.3499984741211, 83.19999694824219, 83.4000015258789, 83.55000305175781, 83.80000305175781, 83.6500015258789, 83.44999694824219, 83.69999694824219, 83.80000305175781, 83.80000305175781, 84.0, 83.69999694824219, 83.6500015258789, 83.8499984741211, 83.5999984741211, 83.69999694824219, 83.30000305175781, 83.4000015258789, 83.44999694824219, 83.30000305175781, 83.5999984741211, 83.44999694824219, 83.44999694824219, 83.8499984741211, 83.75, 83.5, 83.30000305175781, 83.5999984741211, 83.30000305175781, 83.30000305175781, 83.6500015258789, 83.69999694824219, 83.3499984741211, 83.6500015258789, 83.9000015258789, 83.55000305175781, 83.6500015258789, 83.69999694824219, 84.0999984741211, 84.19999694824219, 83.75, 83.69999694824219, 84.0, 83.69999694824219, 83.55000305175781, 83.75, 83.5999984741211, 83.6500015258789, 83.69999694824219, 83.69999694824219, 83.5999984741211, 83.05000305175781, 83.30000305175781, 83.69999694824219, 83.6500015258789, 83.5, 83.55000305175781, 83.6500015258789, 83.5999984741211, 83.5999984741211, 83.9000015258789, 83.0, 83.69999694824219, 83.19999694824219, 83.3499984741211, 84.0, 84.0, 83.8499984741211, 83.8499984741211, 83.9000015258789, 83.9000015258789, 83.6500015258789, 83.75, 83.69999694824219, 83.8499984741211]\n",
    "\n",
    "#50 antenna lr=0.001\n",
    "Cluster_0_accuracy_50_small=[49.75, 50.0, 46.95000076293945, 46.150001525878906, 3.0, 42.810001373291016, 40.38999876976013, 50.0, 55.150001525878906, 60.45000076293945, 66.19999694824219, 69.55000305175781, 55.45000076293945, 65.5, 69.19999694824219, 72.1500015258789, 64.4000015258789, 63.849998474121094, 62.099998474121094, 66.6500015258789, 66.3499984741211, 39.849998474121094, 60.849998474121094, 68.05000305175781, 59.150001525878906, 68.30000305175781, 67.1500015258789, 68.3499984741211, 68.8499984741211, 58.45000076293945, 71.8499984741211, 66.94999694824219, 68.4000015258789, 61.0, 64.44999694824219, 65.3499984741211, 62.0, 61.04999923706055, 63.70000076293945, 66.75, 59.150001525878906, 58.54999923706055, 65.69999694824219, 66.69999694824219, 63.400001525878906, 61.400001525878906, 62.20000076293945, 54.70000076293945, 64.30000305175781, 66.05000305175781, 62.349998474121094, 67.05000305175781, 65.1500015258789, 66.44999694824219, 66.8499984741211, 70.4000015258789, 69.1500015258789, 70.44999694824219, 62.5, 68.3499984741211, 66.4000015258789, 63.45000076293945, 64.80000305175781, 63.04999923706055, 67.75, 69.0, 67.75, 65.94999694824219, 66.5, 61.150001525878906, 62.20000076293945, 66.0, 66.25, 66.1500015258789, 65.55000305175781, 66.19999694824219, 65.3499984741211, 67.30000305175781, 69.25, 71.19999694824219, 71.44999694824219, 68.80000305175781, 68.30000305175781, 67.1500015258789, 67.3499984741211, 63.54999923706055, 63.150001525878906, 63.20000076293945, 67.4000015258789, 61.25, 56.79999923706055, 63.20000076293945, 63.45000076293945, 64.80000305175781, 66.9000015258789, 68.30000305175781, 63.349998474121094, 70.5, 68.5, 70.0, 70.5, 69.4000015258789, 69.0999984741211, 69.19999694824219, 68.19999694824219, 68.69999694824219, 71.44999694824219, 72.19999694824219, 72.69999694824219, 72.05000305175781, 71.75, 71.75, 69.9000015258789, 70.1500015258789, 72.9000015258789, 71.6500015258789, 69.8499984741211, 72.5999984741211, 66.0, 74.05000305175781, 72.3499984741211, 75.05000305175781, 69.8499984741211, 71.25, 71.5, 71.80000305175781, 68.5999984741211, 72.3499984741211, 72.9000015258789, 70.75, 66.94999694824219, 71.30000305175781, 71.9000015258789, 71.0999984741211, 69.44999694824219, 69.25, 68.5999984741211, 65.69999694824219, 65.5, 66.9000015258789, 53.349998474121094, 57.150001525878906, 64.5999984741211, 65.0, 67.44999694824219, 66.55000305175781, 70.5, 65.0, 68.1500015258789, 70.55000305175781, 69.44999694824219, 70.75, 66.69999694824219, 66.30000305175781, 68.30000305175781, 70.1500015258789, 62.650001525878906, 69.25, 67.30000305175781, 68.6500015258789, 64.8499984741211, 67.1500015258789, 70.5, 70.44999694824219, 68.94999694824219, 67.80000305175781, 68.44999694824219, 66.5, 71.6500015258789, 71.30000305175781, 71.25, 72.19999694824219, 74.3499984741211, 67.69999694824219, 71.19999694824219, 69.44999694824219, 68.30000305175781, 70.75, 64.5999984741211, 68.5999984741211, 63.25, 66.94999694824219, 71.25, 73.0, 71.94999694824219, 71.55000305175781, 73.9000015258789, 73.3499984741211, 71.4000015258789, 73.4000015258789, 73.8499984741211, 72.0, 69.8499984741211, 71.44999694824219, 61.95000076293945, 67.25, 67.05000305175781, 70.30000305175781, 66.4000015258789, 69.4000015258789, 69.94999694824219, 68.55000305175781, 71.1500015258789, 72.75, 62.20000076293945, 65.0999984741211, 72.69999694824219, 70.3499984741211, 72.30000305175781, 72.44999694824219, 72.05000305175781, 71.3499984741211, 72.3499984741211, 72.4000015258789, 72.8499984741211, 70.44999694824219, 69.19999694824219, 70.9000015258789, 70.05000305175781, 67.5999984741211, 70.55000305175781, 67.69999694824219, 70.05000305175781, 72.69999694824219, 71.30000305175781, 69.1500015258789, 70.69999694824219, 72.25, 72.8499984741211, 74.80000305175781, 72.9000015258789, 69.5, 66.69999694824219, 71.6500015258789, 71.8499984741211, 68.8499984741211, 71.0999984741211, 68.0999984741211, 68.94999694824219, 71.05000305175781, 71.25, 69.80000305175781, 70.44999694824219, 72.25, 71.4000015258789, 70.6500015258789, 69.5, 70.3499984741211, 70.0, 71.69999694824219, 69.94999694824219, 70.44999694824219, 69.80000305175781, 70.8499984741211, 71.5999984741211, 67.1500015258789, 71.4000015258789, 70.4000015258789, 70.75, 71.25, 68.30000305175781, 68.80000305175781, 65.44999694824219, 69.8499984741211, 69.80000305175781, 67.69999694824219, 68.6500015258789, 72.0999984741211, 72.8499984741211, 71.8499984741211, 71.75, 71.5, 71.05000305175781, 70.30000305175781, 70.05000305175781, 71.19999694824219, 67.75, 71.44999694824219, 69.1500015258789, 69.69999694824219, 69.5999984741211, 66.6500015258789, 66.94999694824219, 68.5999984741211, 68.3499984741211, 67.1500015258789, 68.44999694824219, 70.4000015258789, 66.19999694824219, 68.5, 70.8499984741211, 69.30000305175781, 69.0, 68.0999984741211, 66.94999694824219, 69.94999694824219, 68.0, 66.5, 67.19999694824219, 65.44999694824219, 65.25, 61.54999923706055, 60.95000076293945, 63.0, 67.75, 66.1500015258789, 64.55000305175781, 63.650001525878906, 64.25, 66.30000305175781, 67.19999694824219, 67.44999694824219, 63.400001525878906, 68.25, 68.3499984741211, 67.25, 67.0, 67.25, 69.5999984741211, 67.6500015258789, 67.94999694824219, 67.9000015258789, 65.05000305175781, 65.4000015258789, 64.80000305175781, 67.19999694824219, 67.44999694824219, 67.4000015258789, 66.55000305175781, 66.19999694824219, 68.80000305175781, 70.75, 70.69999694824219, 70.8499984741211, 63.75, 68.5999984741211, 65.5999984741211, 68.19999694824219, 65.0, 64.69999694824219, 65.8499984741211, 67.8499984741211, 67.0999984741211, 65.69999694824219, 64.0999984741211, 64.1500015258789, 65.25, 66.44999694824219, 66.5999984741211, 66.30000305175781, 67.55000305175781, 67.80000305175781, 66.0, 67.05000305175781, 67.5999984741211, 68.0999984741211, 64.05000305175781, 65.5, 67.25, 64.94999694824219, 62.70000076293945, 62.150001525878906, 62.79999923706055, 60.29999923706055, 60.45000076293945, 62.0, 62.95000076293945, 60.849998474121094, 62.150001525878906, 62.849998474121094, 62.099998474121094, 62.95000076293945, 65.30000305175781, 64.44999694824219, 63.25, 63.349998474121094, 63.20000076293945, 63.54999923706055, 64.8499984741211, 65.30000305175781, 68.0, 67.30000305175781, 67.80000305175781, 68.55000305175781, 70.5999984741211, 68.5999984741211, 70.69999694824219, 72.55000305175781, 72.4000015258789, 71.1500015258789, 70.75, 70.4000015258789, 67.4000015258789, 72.55000305175781, 72.0999984741211, 72.30000305175781, 71.6500015258789, 72.1500015258789, 69.55000305175781, 68.30000305175781, 67.19999694824219, 67.80000305175781, 68.55000305175781, 70.30000305175781, 69.44999694824219, 70.55000305175781, 69.44999694824219, 68.3499984741211, 67.80000305175781, 68.8499984741211, 67.75, 67.4000015258789, 66.8499984741211, 68.0, 68.05000305175781, 69.0999984741211, 68.44999694824219, 69.94999694824219, 69.69999694824219, 68.75, 69.4000015258789, 69.75, 67.0, 69.0, 67.8499984741211, 65.75, 69.0, 67.75, 64.69999694824219, 64.8499984741211, 65.25, 65.3499984741211, 64.9000015258789, 65.9000015258789, 67.44999694824219, 67.0, 68.69999694824219, 67.8499984741211, 68.5, 69.1500015258789, 69.19999694824219, 69.4000015258789, 68.0999984741211, 67.75, 67.75, 67.9000015258789, 67.44999694824219, 67.9000015258789, 65.44999694824219, 67.0, 67.94999694824219, 68.55000305175781, 70.6500015258789, 71.4000015258789, 71.5, 70.0, 69.3499984741211, 68.69999694824219, 67.8499984741211, 67.0, 68.30000305175781, 67.80000305175781, 67.94999694824219, 68.8499984741211, 67.55000305175781, 68.8499984741211, 67.0999984741211, 65.5, 62.79999923706055, 66.25, 66.69999694824219, 70.75, 70.55000305175781, 68.6500015258789, 71.5, 72.19999694824219, 70.4000015258789, 67.69999694824219, 68.4000015258789, 71.3499984741211, 67.1500015258789, 67.4000015258789, 69.30000305175781, 69.05000305175781, 67.8499984741211, 63.400001525878906, 65.5999984741211, 67.4000015258789, 68.6500015258789, 68.9000015258789, 68.25, 68.55000305175781, 67.4000015258789, 66.80000305175781, 66.8499984741211, 66.0, 66.30000305175781, 67.0, 66.05000305175781, 66.0]\n",
    "Cluster_1_accuracy_50_small=[49.25, 49.95000076293945, 50.04999923706055, 49.95000076293945, 54.29999923706055, 53.900001525878906, 52.75, 53.599998474121094, 50.150001525878906, 50.95000076293945, 55.099998474121094, 51.599998474121094, 52.599998474121094, 58.349998474121094, 59.25, 58.900001525878906, 53.650001525878906, 57.650001525878906, 54.150001525878906, 58.150001525878906, 51.54999923706055, 52.900001525878906, 54.5, 54.70000076293945, 52.29999923706055, 50.25, 54.04999923706055, 52.900001525878906, 53.349998474121094, 54.5, 55.75, 53.0, 51.70000076293945, 49.099998474121094, 52.0, 55.400001525878906, 54.0, 55.849998474121094, 55.70000076293945, 51.25, 51.400001525878906, 51.95000076293945, 47.099998474121094, 45.29999923706055, 48.400001525878906, 50.349998474121094, 51.349998474121094, 53.099998474121094, 53.45000076293945, 55.0, 55.650001525878906, 54.95000076293945, 57.45000076293945, 55.20000076293945, 56.54999923706055, 56.650001525878906, 56.79999923706055, 55.04999923706055, 54.04999923706055, 54.04999923706055, 55.20000076293945, 56.650001525878906, 56.79999923706055, 54.95000076293945, 53.349998474121094, 54.400001525878906, 53.849998474121094, 55.25, 56.849998474121094, 55.20000076293945, 53.20000076293945, 50.650001525878906, 49.650001525878906, 57.900001525878906, 59.0, 56.599998474121094, 55.25, 57.54999923706055, 56.54999923706055, 57.650001525878906, 58.45000076293945, 59.349998474121094, 56.599998474121094, 57.599998474121094, 54.900001525878906, 56.20000076293945, 56.45000076293945, 57.54999923706055, 57.20000076293945, 57.0, 56.349998474121094, 57.45000076293945, 55.849998474121094, 54.75, 54.599998474121094, 55.29999923706055, 55.349998474121094, 57.650001525878906, 57.150001525878906, 59.75, 58.79999923706055, 58.150001525878906, 59.95000076293945, 56.95000076293945, 56.95000076293945, 55.45000076293945, 55.0, 53.95000076293945, 54.75, 57.349998474121094, 53.04999923706055, 53.20000076293945, 52.95000076293945, 54.099998474121094, 54.400001525878906, 51.900001525878906, 52.54999923706055, 52.29999923706055, 54.599998474121094, 52.75, 53.04999923706055, 52.20000076293945, 49.29999923706055, 50.75, 53.29999923706055, 53.099998474121094, 51.599998474121094, 53.75, 53.650001525878906, 53.150001525878906, 50.900001525878906, 53.900001525878906, 50.04999923706055, 49.400001525878906, 49.54999923706055, 52.599998474121094, 49.75, 48.349998474121094, 43.0, 52.29999923706055, 51.79999923706055, 52.650001525878906, 55.099998474121094, 54.54999923706055, 53.349998474121094, 52.54999923706055, 50.349998474121094, 50.79999923706055, 50.400001525878906, 51.150001525878906, 50.75, 51.5, 51.20000076293945, 53.400001525878906, 51.650001525878906, 51.04999923706055, 51.0, 52.20000076293945, 53.95000076293945, 51.599998474121094, 55.54999923706055, 55.25, 53.849998474121094, 54.650001525878906, 53.70000076293945, 55.400001525878906, 53.349998474121094, 55.650001525878906, 56.099998474121094, 54.20000076293945, 55.45000076293945, 54.29999923706055, 55.04999923706055, 52.599998474121094, 53.900001525878906, 52.70000076293945, 51.400001525878906, 53.0, 53.20000076293945, 51.20000076293945, 55.29999923706055, 55.400001525878906, 54.70000076293945, 56.400001525878906, 55.5, 54.099998474121094, 54.150001525878906, 55.75, 55.54999923706055, 55.54999923706055, 57.099998474121094, 57.099998474121094, 56.70000076293945, 55.25, 55.04999923706055, 54.54999923706055, 54.5, 53.849998474121094, 52.54999923706055, 53.04999923706055, 56.349998474121094, 55.45000076293945, 53.849998474121094, 55.5, 55.5, 56.150001525878906, 56.29999923706055, 57.45000076293945, 58.45000076293945, 54.45000076293945, 56.650001525878906, 57.150001525878906, 56.400001525878906, 55.400001525878906, 56.25, 56.849998474121094, 55.650001525878906, 55.349998474121094, 54.95000076293945, 56.150001525878906, 54.95000076293945, 55.25, 55.20000076293945, 54.79999923706055, 54.20000076293945, 53.29999923706055, 53.54999923706055, 52.70000076293945, 53.5, 52.900001525878906, 53.599998474121094, 53.099998474121094, 53.599998474121094, 52.54999923706055, 52.04999923706055, 51.25, 52.95000076293945, 51.5, 52.20000076293945, 53.75, 52.0, 53.20000076293945, 52.599998474121094, 51.70000076293945, 52.79999923706055, 52.79999923706055, 53.099998474121094, 52.20000076293945, 52.349998474121094, 50.599998474121094, 50.099998474121094, 52.0, 51.79999923706055, 52.29999923706055, 54.20000076293945, 52.849998474121094, 52.400001525878906, 52.20000076293945, 51.79999923706055, 52.900001525878906, 53.79999923706055, 53.150001525878906, 52.70000076293945, 52.70000076293945, 52.900001525878906, 53.79999923706055, 53.29999923706055, 53.849998474121094, 54.0, 53.150001525878906, 53.54999923706055, 52.849998474121094, 53.900001525878906, 54.95000076293945, 54.400001525878906, 54.70000076293945, 53.650001525878906, 53.95000076293945, 54.5, 53.349998474121094, 53.0, 53.900001525878906, 53.349998474121094, 52.150001525878906, 52.349998474121094, 52.79999923706055, 53.099998474121094, 52.900001525878906, 51.5, 53.45000076293945, 54.150001525878906, 54.20000076293945, 54.900001525878906, 54.25, 54.900001525878906, 54.599998474121094, 54.25, 54.20000076293945, 54.54999923706055, 54.150001525878906, 54.54999923706055, 54.900001525878906, 53.79999923706055, 54.099998474121094, 54.04999923706055, 55.650001525878906, 54.20000076293945, 54.79999923706055, 55.150001525878906, 53.45000076293945, 53.70000076293945, 54.70000076293945, 53.29999923706055, 53.70000076293945, 54.54999923706055, 54.349998474121094, 53.900001525878906, 54.54999923706055, 55.29999923706055, 56.0, 54.400001525878906, 54.95000076293945, 54.900001525878906, 54.349998474121094, 54.54999923706055, 53.349998474121094, 53.400001525878906, 54.599998474121094, 54.54999923706055, 53.599998474121094, 54.70000076293945, 55.150001525878906, 53.04999923706055, 53.650001525878906, 55.900001525878906, 54.599998474121094, 53.70000076293945, 55.29999923706055, 55.150001525878906, 52.79999923706055, 53.5, 52.70000076293945, 53.900001525878906, 54.04999923706055, 54.150001525878906, 53.25, 54.5, 54.5, 54.25, 53.599998474121094, 53.20000076293945, 53.75, 50.95000076293945, 50.900001525878906, 50.400001525878906, 55.099998474121094, 54.95000076293945, 55.849998474121094, 54.900001525878906, 53.150001525878906, 53.95000076293945, 58.599998474121094, 57.29999923706055, 58.20000076293945, 57.5, 57.5, 56.45000076293945, 56.849998474121094, 56.150001525878906, 57.45000076293945, 58.04999923706055, 58.25, 60.0, 59.54999923706055, 60.150001525878906, 59.849998474121094, 60.849998474121094, 60.650001525878906, 61.5, 61.849998474121094, 60.849998474121094, 61.349998474121094, 60.599998474121094, 59.849998474121094, 58.79999923706055, 60.79999923706055, 60.099998474121094, 60.849998474121094, 59.04999923706055, 59.900001525878906, 59.54999923706055, 59.0, 57.45000076293945, 58.5, 58.04999923706055, 58.20000076293945, 58.400001525878906, 57.400001525878906, 59.599998474121094, 59.54999923706055, 57.04999923706055, 57.54999923706055, 57.650001525878906, 58.900001525878906, 56.849998474121094, 59.599998474121094, 59.5, 58.650001525878906, 58.650001525878906, 59.849998474121094, 59.25, 57.0, 56.5, 59.0, 59.5, 60.150001525878906, 60.849998474121094, 58.70000076293945, 59.900001525878906, 60.400001525878906, 58.45000076293945, 58.45000076293945, 57.20000076293945, 57.25, 56.900001525878906, 58.04999923706055, 56.54999923706055, 55.5, 56.25, 58.400001525878906, 57.349998474121094, 60.150001525878906, 58.70000076293945, 57.349998474121094, 57.54999923706055, 56.04999923706055, 58.5, 57.25, 56.70000076293945, 56.599998474121094, 57.70000076293945, 57.45000076293945, 56.900001525878906, 58.349998474121094, 58.650001525878906, 56.650001525878906, 56.20000076293945, 58.70000076293945, 60.75, 60.95000076293945, 59.349998474121094, 59.150001525878906, 59.20000076293945, 60.75, 59.599998474121094, 60.099998474121094, 59.79999923706055, 60.900001525878906, 61.25, 60.95000076293945, 59.5, 59.849998474121094, 60.45000076293945, 59.0, 58.45000076293945, 57.5, 58.150001525878906, 57.29999923706055, 58.650001525878906, 57.25, 60.29999923706055, 58.849998474121094, 58.599998474121094, 57.150001525878906, 57.20000076293945, 59.400001525878906, 58.349998474121094, 58.45000076293945, 58.25, 58.900001525878906, 57.20000076293945, 57.349998474121094, 56.900001525878906, 58.79999923706055, 57.04999923706055, 57.04999923706055, 56.0, 56.25, 56.25, 55.04999923706055, 55.849998474121094, 56.099998474121094, 54.349998474121094, 55.150001525878906, 56.099998474121094, 56.25, 55.04999923706055, 56.79999923706055, 56.29999923706055, 56.54999923706055]\n",
    "Cluster_2_accuracy_50_small=[36.25, 53.650001525878906, 49.45000076293945, 53.25, 55.25, 53.5, 49.099998474121094, 52.150001525878906, 51.400001525878906, 53.79999923706055, 51.099998474121094, 55.150001525878906, 53.900001525878906, 55.70000076293945, 54.45000076293945, 55.29999923706055, 50.5, 56.70000076293945, 59.650001525878906, 50.099998474121094, 53.650001525878906, 56.849998474121094, 51.29999923706055, 55.25, 52.400001525878906, 55.29999923706055, 55.099998474121094, 57.150001525878906, 53.650001525878906, 56.29999923706055, 59.099998474121094, 55.650001525878906, 52.79999923706055, 54.0, 56.29999923706055, 54.45000076293945, 51.599998474121094, 53.5, 50.75, 46.54999923706055, 55.25, 57.20000076293945, 59.400001525878906, 53.54999923706055, 59.75, 56.45000076293945, 61.599998474121094, 53.0, 57.349998474121094, 55.75, 57.900001525878906, 58.04999923706055, 56.75, 58.599998474121094, 58.70000076293945, 52.04999923706055, 51.900001525878906, 53.04999923706055, 55.099998474121094, 58.5, 50.29999923706055, 53.79999923706055, 58.099998474121094, 60.20000076293945, 57.400001525878906, 51.349998474121094, 54.349998474121094, 50.70000076293945, 50.25, 54.5, 56.20000076293945, 55.95000076293945, 57.79999923706055, 56.849998474121094, 52.54999923706055, 51.54999923706055, 58.29999923706055, 50.900001525878906, 48.900001525878906, 49.25, 50.70000076293945, 56.20000076293945, 50.95000076293945, 52.849998474121094, 56.599998474121094, 56.0, 50.900001525878906, 56.150001525878906, 57.900001525878906, 58.25, 55.29999923706055, 54.25, 56.349998474121094, 53.5, 49.45000076293945, 51.599998474121094, 53.849998474121094, 53.04999923706055, 56.29999923706055, 53.29999923706055, 53.70000076293945, 55.349998474121094, 57.099998474121094, 56.099998474121094, 53.29999923706055, 51.349998474121094, 54.400001525878906, 53.95000076293945, 52.650001525878906, 54.650001525878906, 56.0, 55.04999923706055, 54.650001525878906, 55.45000076293945, 54.54999923706055, 57.650001525878906, 57.650001525878906, 56.599998474121094, 56.79999923706055, 57.0, 60.900001525878906, 59.70000076293945, 59.25, 56.5, 58.599998474121094, 58.45000076293945, 53.599998474121094, 58.650001525878906, 58.400001525878906, 52.29999923706055, 52.54999923706055, 52.79999923706055, 57.099998474121094, 59.29999923706055, 57.70000076293945, 58.150001525878906, 59.5, 58.900001525878906, 59.599998474121094, 59.099998474121094, 60.04999923706055, 59.099998474121094, 57.150001525878906, 57.25, 55.79999923706055, 54.70000076293945, 57.04999923706055, 58.5, 58.5, 59.95000076293945, 58.5, 59.599998474121094, 60.04999923706055, 59.900001525878906, 53.70000076293945, 56.400001525878906, 60.75, 59.20000076293945, 61.599998474121094, 60.75, 59.099998474121094, 59.349998474121094, 60.29999923706055, 59.5, 59.900001525878906, 59.25, 60.79999923706055, 60.79999923706055, 57.0, 58.54999923706055, 56.150001525878906, 60.849998474121094, 59.900001525878906, 61.400001525878906, 61.099998474121094, 58.349998474121094, 60.95000076293945, 54.5, 55.25, 58.25, 59.54999923706055, 56.150001525878906, 57.5, 61.79999923706055, 62.54999923706055, 57.45000076293945, 57.599998474121094, 54.349998474121094, 52.849998474121094, 58.5, 58.5, 62.099998474121094, 60.45000076293945, 58.650001525878906, 59.45000076293945, 58.349998474121094, 58.29999923706055, 53.20000076293945, 56.099998474121094, 55.25, 57.29999923706055, 60.54999923706055, 61.849998474121094, 62.099998474121094, 59.150001525878906, 61.150001525878906, 61.650001525878906, 57.349998474121094, 57.599998474121094, 54.849998474121094, 59.29999923706055, 60.54999923706055, 56.5, 59.04999923706055, 60.0, 60.5, 61.95000076293945, 61.75, 61.400001525878906, 63.95000076293945, 62.900001525878906, 63.5, 60.650001525878906, 60.45000076293945, 60.599998474121094, 61.29999923706055, 61.95000076293945, 60.25, 61.599998474121094, 60.75, 60.79999923706055, 61.400001525878906, 62.099998474121094, 62.0, 63.0, 61.29999923706055, 59.70000076293945, 61.400001525878906, 61.79999923706055, 60.95000076293945, 59.54999923706055, 61.25, 61.25, 58.20000076293945, 59.599998474121094, 57.150001525878906, 60.5, 59.29999923706055, 59.04999923706055, 59.95000076293945, 58.150001525878906, 59.849998474121094, 61.45000076293945, 59.599998474121094, 59.0, 60.29999923706055, 61.29999923706055, 60.54999923706055, 59.20000076293945, 59.150001525878906, 58.45000076293945, 61.04999923706055, 59.150001525878906, 62.54999923706055, 60.650001525878906, 58.75, 60.0, 60.349998474121094, 60.75, 60.25, 61.599998474121094, 64.1500015258789, 62.20000076293945, 60.75, 59.150001525878906, 58.45000076293945, 59.650001525878906, 59.900001525878906, 59.900001525878906, 58.29999923706055, 57.20000076293945, 56.349998474121094, 55.45000076293945, 57.04999923706055, 57.599998474121094, 58.599998474121094, 58.650001525878906, 54.849998474121094, 57.75, 57.5, 57.650001525878906, 57.29999923706055, 58.900001525878906, 57.0, 58.400001525878906, 58.79999923706055, 59.79999923706055, 59.95000076293945, 58.70000076293945, 60.29999923706055, 60.349998474121094, 62.20000076293945, 61.349998474121094, 59.599998474121094, 57.20000076293945, 59.75, 61.0, 60.29999923706055, 59.0, 60.25, 60.650001525878906, 61.75, 61.400001525878906, 62.79999923706055, 62.25, 62.04999923706055, 61.04999923706055, 62.79999923706055, 62.29999923706055, 61.45000076293945, 57.75, 60.04999923706055, 61.70000076293945, 59.75, 61.599998474121094, 62.20000076293945, 62.5, 63.04999923706055, 62.400001525878906, 64.30000305175781, 63.400001525878906, 64.4000015258789, 61.849998474121094, 61.45000076293945, 61.04999923706055, 62.04999923706055, 61.45000076293945, 61.25, 60.099998474121094, 59.29999923706055, 59.349998474121094, 60.20000076293945, 59.79999923706055, 54.849998474121094, 59.79999923706055, 56.849998474121094, 57.599998474121094, 57.70000076293945, 57.150001525878906, 56.75, 53.5, 57.45000076293945, 56.0, 59.0, 59.0, 58.54999923706055, 60.5, 60.900001525878906, 59.29999923706055, 55.95000076293945, 60.150001525878906, 61.54999923706055, 61.849998474121094, 59.45000076293945, 58.099998474121094, 59.099998474121094, 61.04999923706055, 58.70000076293945, 63.599998474121094, 64.19999694824219, 58.5, 63.599998474121094, 62.349998474121094, 63.5, 64.1500015258789, 64.19999694824219, 64.3499984741211, 63.849998474121094, 64.25, 58.5, 59.150001525878906, 59.70000076293945, 58.650001525878906, 58.599998474121094, 61.20000076293945, 62.900001525878906, 63.400001525878906, 62.20000076293945, 58.900001525878906, 61.849998474121094, 61.70000076293945, 62.400001525878906, 59.04999923706055, 62.29999923706055, 61.75, 63.650001525878906, 62.20000076293945, 63.599998474121094, 64.5, 63.70000076293945, 64.05000305175781, 65.05000305175781, 64.80000305175781, 64.9000015258789, 64.80000305175781, 65.0999984741211, 63.400001525878906, 64.80000305175781, 65.1500015258789, 63.400001525878906, 59.5, 63.0, 62.650001525878906, 60.70000076293945, 62.400001525878906, 63.29999923706055, 62.70000076293945, 63.04999923706055, 62.650001525878906, 57.849998474121094, 57.650001525878906, 55.04999923706055, 58.29999923706055, 57.099998474121094, 61.45000076293945, 63.150001525878906, 59.29999923706055, 60.5, 61.349998474121094, 58.650001525878906, 58.75, 58.54999923706055, 59.099998474121094, 57.25, 61.900001525878906, 62.04999923706055, 61.349998474121094, 60.95000076293945, 61.900001525878906, 63.29999923706055, 62.54999923706055, 62.29999923706055, 58.650001525878906, 60.95000076293945, 59.349998474121094, 61.150001525878906, 60.349998474121094, 60.95000076293945, 61.599998474121094, 57.349998474121094, 58.650001525878906, 59.400001525878906, 59.54999923706055, 59.75, 60.54999923706055, 62.79999923706055, 61.5, 59.400001525878906, 60.849998474121094, 60.79999923706055, 58.849998474121094, 60.70000076293945, 58.70000076293945, 58.54999923706055, 57.29999923706055, 60.150001525878906, 60.70000076293945, 60.29999923706055, 62.099998474121094, 59.0, 60.70000076293945, 61.45000076293945, 59.70000076293945, 58.349998474121094, 58.75, 61.54999923706055, 60.400001525878906, 58.70000076293945, 61.599998474121094, 59.900001525878906, 61.849998474121094, 61.75, 60.04999923706055, 58.79999923706055, 60.45000076293945, 59.849998474121094, 59.650001525878906, 61.75, 58.54999923706055, 60.400001525878906, 63.150001525878906, 65.0, 63.150001525878906, 63.29999923706055, 63.70000076293945, 65.1500015258789, 63.20000076293945, 62.95000076293945, 61.20000076293945, 63.349998474121094]\n",
    "Cluster_3_accuracy_50_small=[42.30999984741211, 51.75, 6.300000190734863, 11.5, 57.099998474121094, 24.649999618530273, 64.55000305175781, 54.25, 56.0, 54.0, 60.5, 53.29999923706055, 59.099998474121094, 52.04999923706055, 54.650001525878906, 56.95000076293945, 62.099998474121094, 66.19999694824219, 52.099998474121094, 57.5, 55.849998474121094, 62.849998474121094, 57.650001525878906, 45.349998474121094, 56.25, 52.29999923706055, 57.5, 56.900001525878906, 52.45000076293945, 57.599998474121094, 65.94999694824219, 53.75, 60.099998474121094, 61.0, 59.599998474121094, 66.3499984741211, 60.099998474121094, 70.44999694824219, 67.69999694824219, 60.75, 68.44999694824219, 60.099998474121094, 63.150001525878906, 71.44999694824219, 64.94999694824219, 66.3499984741211, 70.8499984741211, 70.30000305175781, 72.8499984741211, 64.69999694824219, 70.75, 67.19999694824219, 68.30000305175781, 63.04999923706055, 64.80000305175781, 67.5, 64.19999694824219, 66.6500015258789, 69.4000015258789, 68.55000305175781, 69.6500015258789, 67.25, 63.29999923706055, 70.30000305175781, 68.69999694824219, 67.05000305175781, 73.6500015258789, 70.3499984741211, 65.80000305175781, 66.19999694824219, 65.0999984741211, 66.80000305175781, 71.44999694824219, 65.4000015258789, 69.75, 69.75, 66.9000015258789, 72.25, 68.6500015258789, 69.0, 63.5, 68.55000305175781, 69.4000015258789, 69.69999694824219, 61.400001525878906, 64.80000305175781, 69.4000015258789, 70.44999694824219, 72.3499984741211, 70.4000015258789, 72.0999984741211, 71.3499984741211, 72.5999984741211, 71.80000305175781, 73.4000015258789, 71.6500015258789, 71.5999984741211, 71.94999694824219, 73.9000015258789, 71.55000305175781, 71.0999984741211, 63.650001525878906, 67.05000305175781, 68.3499984741211, 68.55000305175781, 67.69999694824219, 71.44999694824219, 74.5, 73.0999984741211, 72.1500015258789, 70.80000305175781, 71.1500015258789, 74.6500015258789, 74.25, 73.0, 61.849998474121094, 66.5, 70.3499984741211, 69.44999694824219, 71.9000015258789, 70.30000305175781, 67.69999694824219, 68.19999694824219, 71.25, 71.6500015258789, 67.55000305175781, 69.3499984741211, 68.44999694824219, 71.0, 68.3499984741211, 71.19999694824219, 73.44999694824219, 72.69999694824219, 72.5, 74.30000305175781, 73.0999984741211, 70.25, 72.3499984741211, 70.30000305175781, 71.4000015258789, 72.9000015258789, 72.5, 72.25, 72.25, 75.30000305175781, 75.75, 74.30000305175781, 73.30000305175781, 72.4000015258789, 72.5999984741211, 71.80000305175781, 71.9000015258789, 73.80000305175781, 72.5, 74.9000015258789, 75.25, 73.8499984741211, 71.05000305175781, 73.4000015258789, 72.3499984741211, 73.0999984741211, 72.94999694824219, 72.0, 72.30000305175781, 71.94999694824219, 71.80000305175781, 74.1500015258789, 70.75, 73.4000015258789, 71.9000015258789, 72.6500015258789, 73.0999984741211, 73.25, 74.5, 73.5, 68.94999694824219, 70.4000015258789, 72.1500015258789, 69.5, 72.55000305175781, 70.19999694824219, 72.44999694824219, 70.1500015258789, 69.5, 70.30000305175781, 67.6500015258789, 70.69999694824219, 70.4000015258789, 69.5, 70.19999694824219, 68.80000305175781, 69.1500015258789, 66.9000015258789, 69.55000305175781, 69.1500015258789, 69.0, 71.5999984741211, 71.0999984741211, 69.19999694824219, 67.94999694824219, 68.75, 71.0999984741211, 70.30000305175781, 71.0, 71.69999694824219, 70.44999694824219, 73.30000305175781, 73.9000015258789, 71.8499984741211, 71.1500015258789, 71.0, 71.30000305175781, 71.6500015258789, 72.19999694824219, 71.94999694824219, 72.3499984741211, 70.5, 69.4000015258789, 73.44999694824219, 72.9000015258789, 70.25, 70.69999694824219, 70.0999984741211, 71.0999984741211, 71.0999984741211, 69.5, 69.9000015258789, 69.30000305175781, 70.9000015258789, 70.80000305175781, 72.44999694824219, 72.30000305175781, 71.5, 71.94999694824219, 70.0, 70.8499984741211, 72.1500015258789, 70.3499984741211, 72.0999984741211, 67.9000015258789, 69.75, 69.25, 70.94999694824219, 70.6500015258789, 71.4000015258789, 72.44999694824219, 72.75, 71.1500015258789, 72.1500015258789, 70.4000015258789, 70.8499984741211, 71.6500015258789, 72.0, 71.5, 71.5, 70.30000305175781, 67.1500015258789, 65.8499984741211, 67.0999984741211, 66.55000305175781, 67.5999984741211, 67.94999694824219, 70.4000015258789, 71.6500015258789, 72.80000305175781, 73.6500015258789, 72.1500015258789, 73.0, 73.25, 73.69999694824219, 73.9000015258789, 72.44999694824219, 71.4000015258789, 72.8499984741211, 72.75, 73.4000015258789, 72.3499984741211, 74.0999984741211, 73.6500015258789, 74.9000015258789, 74.19999694824219, 74.25, 73.3499984741211, 73.0999984741211, 73.0999984741211, 73.19999694824219, 72.44999694824219, 72.69999694824219, 73.5, 72.44999694824219, 73.5999984741211, 74.6500015258789, 74.1500015258789, 74.30000305175781, 73.3499984741211, 73.4000015258789, 75.3499984741211, 75.0999984741211, 73.44999694824219, 73.6500015258789, 71.9000015258789, 73.0999984741211, 71.1500015258789, 72.25, 73.0, 73.30000305175781, 72.9000015258789, 73.5999984741211, 73.25, 70.80000305175781, 71.30000305175781, 70.5, 70.0, 69.0999984741211, 69.75, 68.69999694824219, 70.55000305175781, 71.44999694824219, 69.25, 70.9000015258789, 72.80000305175781, 71.5999984741211, 74.05000305175781, 72.5, 73.6500015258789, 73.4000015258789, 71.80000305175781, 72.80000305175781, 73.6500015258789, 75.1500015258789, 73.30000305175781, 74.0, 74.30000305175781, 73.3499984741211, 75.0, 72.3499984741211, 75.55000305175781, 74.9000015258789, 74.6500015258789, 73.8499984741211, 72.8499984741211, 74.19999694824219, 73.3499984741211, 72.9000015258789, 71.75, 73.75, 72.19999694824219, 72.3499984741211, 73.55000305175781, 72.05000305175781, 72.75, 72.9000015258789, 71.30000305175781, 72.19999694824219, 72.4000015258789, 72.55000305175781, 71.55000305175781, 75.0999984741211, 75.6500015258789, 75.44999694824219, 74.44999694824219, 74.94999694824219, 74.5999984741211, 75.9000015258789, 74.30000305175781, 74.4000015258789, 74.1500015258789, 75.5999984741211, 73.8499984741211, 74.0999984741211, 75.80000305175781, 74.5999984741211, 74.3499984741211, 73.6500015258789, 74.9000015258789, 75.4000015258789, 75.30000305175781, 74.55000305175781, 75.19999694824219, 75.25, 74.4000015258789, 74.55000305175781, 74.75, 75.0, 75.1500015258789, 74.19999694824219, 75.30000305175781, 74.4000015258789, 75.25, 76.44999694824219, 75.5999984741211, 76.25, 74.8499984741211, 75.4000015258789, 74.1500015258789, 73.9000015258789, 74.9000015258789, 74.6500015258789, 73.3499984741211, 74.69999694824219, 74.4000015258789, 75.30000305175781, 76.0999984741211, 76.4000015258789, 75.05000305175781, 74.0999984741211, 73.94999694824219, 75.3499984741211, 74.75, 71.0999984741211, 74.1500015258789, 73.3499984741211, 73.94999694824219, 75.0999984741211, 76.25, 75.69999694824219, 75.4000015258789, 74.75, 76.19999694824219, 74.94999694824219, 75.19999694824219, 75.5999984741211, 74.75, 75.9000015258789, 75.05000305175781, 75.55000305175781, 74.8499984741211, 75.9000015258789, 75.44999694824219, 75.9000015258789, 75.69999694824219, 76.4000015258789, 76.55000305175781, 77.05000305175781, 76.05000305175781, 77.5, 75.9000015258789, 75.75, 76.69999694824219, 76.05000305175781, 76.9000015258789, 76.19999694824219, 74.69999694824219, 77.55000305175781, 75.6500015258789, 76.80000305175781, 75.8499984741211, 77.55000305175781, 76.69999694824219, 76.0, 76.0999984741211, 78.0999984741211, 77.75, 77.19999694824219, 77.5999984741211, 77.6500015258789, 77.69999694824219, 78.05000305175781, 76.80000305175781, 78.5, 77.75, 77.5999984741211, 76.8499984741211, 75.19999694824219, 76.0999984741211, 76.25, 75.4000015258789, 74.05000305175781, 76.69999694824219, 76.05000305175781, 76.4000015258789, 75.0, 77.1500015258789, 76.19999694824219, 75.55000305175781, 75.94999694824219, 76.19999694824219, 76.25, 76.5999984741211, 76.19999694824219, 77.3499984741211, 77.8499984741211, 76.8499984741211, 77.05000305175781, 75.1500015258789, 76.4000015258789, 74.75, 73.4000015258789, 74.4000015258789, 74.25, 75.05000305175781, 74.05000305175781, 73.80000305175781, 73.19999694824219, 73.3499984741211, 73.0999984741211, 73.30000305175781, 72.55000305175781, 73.5999984741211, 74.05000305175781]\n",
    "Cluster_4_accuracy_50_small=[0.6499999761581421, 38.099998474121094, 58.04999923706055, 59.150001525878906, 64.75, 63.20000076293945, 61.099998474121094, 59.599998474121094, 67.44999694824219, 66.6500015258789, 60.54999923706055, 60.20000076293945, 65.5, 66.19999694824219, 64.19999694824219, 69.1500015258789, 61.849998474121094, 67.0, 69.5, 69.55000305175781, 68.80000305175781, 68.8499984741211, 67.0999984741211, 69.55000305175781, 68.0, 68.4000015258789, 66.3499984741211, 67.69999694824219, 64.6500015258789, 66.8499984741211, 63.400001525878906, 63.20000076293945, 64.3499984741211, 63.900001525878906, 67.30000305175781, 68.75, 65.6500015258789, 67.80000305175781, 59.349998474121094, 61.650001525878906, 64.8499984741211, 61.25, 62.5, 61.95000076293945, 63.900001525878906, 67.3499984741211, 65.05000305175781, 66.30000305175781, 64.55000305175781, 63.04999923706055, 66.75, 65.30000305175781, 59.45000076293945, 63.849998474121094, 58.75, 64.55000305175781, 64.5999984741211, 65.0, 66.0, 66.4000015258789, 65.30000305175781, 63.599998474121094, 62.45000076293945, 60.5, 62.400001525878906, 59.5, 52.650001525878906, 52.75, 57.099998474121094, 61.45000076293945, 63.79999923706055, 64.44999694824219, 66.8499984741211, 67.19999694824219, 56.29999923706055, 69.55000305175781, 61.599998474121094, 64.25, 58.5, 60.25, 57.150001525878906, 55.45000076293945, 58.349998474121094, 67.1500015258789, 63.04999923706055, 63.150001525878906, 67.1500015258789, 65.5999984741211, 65.69999694824219, 63.20000076293945, 63.25, 61.650001525878906, 62.20000076293945, 60.04999923706055, 62.20000076293945, 61.45000076293945, 62.20000076293945, 58.099998474121094, 61.349998474121094, 62.54999923706055, 63.79999923706055, 59.95000076293945, 67.25, 66.30000305175781, 62.5, 59.45000076293945, 60.70000076293945, 63.45000076293945, 59.349998474121094, 62.04999923706055, 64.55000305175781, 63.25, 59.45000076293945, 62.599998474121094, 57.650001525878906, 60.849998474121094, 64.75, 54.0, 65.44999694824219, 62.25, 69.75, 68.6500015258789, 64.8499984741211, 69.0, 67.75, 68.0999984741211, 69.1500015258789, 66.94999694824219, 69.0, 67.80000305175781, 66.5, 67.8499984741211, 66.5999984741211, 69.6500015258789, 67.19999694824219, 68.94999694824219, 65.30000305175781, 68.30000305175781, 67.8499984741211, 66.19999694824219, 66.0999984741211, 65.8499984741211, 66.94999694824219, 67.5, 65.75, 59.599998474121094, 66.44999694824219, 67.19999694824219, 66.1500015258789, 67.44999694824219, 67.9000015258789, 62.5, 61.95000076293945, 62.75, 68.19999694824219, 65.94999694824219, 65.0999984741211, 55.150001525878906, 59.70000076293945, 61.54999923706055, 60.849998474121094, 59.45000076293945, 61.849998474121094, 59.04999923706055, 57.54999923706055, 55.04999923706055, 58.54999923706055, 59.349998474121094, 59.75, 60.400001525878906, 65.30000305175781, 64.55000305175781, 60.0, 56.95000076293945, 57.650001525878906, 62.900001525878906, 66.05000305175781, 62.900001525878906, 63.400001525878906, 60.599998474121094, 58.900001525878906, 60.45000076293945, 64.5, 65.69999694824219, 58.70000076293945, 64.75, 57.79999923706055, 51.95000076293945, 60.29999923706055, 65.5999984741211, 66.0, 69.0999984741211, 70.30000305175781, 68.75, 69.30000305175781, 66.8499984741211, 68.5999984741211, 66.69999694824219, 62.04999923706055, 70.75, 70.4000015258789, 70.9000015258789, 70.19999694824219, 71.19999694824219, 72.5999984741211, 69.69999694824219, 67.5, 67.19999694824219, 68.55000305175781, 67.94999694824219, 68.44999694824219, 68.75, 69.69999694824219, 68.9000015258789, 68.0, 69.80000305175781, 67.1500015258789, 67.94999694824219, 66.44999694824219, 64.69999694824219, 58.5, 65.9000015258789, 63.20000076293945, 60.25, 61.25, 61.150001525878906, 64.25, 65.5999984741211, 67.94999694824219, 62.79999923706055, 64.69999694824219, 63.5, 61.75, 62.70000076293945, 60.95000076293945, 66.0, 65.80000305175781, 63.79999923706055, 66.05000305175781, 64.0999984741211, 63.79999923706055, 65.80000305175781, 66.0, 63.04999923706055, 64.1500015258789, 64.05000305175781, 64.05000305175781, 64.5, 61.79999923706055, 67.1500015258789, 67.25, 69.05000305175781, 65.55000305175781, 65.4000015258789, 67.05000305175781, 65.9000015258789, 66.19999694824219, 60.900001525878906, 63.29999923706055, 67.0, 67.0999984741211, 66.5, 64.0, 66.05000305175781, 64.55000305175781, 64.6500015258789, 65.69999694824219, 65.94999694824219, 60.45000076293945, 59.900001525878906, 61.099998474121094, 62.95000076293945, 63.04999923706055, 61.650001525878906, 59.650001525878906, 62.95000076293945, 62.75, 64.55000305175781, 65.5, 65.4000015258789, 63.650001525878906, 65.25, 67.5999984741211, 65.3499984741211, 60.70000076293945, 67.0, 66.05000305175781, 65.0999984741211, 67.0, 64.30000305175781, 64.9000015258789, 62.95000076293945, 60.79999923706055, 62.0, 62.599998474121094, 62.79999923706055, 62.0, 61.25, 60.79999923706055, 59.599998474121094, 57.5, 58.349998474121094, 59.70000076293945, 60.95000076293945, 62.5, 62.900001525878906, 63.25, 62.25, 61.400001525878906, 60.400001525878906, 60.400001525878906, 60.25, 61.099998474121094, 61.650001525878906, 61.45000076293945, 59.400001525878906, 62.150001525878906, 64.55000305175781, 62.150001525878906, 62.79999923706055, 64.0999984741211, 62.25, 61.20000076293945, 59.70000076293945, 60.54999923706055, 61.599998474121094, 62.599998474121094, 63.5, 62.45000076293945, 60.54999923706055, 61.29999923706055, 61.75, 61.95000076293945, 63.849998474121094, 63.79999923706055, 64.3499984741211, 63.95000076293945, 65.05000305175781, 64.44999694824219, 65.94999694824219, 65.05000305175781, 65.05000305175781, 64.44999694824219, 64.80000305175781, 64.1500015258789, 63.150001525878906, 60.20000076293945, 64.9000015258789, 65.94999694824219, 66.3499984741211, 67.44999694824219, 66.6500015258789, 65.0999984741211, 64.25, 64.9000015258789, 64.19999694824219, 65.4000015258789, 65.1500015258789, 65.30000305175781, 66.0, 65.3499984741211, 64.0999984741211, 65.05000305175781, 66.80000305175781, 65.6500015258789, 65.5999984741211, 64.0999984741211, 63.54999923706055, 63.29999923706055, 66.44999694824219, 66.69999694824219, 66.0999984741211, 64.05000305175781, 65.30000305175781, 64.6500015258789, 67.25, 65.44999694824219, 67.1500015258789, 64.3499984741211, 66.05000305175781, 66.3499984741211, 63.150001525878906, 66.55000305175781, 66.75, 66.0999984741211, 68.30000305175781, 68.5999984741211, 67.6500015258789, 67.3499984741211, 67.19999694824219, 63.400001525878906, 67.94999694824219, 67.0999984741211, 68.4000015258789, 67.94999694824219, 67.6500015258789, 68.25, 67.5, 69.69999694824219, 70.6500015258789, 69.9000015258789, 67.55000305175781, 70.19999694824219, 73.1500015258789, 71.3499984741211, 69.6500015258789, 70.9000015258789, 71.30000305175781, 71.3499984741211, 70.3499984741211, 70.0999984741211, 69.55000305175781, 69.19999694824219, 68.8499984741211, 66.69999694824219, 65.94999694824219, 63.75, 67.19999694824219, 66.9000015258789, 67.6500015258789, 63.04999923706055, 68.3499984741211, 67.75, 66.25, 68.19999694824219, 69.44999694824219, 65.5, 65.4000015258789, 69.0, 70.55000305175781, 70.05000305175781, 71.19999694824219, 72.75, 73.0, 72.30000305175781, 71.55000305175781, 72.1500015258789, 72.55000305175781, 72.05000305175781, 73.5, 71.44999694824219, 73.05000305175781, 73.44999694824219, 70.8499984741211, 69.9000015258789, 71.3499984741211, 71.3499984741211, 71.80000305175781, 72.94999694824219, 73.19999694824219, 72.9000015258789, 67.44999694824219, 71.5999984741211, 71.0999984741211, 71.6500015258789, 72.4000015258789, 68.94999694824219, 66.8499984741211, 67.1500015258789, 71.05000305175781, 70.0, 70.0999984741211, 70.8499984741211, 68.94999694824219, 67.8499984741211, 68.69999694824219, 66.05000305175781, 70.5, 70.80000305175781, 71.1500015258789, 69.80000305175781, 71.5, 70.75, 71.1500015258789, 71.0, 71.0, 70.0999984741211, 67.69999694824219, 67.05000305175781, 67.9000015258789, 67.80000305175781, 68.5999984741211, 68.69999694824219, 67.69999694824219, 66.9000015258789, 67.9000015258789, 67.75, 68.6500015258789, 66.9000015258789, 67.69999694824219, 67.75, 68.05000305175781, 68.44999694824219, 70.5, 70.9000015258789, 69.6500015258789, 67.80000305175781, 70.3499984741211, 69.5999984741211, 69.1500015258789]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(Avg_acc_cluster0)), Avg_acc_cluster0,label=\"smaller learning rate\")\n",
    "#plt.plot(range(len(Cluster_0_accuracy)), Cluster_0_accuracy,label=\"larger learning rate\")\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 0 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig('./save/fed_{}_{}_{}_C{}_iid{}.png'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))\n",
    "plt.plot(range(len(Avg_acc_cluster1)), Avg_acc_cluster1,label=\"smaller learning rate\")\n",
    "#plt.plot(range(len(Cluster_1_accuracy)), Cluster_1_accuracy, label=\"larger learning rate\")\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 1 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(Avg_acc_cluster2)), Avg_acc_cluster2, label=\"smaller learning rate\")\n",
    "#plt.plot(range(len(Cluster_2_accuracy)), Cluster_2_accuracy, label=\"larger learning rate\")\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 2 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(range(len(Avg_acc_cluster3)), Avg_acc_cluster3, label=\"smaller learning rate\" )\n",
    "#plt.plot(range(len(Cluster_3_accuracy)), Cluster_3_accuracy, label=\"larger learning rate\")\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 3 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(range(len(Avg_acc_cluster4)), Avg_acc_cluster4,label=\"smaller learning rate\")\n",
    "#plt.plot(range(len(Cluster_4_accuracy)), Cluster_4_accuracy, label=\"larger learning rate\")\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 4 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abs_vect)\n",
    "print(w_glob[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_glob=copy.deepcopy(w_glob_in)\n",
    "print(w_glob[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acc=0\n",
    "#print(sum(noise_acc[20]))\n",
    "for i in range(25):\n",
    "    avg_acc=avg_acc+sum(noise_acc[i])/200\n",
    "#avg_acc=(sum(Avg_acc_cluster0)+sum(Avg_acc_cluster3)+sum(Avg_acc_cluster4))/(216*3)\n",
    "avg_acc=avg_acc/25\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Cluster 0 accuracy\")\n",
    "print(Avg_acc_cluster0)\n",
    "print(\"Cluster 1 accuracy\")\n",
    "print(Avg_acc_cluster1)\n",
    "print(\"Cluster 2 accuracy\")\n",
    "print(Avg_acc_cluster2)\n",
    "print(\"Cluster 3 accuracy\")\n",
    "print(Avg_acc_cluster3)\n",
    "print(\"Cluster 4 accuracy\")\n",
    "print(Avg_acc_cluster4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1e-2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
