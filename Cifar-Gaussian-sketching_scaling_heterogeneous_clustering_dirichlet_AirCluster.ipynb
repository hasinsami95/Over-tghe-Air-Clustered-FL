{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from numpy import *\n",
    "\n",
    "from utils.sampling import mnist_iid_cluster, mnist_noniid_cluster, cifar_noniid_cluster, cifar_dirichlet_varying_users_class_v2\n",
    "\n",
    "from models_v4.Update import LocalTrain,ClusterDetect\n",
    "from models_v4.Nets import CNNMnist, CNNCifar,  CNNMnist2\n",
    "from models_v4.Fed import FedAvg_vectorization\n",
    "\n",
    "from models_v4.test import test_acc\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "class my_argument:    \n",
    "    epochs = 10   #\"rounds of training\"\n",
    "    num_users = 5 # \"number of users: K\"\n",
    "    frac = 0.1 #\"the fraction of clients: C\"\n",
    "    local_ep=5 #\"the number of local epochs: E\"\n",
    "    local_bs=50 #\"local batch size: B\"\n",
    "    bs=128 #\"test batch size\"\n",
    "    lr=0.0001 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "   \n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=0\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=3#, help=\"number of channels of images\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    cluster=5\n",
    "    opt='ADAM'\n",
    "args = my_argument()\n",
    "#args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "print(args.epochs)\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "6\n",
      "(5000, 1000)\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "encoder1=[]\n",
    "encoder2=[]\n",
    "decoder1=[]\n",
    "decoder2=[]\n",
    "A_bar1=[]\n",
    "A_bar2=[]\n",
    "U1=[]\n",
    "U2=[]\n",
    "mu=0\n",
    "sigma=1\n",
    "#alpha=0.001\n",
    "for i in range(args.cluster):\n",
    "    encoder1.append([])\n",
    "    decoder1.append([])\n",
    "    encoder2.append([])\n",
    "    decoder2.append([])\n",
    "    A_bar1.append([])\n",
    "    A_bar2.append([])\n",
    "    U1.append([])\n",
    "    U2.append([])\n",
    "    decoder1.append([])\n",
    "    decoder2.append([])\n",
    "part=1000; # the size of vector that we want to send each time\n",
    "total_size=62006 #total length of the vector\n",
    "d1=part\n",
    "slot=math.ceil(total_size/part) # the no. of slots we need to send vectors\n",
    "d2=total_size%part\n",
    "slot1=math.ceil(total_size/d1)-1\n",
    "slot2=total_size%d1\n",
    "print(slot)\n",
    "print(d2)\n",
    "K=args.cluster\n",
    "for i in range(args.cluster):\n",
    "    num1=np.random.normal(mu, sigma, K*d1*d1)\n",
    "    #num1=np.random.rand(K*d1*d1)\n",
    "    num2=np.random.normal(mu, sigma, K*d2*d2)\n",
    "    #num2=np.random.rand(K*d2*d2)\n",
    "    J1=np.random.rand(d1,d1)\n",
    "    for j in range(K):\n",
    "        if (j!=i):\n",
    "            encoder=np.zeros((d1,d1))\n",
    "            J1=np.vstack([J1,encoder])\n",
    "        else:\n",
    "            encoder=np.identity(d1)\n",
    "            J1=np.vstack([J1,encoder])\n",
    "    J1 = np.delete(J1,np.s_[0:d1], axis=0)\n",
    "    encoder1[i]=J1\n",
    "    #encoder2[i]=num2.reshape(K*d2,d2)\n",
    "    #divide the vector into 15 parts of size 10000 each and one part of size 9010\n",
    "#define encoder[0],encoder[1],encoder[2],encoder[3]\n",
    "K=args.cluster\n",
    "print(encoder1[1].shape)\n",
    "#N_R=5#no of receiver antenna\n",
    "for i in range(K): # K= no. of clusters\n",
    "    J1=np.random.rand(K*d1,d1)\n",
    "    #J2=np.random.rand(K*d2,d2)\n",
    "    #print(type(J2))\n",
    "    print(type(encoder1[i]))\n",
    "    for j in range(args.cluster):\n",
    "        if(j!=i):\n",
    "            J1=np.hstack([J1,encoder1[j]]) #concatenating different encoder matrices\n",
    "            #J1.append(encoder1[j])\n",
    "            #J2=np.hstack([J2,encoder2[j]])\n",
    "    print(J1.shape)\n",
    "    J1 = np.delete(J1,np.s_[0:d1], axis=1)# delete the initial d rows in J\n",
    "    print(J1.shape)\n",
    "    #J2 = np.delete(J2,np.s_[0:d2], axis=1)\n",
    "    #print(J2.shape)\n",
    "    A_bar1[i]=J1\n",
    "    #A_bar2[i]=J2\n",
    "    print(A_bar1[i].shape)\n",
    "    #print(A_bar2[i].shape)\n",
    "    print(\"printing type of A_bar\")\n",
    "    print(type(A_bar1[i]))\n",
    "    U1[i]=null_space((A_bar1[i].transpose()))\n",
    "    print(U1[i].shape)\n",
    "    #U2[i]=null_space((A_bar2[i].transpose()))\n",
    "    #print(U2[i].shape)\n",
    "    decoder1[i]=(np.linalg.inv((U1[i].transpose())@(encoder1[i])) @(U1[i].transpose()))\n",
    "    #decoder2[i]=(np.linalg.inv((U2[i].transpose())@(encoder2[i])) @(U2[i].transpose()))\n",
    "    #decoder1[i]=\n",
    "    print(\"printing size of decoder1\")\n",
    "    print(decoder1[0].shape)\n",
    "    print(type(decoder1))\n",
    "    #print(\"printing size of decoder2\")\n",
    "    #print(decoder2[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n",
      "5\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# load dataset and split users\n",
    "u=[4,5,8,12,15]\n",
    "u=[3,4,4,4,10]\n",
    "u=[10,4,4,4,3]\n",
    "u=[15,3,3,2,2]\n",
    "#u=[5,5,5,5,5]\n",
    "#u=[2,5,5,6,7]\n",
    "#u=[2,3,4,4,12]\n",
    "#u=[5,5,5,5,5]\n",
    "#u=[2,2,3,3,15]\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    count=0\n",
    "    #print(len(dataset_train))\n",
    "    dict_users=[] #2D array in each row, users of a particular cluster\n",
    "    train_data=[]\n",
    "    test_data=[]\n",
    "    for j in range(args.cluster):\n",
    "        train_data.append([])\n",
    "        test_data.append([])\n",
    "        dict_users.append([])\n",
    "    for j in range(len(dataset_train)):\n",
    "        data,label=j\n",
    "        if (label==0) | (label==1):\n",
    "            train_data[0].append(dataset_train[j])\n",
    "        elif (label==2) | (label==3):\n",
    "            train_data[1].append(dataset_train[j])\n",
    "        elif (label==4) | (label==5):\n",
    "            train_data[2].append(dataset_train[j])\n",
    "        elif (label==6) | (label==7):\n",
    "            train_data[3].append(dataset_train[j])\n",
    "        elif (label==8) | (label==9):\n",
    "            train_data[4].append(dataset_train[j])\n",
    "    for j in range(len(dataset_test)):\n",
    "        data,label=j\n",
    "        if (label==0) | (label==1):\n",
    "            test_data[0].append(dataset_test[j])\n",
    "        elif (label==2) | (label==3):\n",
    "            test_data[1].append(dataset_test[j])\n",
    "        elif (label==4) | (label==5):\n",
    "            test_data[2].append(dataset_test[j])\n",
    "        elif (label==6) | (label==7):\n",
    "            test_data[3].append(dataset_test[j])\n",
    "        elif (label==8) | (label==9):\n",
    "            test_data[4].append(dataset_test[j])\n",
    "    \n",
    "#defining 5 different types of datasets for 5 different clusters\n",
    "    \n",
    "    if args.iid:\n",
    "        for cluster_no in range(args.cluster):\n",
    "            dict_users[cluster_no] = mnist_iid_cluster(train_data[cluster_no], args.num_users)\n",
    "    else:\n",
    "        for cluster_no in range(args.cluster):\n",
    "            dict_users[cluster_no] = mnist_noniid_cluster(train_data[cluster_no], args.num_users)\n",
    "elif args.dataset == 'cifar':\n",
    "    trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    dataset_train = datasets.CIFAR10('./data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "    dataset_test = datasets.CIFAR10('./data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "#defining 5 different types of datasets for 5 different clusters\n",
    "    count=0\n",
    "    #print(len(dataset_train))\n",
    "    dict_users=[] #2D array in each row, users of a particular cluster\n",
    "    train_data=[]\n",
    "    test_data=[]\n",
    "    for j in range(args.cluster):\n",
    "        train_data.append([])\n",
    "        test_data.append([])\n",
    "        dict_users.append([])\n",
    "    for j in range(len(dataset_train)):\n",
    "        data,label=dataset_train[j]\n",
    "        if (label==0) | (label==1):\n",
    "            train_data[0].append(dataset_train[j])\n",
    "        elif (label==2) | (label==3):\n",
    "            train_data[1].append(dataset_train[j])\n",
    "        elif (label==4) | (label==5):\n",
    "            train_data[2].append(dataset_train[j])\n",
    "        elif (label==6) | (label==7):\n",
    "            train_data[3].append(dataset_train[j])\n",
    "        elif (label==8) | (label==9):\n",
    "            train_data[4].append(dataset_train[j])\n",
    "    for j in range(len(dataset_test)):\n",
    "        data,label=dataset_test[j]\n",
    "        if (label==0) | (label==1):\n",
    "            test_data[0].append(dataset_test[j])\n",
    "        elif (label==2) | (label==3):\n",
    "            test_data[1].append(dataset_test[j])\n",
    "        elif (label==4) | (label==5):\n",
    "            test_data[2].append(dataset_test[j])\n",
    "        elif (label==6) | (label==7):\n",
    "            test_data[3].append(dataset_test[j])\n",
    "        elif (label==8) | (label==9):\n",
    "            test_data[4].append(dataset_test[j])\n",
    "\n",
    "    if args.iid:\n",
    "        for cluster_no in range(args.cluster):\n",
    "            dict_users[cluster_no] = cifar_iid_cluster(train_data[cluster_no], u[cluster_no])\n",
    "    else:\n",
    "        for cluster_no in range(args.cluster):\n",
    "            dict_users[cluster_no] = cifar_dirichlet_varying_users_class_v2(train_data[cluster_no], u[cluster_no],args.cluster)\n",
    "else:\n",
    "    exit('Error: dataset not found')\n",
    "img_size = dataset_train[0][0].shape\n",
    "#print(dict_users[0])\n",
    "#print((dict_users[0][4]))\n",
    "print(len(dataset_train))\n",
    "print(len(dict_users))\n",
    "print(len(train_data[0]))\n",
    "#print(train_data[0])\n",
    "#idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "#print(idxs_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dict_users[0][0]))\n",
    "print(len(dict_users[0][1]))\n",
    "print(len(dict_users[0][2]))\n",
    "print(len(dict_users[0][3]))\n",
    "print(len(dict_users[0][4]))\n",
    "print(len(dict_users[0][5]))\n",
    "print(len(dict_users[0][6]))\n",
    "print(len(dict_users[0][7]))\n",
    "print(len(dict_users[0][8]))\n",
    "print(len(dict_users[0][9]))\n",
    "print(len(dict_users[0][10]))\n",
    "print(len(dict_users[0][11]))\n",
    "print(len(dict_users[0][12]))\n",
    "print(len(dict_users[0][13]))\n",
    "print(len(dict_users[0][14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "#print(use_cuda)\n",
    "args.device = torch.device(\"cuda:3\" if use_cuda else \"cpu\")\n",
    "#args.device=torch.device(\"cpu\")\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[], [], [], [], []], [[], [], [], [], []], [[], [], [], [], []], [[], [], [], [], []], [[], [], [], [], []]]\n"
     ]
    }
   ],
   "source": [
    "acc_test=[]\n",
    "acc_test_arr=[]\n",
    "loss_test=[]\n",
    "loss_test_arr=[]\n",
    "for cluster_no in range(args.cluster):\n",
    "    acc_test.append([])\n",
    "    loss_test.append([])\n",
    "    acc_test_arr.append([])\n",
    "    loss_test_arr.append([])\n",
    "for cluster_no in range(args.cluster):\n",
    "    for i in range(args.cluster):\n",
    "        acc_test_arr[cluster_no].append([])\n",
    "        loss_test_arr[cluster_no].append([])\n",
    "print(acc_test_arr)\n",
    "noise_acc=[]\n",
    "for user in range(25):\n",
    "    noise_acc.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        seed=123\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        nn.init.xavier_uniform(m.weight.data, nn.init.calculate_gain('relu'))\n",
    "        #nn.init.xavier_uniform(m.bias.data)\n",
    "        torch.nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        seed=123\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        #torch.nn.init.xavier_uniform_(m.bias.data)\n",
    "        torch.nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        seed=123\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        #torch.nn.init.xavier_uniform_(m.bias.data)\n",
    "        torch.nn.init.zeros_(m.bias.data)\n",
    "        #conv1.bias.data.fill_(0.01)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62006\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "from models_v4.Fed import weight_vectorization,FedSubstract,FedAvg_gradient,FedAdd\n",
    "from models_v4.Fed import FedAdd,FedSubstract,weight_vectorization_gen,FedAvg_gradient\n",
    "import numpy as np\n",
    "import copy\n",
    "if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "    net_glob=[]\n",
    "    for i in range(args.cluster):\n",
    "        net_glob.append(CNNCifar(args=args).to(args.device))\n",
    "elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "    net_glob=[]\n",
    "    for i in range(args.cluster):\n",
    "        net_glob.append(CNNMnist2(args=args).to(args.device))\n",
    "\n",
    "else:\n",
    "    exit('Error: model not found')\n",
    "#print(net_glob)\n",
    "w_glob=[]\n",
    "# copy weights\n",
    "for i in range(args.cluster):\n",
    "    net_glob[i].train()\n",
    "    w_glob.append(net_glob[i].state_dict())\n",
    "abs_vect,layer_size=weight_vectorization_gen(w_glob[2])\n",
    "net_glob_in=copy.deepcopy(net_glob)\n",
    "w_glob_in=copy.deepcopy(w_glob)\n",
    "print(len(abs_vect))\n",
    "#print(\"weights\")\n",
    "#print(w_glob)\n",
    "#print(w_glob.shape)\n",
    "#print(w_glob[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_glob_in[0]=torch.load(\"net1_2.pt\")\n",
    "w_glob_in[1]=torch.load(\"net2_2.pt\")\n",
    "w_glob_in[2]=torch.load(\"net3_2.pt\")\n",
    "w_glob_in[3]=torch.load(\"net4_2.pt\")\n",
    "w_glob_in[4]=torch.load(\"net5_2.pt\")\n",
    "net_glob_in[0].load_state_dict(w_glob_in[0])\n",
    "net_glob_in[1].load_state_dict(w_glob_in[1])\n",
    "net_glob_in[2].load_state_dict(w_glob_in[2])\n",
    "net_glob_in[3].load_state_dict(w_glob_in[3])\n",
    "net_glob_in[4].load_state_dict(w_glob_in[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "[15, 17, 22]\n",
      "[0, 2, 3, 4, 6, 19, 21]\n",
      "[1, 5, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[16, 18, 20]\n",
      "[23, 24]\n",
      "tensor(50.)\n",
      "tensor(0.)\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "iteration number 1\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(54.5000)\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "iteration number 2\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(58.0500)\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "tensor(63.7000)\n",
      "iteration number 3\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(62.4000)\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "tensor(65.1000)\n",
      "iteration number 4\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(50.)\n",
      "tensor(59.4000)\n",
      "tensor(59.4000)\n",
      "tensor(59.4000)\n",
      "tensor(54.8500)\n",
      "iteration number 5\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(62.8000)\n",
      "tensor(78.)\n",
      "tensor(78.)\n",
      "tensor(78.)\n",
      "tensor(64.2500)\n",
      "iteration number 6\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(65.9500)\n",
      "tensor(73.2000)\n",
      "tensor(73.2000)\n",
      "tensor(73.2000)\n",
      "tensor(63.8000)\n",
      "iteration number 7\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(67.0500)\n",
      "tensor(69.7000)\n",
      "tensor(69.7000)\n",
      "tensor(69.7000)\n",
      "tensor(68.1000)\n",
      "iteration number 8\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(68.6500)\n",
      "tensor(77.4500)\n",
      "tensor(77.4500)\n",
      "tensor(77.4500)\n",
      "tensor(62.9500)\n",
      "iteration number 9\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(71.)\n",
      "tensor(72.8000)\n",
      "tensor(72.8000)\n",
      "tensor(72.8000)\n",
      "tensor(65.2000)\n",
      "iteration number 10\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(69.6000)\n",
      "tensor(73.0500)\n",
      "tensor(73.0500)\n",
      "tensor(73.0500)\n",
      "tensor(69.7500)\n",
      "iteration number 11\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(70.7000)\n",
      "tensor(76.9500)\n",
      "tensor(76.9500)\n",
      "tensor(76.9500)\n",
      "tensor(71.4000)\n",
      "iteration number 12\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(68.7500)\n",
      "tensor(76.6500)\n",
      "tensor(76.6500)\n",
      "tensor(76.6500)\n",
      "tensor(73.4500)\n",
      "iteration number 13\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(69.4500)\n",
      "tensor(62.9000)\n",
      "tensor(62.9000)\n",
      "tensor(62.9000)\n",
      "tensor(69.5000)\n",
      "iteration number 14\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(64.8500)\n",
      "tensor(74.4000)\n",
      "tensor(74.4000)\n",
      "tensor(74.4000)\n",
      "tensor(73.7500)\n",
      "iteration number 15\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(69.)\n",
      "tensor(77.7000)\n",
      "tensor(77.7000)\n",
      "tensor(77.7000)\n",
      "tensor(71.3000)\n",
      "iteration number 16\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(69.7000)\n",
      "tensor(64.7000)\n",
      "tensor(64.7000)\n",
      "tensor(64.7000)\n",
      "tensor(76.0500)\n",
      "iteration number 17\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(69.2500)\n",
      "tensor(75.2500)\n",
      "tensor(75.2500)\n",
      "tensor(75.2500)\n",
      "tensor(76.6000)\n",
      "iteration number 18\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(71.0500)\n",
      "tensor(78.)\n",
      "tensor(78.)\n",
      "tensor(78.)\n",
      "tensor(76.2000)\n",
      "iteration number 19\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.7500)\n",
      "tensor(79.2000)\n",
      "tensor(79.2000)\n",
      "tensor(79.2000)\n",
      "tensor(76.4000)\n",
      "iteration number 20\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(67.4500)\n",
      "tensor(63.1000)\n",
      "tensor(63.1000)\n",
      "tensor(63.1000)\n",
      "tensor(68.0500)\n",
      "iteration number 21\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(65.2000)\n",
      "tensor(76.7000)\n",
      "tensor(76.7000)\n",
      "tensor(76.7000)\n",
      "tensor(69.9000)\n",
      "iteration number 22\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(66.3000)\n",
      "tensor(78.9000)\n",
      "tensor(78.9000)\n",
      "tensor(78.9000)\n",
      "tensor(76.9500)\n",
      "iteration number 23\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(71.4500)\n",
      "tensor(76.0500)\n",
      "tensor(76.0500)\n",
      "tensor(76.0500)\n",
      "tensor(77.8000)\n",
      "iteration number 24\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.3500)\n",
      "tensor(78.6000)\n",
      "tensor(78.6000)\n",
      "tensor(78.6000)\n",
      "tensor(74.4000)\n",
      "iteration number 25\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(71.6500)\n",
      "tensor(79.1000)\n",
      "tensor(79.1000)\n",
      "tensor(79.1000)\n",
      "tensor(72.3500)\n",
      "iteration number 26\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.4000)\n",
      "tensor(79.1500)\n",
      "tensor(79.1500)\n",
      "tensor(79.1500)\n",
      "tensor(72.6500)\n",
      "iteration number 27\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(65.3500)\n",
      "tensor(79.2000)\n",
      "tensor(79.2000)\n",
      "tensor(79.2000)\n",
      "tensor(76.5000)\n",
      "iteration number 28\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(70.6000)\n",
      "tensor(79.7500)\n",
      "tensor(79.7500)\n",
      "tensor(79.7500)\n",
      "tensor(78.1500)\n",
      "iteration number 29\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(71.5500)\n",
      "tensor(79.2500)\n",
      "tensor(79.2500)\n",
      "tensor(79.2500)\n",
      "tensor(77.2000)\n",
      "iteration number 30\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.0500)\n",
      "tensor(79.4500)\n",
      "tensor(79.4500)\n",
      "tensor(79.4500)\n",
      "tensor(77.9000)\n",
      "iteration number 31\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(70.7000)\n",
      "tensor(80.3500)\n",
      "tensor(80.3500)\n",
      "tensor(80.3500)\n",
      "tensor(77.3500)\n",
      "iteration number 32\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.3500)\n",
      "tensor(80.2500)\n",
      "tensor(80.2500)\n",
      "tensor(80.2500)\n",
      "tensor(77.3000)\n",
      "iteration number 33\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.3000)\n",
      "tensor(78.9000)\n",
      "tensor(78.9000)\n",
      "tensor(78.9000)\n",
      "tensor(76.1000)\n",
      "iteration number 34\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(72.2000)\n",
      "tensor(80.5500)\n",
      "tensor(80.5500)\n",
      "tensor(80.5500)\n",
      "tensor(76.0500)\n",
      "iteration number 35\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(72.9000)\n",
      "tensor(80.3000)\n",
      "tensor(80.3000)\n",
      "tensor(80.3000)\n",
      "tensor(72.2000)\n",
      "iteration number 36\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.0500)\n",
      "tensor(80.7000)\n",
      "tensor(80.7000)\n",
      "tensor(80.7000)\n",
      "tensor(75.2000)\n",
      "iteration number 37\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.1000)\n",
      "tensor(80.5000)\n",
      "tensor(80.5000)\n",
      "tensor(80.5000)\n",
      "tensor(76.2000)\n",
      "iteration number 38\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.1500)\n",
      "tensor(80.8500)\n",
      "tensor(80.8500)\n",
      "tensor(80.8500)\n",
      "tensor(78.1000)\n",
      "iteration number 39\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(72.)\n",
      "tensor(79.8000)\n",
      "tensor(79.8000)\n",
      "tensor(79.8000)\n",
      "tensor(78.7000)\n",
      "iteration number 40\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.4000)\n",
      "tensor(79.9500)\n",
      "tensor(79.9500)\n",
      "tensor(79.9500)\n",
      "tensor(77.8500)\n",
      "iteration number 41\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(72.1500)\n",
      "tensor(80.1500)\n",
      "tensor(80.1500)\n",
      "tensor(80.1500)\n",
      "tensor(78.8500)\n",
      "iteration number 42\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(70.1000)\n",
      "tensor(81.5500)\n",
      "tensor(81.5500)\n",
      "tensor(81.5500)\n",
      "tensor(75.5500)\n",
      "iteration number 43\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.1500)\n",
      "tensor(81.7500)\n",
      "tensor(81.7500)\n",
      "tensor(81.7500)\n",
      "tensor(75.9500)\n",
      "iteration number 44\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.2500)\n",
      "tensor(81.6500)\n",
      "tensor(81.6500)\n",
      "tensor(81.6500)\n",
      "tensor(77.7000)\n",
      "iteration number 45\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.5000)\n",
      "tensor(81.4000)\n",
      "tensor(81.4000)\n",
      "tensor(81.4000)\n",
      "tensor(79.1500)\n",
      "iteration number 46\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.6500)\n",
      "tensor(81.3500)\n",
      "tensor(81.3500)\n",
      "tensor(81.3500)\n",
      "tensor(78.4500)\n",
      "iteration number 47\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.1500)\n",
      "tensor(81.5500)\n",
      "tensor(81.5500)\n",
      "tensor(81.5500)\n",
      "tensor(78.2500)\n",
      "iteration number 48\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(72.4500)\n",
      "tensor(82.2500)\n",
      "tensor(82.2500)\n",
      "tensor(82.2500)\n",
      "tensor(76.1000)\n",
      "iteration number 49\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.1500)\n",
      "tensor(81.8000)\n",
      "tensor(81.8000)\n",
      "tensor(81.8000)\n",
      "tensor(79.0500)\n",
      "iteration number 50\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.7500)\n",
      "tensor(81.8500)\n",
      "tensor(81.8500)\n",
      "tensor(81.8500)\n",
      "tensor(78.8500)\n",
      "iteration number 51\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.5000)\n",
      "tensor(81.6000)\n",
      "tensor(81.6000)\n",
      "tensor(81.6000)\n",
      "tensor(79.)\n",
      "iteration number 52\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.8000)\n",
      "tensor(82.1000)\n",
      "tensor(82.1000)\n",
      "tensor(82.1000)\n",
      "tensor(78.2500)\n",
      "iteration number 53\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.7000)\n",
      "tensor(79.7500)\n",
      "tensor(79.7500)\n",
      "tensor(79.7500)\n",
      "tensor(78.2500)\n",
      "iteration number 54\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.1500)\n",
      "tensor(82.5000)\n",
      "tensor(82.5000)\n",
      "tensor(82.5000)\n",
      "tensor(78.9500)\n",
      "iteration number 55\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(75.)\n",
      "tensor(82.5000)\n",
      "tensor(82.5000)\n",
      "tensor(82.5000)\n",
      "tensor(78.8000)\n",
      "iteration number 56\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(75.5500)\n",
      "tensor(82.7000)\n",
      "tensor(82.7000)\n",
      "tensor(82.7000)\n",
      "tensor(79.5500)\n",
      "iteration number 57\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(75.5500)\n",
      "tensor(81.7500)\n",
      "tensor(81.7500)\n",
      "tensor(81.7500)\n",
      "tensor(79.5000)\n",
      "iteration number 58\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(75.3000)\n",
      "tensor(82.6500)\n",
      "tensor(82.6500)\n",
      "tensor(82.6500)\n",
      "tensor(79.)\n",
      "iteration number 59\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.3500)\n",
      "tensor(82.9000)\n",
      "tensor(82.9000)\n",
      "tensor(82.9000)\n",
      "tensor(79.1500)\n",
      "iteration number 60\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.8500)\n",
      "tensor(82.7000)\n",
      "tensor(82.7000)\n",
      "tensor(82.7000)\n",
      "tensor(75.8000)\n",
      "iteration number 61\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.1000)\n",
      "tensor(82.7500)\n",
      "tensor(82.7500)\n",
      "tensor(82.7500)\n",
      "tensor(79.4000)\n",
      "iteration number 62\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.1000)\n",
      "tensor(83.2500)\n",
      "tensor(83.2500)\n",
      "tensor(83.2500)\n",
      "tensor(79.2500)\n",
      "iteration number 63\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(72.4500)\n",
      "tensor(82.6500)\n",
      "tensor(82.6500)\n",
      "tensor(82.6500)\n",
      "tensor(76.6500)\n",
      "iteration number 64\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.7000)\n",
      "tensor(83.4000)\n",
      "tensor(83.4000)\n",
      "tensor(83.4000)\n",
      "tensor(78.3500)\n",
      "iteration number 65\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(75.2500)\n",
      "tensor(82.5500)\n",
      "tensor(82.5500)\n",
      "tensor(82.5500)\n",
      "tensor(79.6500)\n",
      "iteration number 66\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.)\n",
      "tensor(83.3500)\n",
      "tensor(83.3500)\n",
      "tensor(83.3500)\n",
      "tensor(79.7500)\n",
      "iteration number 67\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.5000)\n",
      "tensor(83.2500)\n",
      "tensor(83.2500)\n",
      "tensor(83.2500)\n",
      "tensor(79.0500)\n",
      "iteration number 68\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.8000)\n",
      "tensor(83.2000)\n",
      "tensor(83.2000)\n",
      "tensor(83.2000)\n",
      "tensor(79.0500)\n",
      "iteration number 69\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.7500)\n",
      "tensor(83.1000)\n",
      "tensor(83.1000)\n",
      "tensor(83.1000)\n",
      "tensor(79.8000)\n",
      "iteration number 70\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.7000)\n",
      "tensor(83.3500)\n",
      "tensor(83.3500)\n",
      "tensor(83.3500)\n",
      "tensor(79.2500)\n",
      "iteration number 71\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.4500)\n",
      "tensor(83.6000)\n",
      "tensor(83.6000)\n",
      "tensor(83.6000)\n",
      "tensor(77.6500)\n",
      "iteration number 72\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.9500)\n",
      "tensor(83.6500)\n",
      "tensor(83.6500)\n",
      "tensor(83.6500)\n",
      "tensor(79.8500)\n",
      "iteration number 73\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(73.4000)\n",
      "tensor(83.4000)\n",
      "tensor(83.4000)\n",
      "tensor(83.4000)\n",
      "tensor(79.5500)\n",
      "iteration number 74\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.1500)\n",
      "tensor(83.5000)\n",
      "tensor(83.5000)\n",
      "tensor(83.5000)\n",
      "tensor(79.8500)\n",
      "iteration number 75\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.4000)\n",
      "tensor(83.4500)\n",
      "tensor(83.4500)\n",
      "tensor(83.4500)\n",
      "tensor(79.6000)\n",
      "iteration number 76\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(75.2000)\n",
      "tensor(83.4000)\n",
      "tensor(83.4000)\n",
      "tensor(83.4000)\n",
      "tensor(79.3500)\n",
      "iteration number 77\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(75.0500)\n",
      "tensor(82.8000)\n",
      "tensor(82.8000)\n",
      "tensor(82.8000)\n",
      "tensor(78.3000)\n",
      "iteration number 78\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.4000)\n",
      "tensor(83.8500)\n",
      "tensor(83.8500)\n",
      "tensor(83.8500)\n",
      "tensor(80.3500)\n",
      "iteration number 79\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.0500)\n",
      "tensor(83.8000)\n",
      "tensor(83.8000)\n",
      "tensor(83.8000)\n",
      "tensor(78.4500)\n",
      "iteration number 80\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(75.)\n",
      "tensor(83.8000)\n",
      "tensor(83.8000)\n",
      "tensor(83.8000)\n",
      "tensor(79.5000)\n",
      "iteration number 81\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.8000)\n",
      "tensor(83.7000)\n",
      "tensor(83.7000)\n",
      "tensor(83.7000)\n",
      "tensor(79.9500)\n",
      "iteration number 82\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(74.4000)\n",
      "tensor(84.0500)\n",
      "tensor(84.0500)\n",
      "tensor(84.0500)\n",
      "tensor(79.6500)\n",
      "iteration number 83\n",
      "[15, 16, 17]\n",
      "[21, 22]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[18, 19, 20]\n",
      "[23, 24]\n",
      "tensor(75.4000)\n",
      "tensor(83.9500)\n",
      "tensor(83.9500)\n",
      "tensor(83.9500)\n",
      "tensor(80.3500)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8dcdd5364484>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mloss_test_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcluster_block\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_glob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_test_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0mnoise_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gulerlab/Documents/Hasin/Github/Over-the-Air-Clustered-FL-main/models_v4/test.py\u001b[0m in \u001b[0;36mtest_acc\u001b[0;34m(net_g, datatest, args)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# build model\n",
    "d=62006\n",
    "from models_v4.Fed import weight_vectorization_cifar,FedAdd,FedSubstract,FedAvg_gradient\n",
    "import numpy as np\n",
    "import copy\n",
    "slot=math.ceil(d/d1) # the no. of slots we need to send vectors\n",
    "w_glob=copy.deepcopy(w_glob_in)\n",
    "net_glob=copy.deepcopy(net_glob_in)\n",
    "#u=[5,8,10,12,15]\n",
    "#u=range(0,50)\n",
    "idx_users=range(0,25)\n",
    "# training\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "num=[]\n",
    "import random\n",
    "D=part #dimension of noise vector = Nr= Kd\n",
    "mu=0\n",
    "sigma=1\n",
    "K=args.cluster\n",
    "\n",
    "#hist_ = np.zeros(10,dtype=int)\n",
    "sample=0 # fro the purpose of using fresh samples in each iteration\n",
    "args.lr=0.0001\n",
    "for iter in range(1000): #args.epochs\n",
    "    print(\"iteration number\",iter)\n",
    "    if(iter%4==0):\n",
    "        sample=0\n",
    "    encoded=[]\n",
    "    decoded=[]\n",
    "    num=[]\n",
    "    noise=[] #noise vector\n",
    "  \n",
    "    num=np.random.normal(mu, sigma, K*D)\n",
    "    num=np.transpose(num.reshape(1,len(num)))\n",
    "    #m = 10\n",
    "    loss_train=[]\n",
    "    cluster_block=[]\n",
    "    noise_dec=[] #decoded noise vector\n",
    "    for cluster_no in range(args.cluster):\n",
    "        loss_train.append([])\n",
    "        cluster_block.append([])\n",
    "        decoded.append([])\n",
    "        noise_dec.append([])\n",
    "    Power=[]\n",
    "    Power2=[]\n",
    "    alpha=[]\n",
    "    feature_vector2=[]\n",
    "    for i in range(args.cluster):\n",
    "        Power.append([])\n",
    "        Power2.append([])\n",
    "        alpha.append([])\n",
    "        feature_vector2.append(np.zeros((d,1)))\n",
    "                \n",
    "    #idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    idx_users=[]\n",
    "    sorted_train_data=[]\n",
    "    sorted_test_data=[]\n",
    "    for cluster_no in range(args.cluster):\n",
    "        for index in dict_users[cluster_no]:\n",
    "            idx_users.append(index) # putting the data indices of users in this list\n",
    "            sorted_train_data.append(train_data[cluster_no])#putting the corresponding training data in this array\n",
    "            sorted_test_data.append(test_data[cluster_no])\n",
    "            \n",
    "    for user in range(len(idx_users)): # no. of loop= no. of users\n",
    "        cluster_loss=[]\n",
    "        L=len(idx_users[user])\n",
    "#         L=1000\n",
    "#         L=600\n",
    "        sample_size=int(L/8)\n",
    "        sample_size=int(len(idx_users[user])/8)\n",
    "        #sample_size=int(L/2)\n",
    "        #sample_size=250\n",
    "        for i in range(args.cluster):\n",
    "             # each type of dataset belong to 5 users\n",
    "            local2 = ClusterDetect(args=args, dataset= sorted_train_data[user], idxs=idx_users[user][(sample*sample_size):(sample+1)*sample_size])\n",
    "            #local2 = ClusterDetect(args=args, dataset= sorted_train_data[user], idxs=idx_users[user][0:599])\n",
    "            # using 1st 600 data\n",
    "            w2, loss2 = local2.train2(net=copy.deepcopy(net_glob[i]).to(args.device))\n",
    "            cluster_loss.append(loss2)\n",
    "        #print(\"printing user\")\n",
    "        #print(user)\n",
    "        #print(cluster_loss)\n",
    "        minimum=min(cluster_loss)\n",
    "        index_of_min=cluster_loss.index(minimum)\n",
    "        cluster_block[index_of_min].append(user)\n",
    "    print(cluster_block[0])\n",
    "    print(cluster_block[1])\n",
    "    print(cluster_block[2])\n",
    "    print(cluster_block[3])\n",
    "    print(cluster_block[4])\n",
    "    K_global=part\n",
    "    m_global=np.zeros((d,1))\n",
    "    location_global=np.random.choice(range(d),K_global,replace=False)\n",
    "    for i in location_global:\n",
    "        m_global[i]=1\n",
    "    updated=[]\n",
    "    model_diff=[]\n",
    "    grad_vect=[]\n",
    "    prev=[]\n",
    "    error=[]\n",
    "    grad_vect_send=[]\n",
    "    store_grad=[]\n",
    "    for i in range(50):\n",
    "        updated.append([])\n",
    "        model_diff.append([])\n",
    "        grad_vect.append([])\n",
    "        prev.append([])\n",
    "        error.append(np.zeros((d,1)))\n",
    "        grad_vect_send.append([])\n",
    "        store_grad.append([])\n",
    "    b=part  \n",
    "    sigma_R=1/math.sqrt(b)\n",
    "    mu_R=0\n",
    "    R=np.random.normal(mu_R, sigma_R, (b,total_size))\n",
    "    superposition=0\n",
    "    for cluster_no in range(args.cluster):\n",
    "        w_locals, loss_locals,grad_locals,diff_locals= [],[],[],[]\n",
    "        if(cluster_block[cluster_no]==[]):\n",
    "            continue\n",
    "        for user2 in cluster_block[cluster_no]:\n",
    "            trans=np.array([])\n",
    "            #local = LocalUpdate(args=args, dataset=sorted_train_data[user], idxs=idx_users[user][(sample+1)*600:(sample+2)*600])\n",
    "            #local = LocalUpdate(args=args, dataset=sorted_train_data[user], idxs=idx_users[user][600:1199])\n",
    "            #print(user2)\n",
    "            total_P=0\n",
    "            updated[user2]=copy.deepcopy(w_glob[cluster_no])\n",
    "            L=len(idx_users[user2])\n",
    "            L=1000\n",
    "            L=600\n",
    "            sample_size=int(L/8)\n",
    "            sample_size=int(len(idx_users[user2])/8)\n",
    "            #sample_size=250\n",
    "            local = LocalTrain(args=args, dataset=sorted_train_data[user2], idxs=idx_users[user2][(sample+1)*sample_size:(sample+2)*sample_size])\n",
    "            #using 2nd half data\n",
    "            w, loss = local.train(net=copy.deepcopy(net_glob[cluster_no]).to(args.device))\n",
    "            w_locals.append(copy.deepcopy(w))\n",
    "            loss_locals.append(copy.deepcopy(loss))\n",
    "            prev[user2]=updated[user2]\n",
    "            model_diff[user2]=FedSubstract(w,prev[user2])\n",
    "            grad_vect[user2],layer_size=weight_vectorization_gen(model_diff[user2]) # vectorizing the gradient\n",
    "            #grad_vect[user2]=grad_vect[user2]+error[user2] # error feedback\n",
    "            array_one=np.ones((d,1))\n",
    "            model_vector,layer_size=weight_vectorization_gen(w)\n",
    "            M=max(abs(model_vector))\n",
    "            mask=m_global\n",
    "            count=0\n",
    "            #calculating the modified gradient to be sent to server\n",
    "            #grad_vect_send[user2]=np.multiply(mask,grad_vect[user2])\n",
    "            grad_vect_send[user2]= R@grad_vect[user2]\n",
    "            #error[user2]=grad_vect[user2]-grad_vect_send[user2]\n",
    "            grad_locals.append(grad_vect_send[user2]*len(idx_users[user2])/10000)\n",
    "            store_grad[user2]=grad_vect_send[user2]\n",
    "            diff_locals.append(copy.deepcopy(model_diff[user2]))\n",
    "            #print(M)\n",
    "            SNR=math.log10(M**2)\n",
    "            #print(SNR)\n",
    "            #n=K*K*d1*d1\n",
    "            #H = np.random.normal(loc=0, scale=np.sqrt(2)/2, size=(n, 2)).view(np.complex128)\n",
    "            #H=H.reshape(K*d1,K*d1)\n",
    "            #U, D, VT = np.linalg.svd(H)\n",
    "            #HT=np.matrix(H).getH()\n",
    "            #left_inv=np.linalg.inv(HT@H)@HT\n",
    "            #E1= left_inv@(encoder1[cluster_no])\n",
    "            #for i in range(slot1):\n",
    "                #z=(abs(np.array(E1 @ model_vector[i*d1:(i+1)*d1])))**2\n",
    "                #trans=np.concatenate((trans,z[:,0]))\n",
    "                #total_P=total_P+sum(z)\n",
    "            d2=total_size%d1\n",
    "            n=K*K*d2*d2\n",
    "            #H = np.random.normal(loc=0, scale=np.sqrt(2)/2, size=(n, 2)).view(np.complex128)\n",
    "            #H=H.reshape(K*d2,K*d2)\n",
    "            #U, D, VT = np.linalg.svd(H)\n",
    "            #HT=np.matrix(H).getH()\n",
    "            #left_inv=np.linalg.inv(HT@H)@HT\n",
    "#print(left_inv.shape)\n",
    "            #E2= left_inv@(encoder2[cluster_no])\n",
    "            #z=(abs(np.array(E2 @ model_vector[slot1*d1:slot1*d1+d2])))**2\n",
    "            #trans=np.concatenate((trans,z[:,0]))\n",
    "            #M=max(abs(trans))\n",
    "            #print(M)\n",
    "            #total_P=total_P + sum(z)\n",
    "            #print(total_P)\n",
    "            Power[cluster_no].append(total_P)\n",
    "        Power2[cluster_no]=(sum(Power[cluster_no]))/len(cluster_block[cluster_no])\n",
    "        P_limit=2000000000\n",
    "        #alpha[cluster_no]=math.sqrt(P_limit/Power2[cluster_no])\n",
    "        alpha[cluster_no]=1000*100\n",
    "        grad_avg=FedAvg_gradient(grad_locals)\n",
    "        grad_avg=0\n",
    "        for i in grad_locals:\n",
    "            grad_avg=grad_avg+i\n",
    "        loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "        loss_train[cluster_no].append(loss_avg)\n",
    "        #print(len(w_locals))\n",
    "        j=w_locals[0]\n",
    "        feature_vector=grad_avg\n",
    "        #print(len(feature_vector))\n",
    "        #print(feature_vector)\n",
    "        #fecture vector is dx1 vector after flattening all types of weight matrices and concatenaitng them\n",
    "        increment=0 #keeps track of feature_vactor index\n",
    "        superposition=superposition+alpha[cluster_no]*encoder1[cluster_no]@feature_vector\n",
    "    received=superposition\n",
    "    #print(len(superposition))\n",
    "    #superposition=superposition+num\n",
    "    #print(\"total received Power\")\n",
    "    #print(sum((received)**2))\n",
    "    #print(\"received min max\")\n",
    "    #print(max(abs(received)))\n",
    "    #print(min(abs(received)))\n",
    "    m_global=np.zeros((d,1))\n",
    "        \n",
    "    for cluster_no in range(args.cluster):\n",
    "        #print(w_glob[cluster_no])\n",
    "        if(cluster_block[cluster_no]==[]):\n",
    "            continue\n",
    "        flat=[]\n",
    "        for i in range(16): # 4 layers in parameter\n",
    "            flat.append([])\n",
    "        increment=0\n",
    "        decoded[cluster_no]=(1/alpha[cluster_no])*decoder1[cluster_no]@superposition\n",
    "        count=0\n",
    "    \n",
    "        decoded[cluster_no]=R.transpose()@decoded[cluster_no]\n",
    "        count=0\n",
    "        w_glob_prev=copy.deepcopy(w_glob[cluster_no])\n",
    "        for i in range(len(w_glob[cluster_no].keys())): # 4 layers in parameter\n",
    "            flat.append([])\n",
    "\n",
    "        for h in w_glob_prev.keys():\n",
    "            s=list(w_glob[cluster_no][h].shape)\n",
    "            if (len(s)==0):\n",
    "                new=np.array(0)\n",
    "                decoded[cluster_no]=np.delete(decoded[cluster_no],np.s_[0])\n",
    "            else:\n",
    "                z=np.prod(list(w_glob[cluster_no][h].shape))\n",
    "                flat[count]=decoded[cluster_no][0:z] # taking out the vector for the specified layer\n",
    "                decoded[cluster_no]=np.delete(decoded[cluster_no],np.s_[0:z])# deleting that vector from decoded after taking out\n",
    "             \n",
    "                new=flat[count].reshape(list(w_glob[cluster_no][h].shape)) #reshaping back to the marix\n",
    "              \n",
    "            w_glob[cluster_no][h]=torch.from_numpy(new) #converting the matrix to a tensor\n",
    "            #print(w_glob[cluster_no][h].shape)\n",
    "            count=count+1\n",
    "    # update global weights\n",
    "        \n",
    "        global_diff = w_glob[cluster_no]\n",
    "        w_glob[cluster_no]=FedAdd(w_glob_prev,global_diff)\n",
    "        net_glob[cluster_no].load_state_dict(w_glob[cluster_no])\n",
    "        #printing loss in each iteration\n",
    "        for i in range(args.cluster):\n",
    "            acc_test[i], loss_test[i] = test_acc(net_glob[cluster_no], test_data[i], args)\n",
    "            acc_test_arr[i][cluster_no].append(acc_test[i])\n",
    "            loss_test_arr[i][cluster_no].append(loss_test[i])\n",
    "        for user in cluster_block[cluster_no]:\n",
    "            acc,loss= test_acc(net_glob[cluster_no], sorted_test_data[user], args)\n",
    "            noise_acc[user].append(acc)\n",
    "            if(user==0)|(user==5)|(user==10)|(user==15)|(user==20):\n",
    "                print(acc)\n",
    "        #print(acc_test[cluster_no])\n",
    "        #print(loss_test[cluster_no])\n",
    "        #if iter % 1 ==0:\n",
    "            #print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg[cluster_no],acc_test[cluster_no]))\n",
    "        #print(hist_)\n",
    "        #print(\"users in cluster\",cluster_no)\n",
    "        #print(cluster_block[cluster_no])\n",
    "        #print(\"Test accuracy of cluster\",cluster_no)\n",
    "        #print(acc_test[cluster_no])\n",
    "    #print(loss_train)\n",
    "    sample=sample+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(idx_users[5]))\n",
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum((received)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_acc=[]\n",
    "for i in range(25):\n",
    "    user_acc.append([])\n",
    "for i in range(25):\n",
    "    for j in noise_acc[i]:\n",
    "        user_acc[i].append(float(j))\n",
    "print(user_acc[19][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Avg_acc_cluster0=[]\n",
    "Avg_acc_cluster1=[]\n",
    "Avg_acc_cluster2=[]\n",
    "Avg_acc_cluster3=[]\n",
    "Avg_acc_cluster4=[]\n",
    "for i in range(1000):\n",
    "    Avg_acc_cluster0.append((user_acc[0][i]+user_acc[1][i]+user_acc[2][i]+user_acc[3][i]+user_acc[4][i]+user_acc[5][i]+user_acc[6][i]\n",
    "                            +user_acc[7][i]+user_acc[8][i]+user_acc[9][i]\n",
    "                            +user_acc[10][i]+user_acc[11][i]+user_acc[12][i]+user_acc[13][i]+user_acc[14][i])/15)\n",
    "    Avg_acc_cluster1.append((user_acc[15][i]+user_acc[16][i]+user_acc[17][i])/3)\n",
    "    #Avg_acc_cluster1.append((user_acc[10][i]+user_acc[11][i]+user_acc[12][i]+user_acc[13][i])/4)\n",
    "    Avg_acc_cluster2.append((user_acc[18][i]+user_acc[19][i]+user_acc[20][i])/3)\n",
    "    #Avg_acc_cluster2.append((user_acc[14][i]+user_acc[15][i]+user_acc[16][i]+user_acc[17][i])/4)\n",
    "    #Avg_acc_cluster3.append((user_acc[18][i]+user_acc[19][i]+user_acc[20][i]+user_acc[21][i])/4)\n",
    "    Avg_acc_cluster3.append((user_acc[21][i]+user_acc[22][i])/2)\n",
    "    Avg_acc_cluster4.append((user_acc[23][i]+user_acc[24][i])/2)\n",
    "    #Avg_acc_cluster4.append((user_acc[22][i]+user_acc[23][i]\n",
    "                           # +user_acc[24][i])/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(Avg_acc_cluster0)), Avg_acc_cluster0)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 0 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()\n",
    "# plt.savefig('./save/fed_{}_{}_{}_C{}_iid{}.png'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))\n",
    "plt.plot(range(len(Avg_acc_cluster1)), Avg_acc_cluster1)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 1 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()\n",
    "plt.plot(range(len(Avg_acc_cluster2)), Avg_acc_cluster2)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 2 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()\n",
    "plt.plot(range(len(Avg_acc_cluster3)), Avg_acc_cluster3)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 3 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()\n",
    "plt.plot(range(len(Avg_acc_cluster4)), Avg_acc_cluster4)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 4 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acc=0\n",
    "#print(sum(noise_acc[20]))\n",
    "for i in range(25):\n",
    "    avg_acc=avg_acc+sum(noise_acc[i])/200\n",
    "#avg_acc=(sum(Avg_acc_cluster0)+sum(Avg_acc_cluster3)+sum(Avg_acc_cluster4))/(216*3)\n",
    "avg_acc=avg_acc/25\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Cluster 0 accuracy\")\n",
    "print(Avg_acc_cluster0)\n",
    "print(\"gap\")\n",
    "print(\"Cluster 1 accuracy\")\n",
    "print(Avg_acc_cluster1)\n",
    "print(\"gap\")\n",
    "print(\"Cluster 2 accuracy\")\n",
    "print(Avg_acc_cluster2)\n",
    "print(\"gap\")\n",
    "print(\"Cluster 3 accuracy\")\n",
    "print(Avg_acc_cluster3)\n",
    "print(\"gap\")\n",
    "print(\"Cluster 4 accuracy\")\n",
    "print(Avg_acc_cluster4)\n",
    "print(\"gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
