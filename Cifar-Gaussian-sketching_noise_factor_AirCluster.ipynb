{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from numpy import *\n",
    "\n",
    "from utils.sampling import mnist_iid_cluster, mnist_noniid_cluster, cifar_noniid_cluster,cifar_noniid_cluster_varying_users\n",
    "\n",
    "from models_v4.Update import LocalTrain,ClusterDetect\n",
    "from models_v4.Nets import  CNNMnist, CNNCifar, CNNMnist2\n",
    "from models_v4.Fed import FedAvg_vectorization\n",
    "\n",
    "from models_v4.test import test_acc\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "class my_argument:    \n",
    "    epochs = 10   #\"rounds of training\"\n",
    "    num_users = 5 # \"number of users: K\"\n",
    "    frac = 0.1 #\"the fraction of clients: C\"\n",
    "    local_ep=5 #\"the number of local epochs: E\"\n",
    "    local_bs=50 #\"local batch size: B\"\n",
    "    bs=128 #\"test batch size\"\n",
    "    lr=0.0001 #\"learning rate\"\n",
    "    momentum=0.5 # \"SGD momentum (default: 0.5)\"\n",
    "    split='user' # \"train-test split type, user or sample\"\n",
    "\n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    \n",
    "    # other arguments\n",
    "    dataset='cifar' #, help=\"name of dataset\")\n",
    "    iid=0\n",
    "    num_classes=10#, help=\"number of classes\")\n",
    "    num_channels=3#, help=\"number of channels of images\")\n",
    "    gpu=1#, help=\"GPU ID, -1 for CPU\")\n",
    "    stopping_rounds=10#, help='rounds of early stopping')\n",
    "    verbose='False'#, help='verbose print')\n",
    "    seed=1#, help='random seed (default: 1)')\n",
    "    cluster=5\n",
    "    opt='ADAM'\n",
    "args = my_argument()\n",
    "#args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "print(args.epochs)\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "6\n",
      "(5000, 1000)\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 5000)\n",
      "(5000, 4000)\n",
      "(5000, 4000)\n",
      "printing type of A_bar\n",
      "<class 'numpy.ndarray'>\n",
      "(5000, 1000)\n",
      "printing size of decoder1\n",
      "(1000, 5000)\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "encoder1=[]\n",
    "encoder2=[]\n",
    "decoder1=[]\n",
    "decoder2=[]\n",
    "A_bar1=[]\n",
    "A_bar2=[]\n",
    "U1=[]\n",
    "U2=[]\n",
    "mu=0\n",
    "sigma=1\n",
    "#alpha=0.001\n",
    "for i in range(args.cluster):\n",
    "    encoder1.append([])\n",
    "    decoder1.append([])\n",
    "    encoder2.append([])\n",
    "    decoder2.append([])\n",
    "    A_bar1.append([])\n",
    "    A_bar2.append([])\n",
    "    U1.append([])\n",
    "    U2.append([])\n",
    "    decoder1.append([])\n",
    "    decoder2.append([])\n",
    "part=1000; # the size of vector that we want to send each time\n",
    "total_size=62006 #total length of the vector\n",
    "d1=part\n",
    "slot=math.ceil(total_size/part) # the no. of slots we need to send vectors\n",
    "d2=total_size%part\n",
    "slot1=math.ceil(total_size/d1)-1\n",
    "slot2=total_size%d1\n",
    "print(slot)\n",
    "print(d2)\n",
    "K=args.cluster\n",
    "for i in range(args.cluster):\n",
    "    num1=np.random.normal(mu, sigma, K*d1*d1)\n",
    "    #num1=np.random.rand(K*d1*d1)\n",
    "    num2=np.random.normal(mu, sigma, K*d2*d2)\n",
    "    #num2=np.random.rand(K*d2*d2)\n",
    "    J1=np.random.rand(d1,d1)\n",
    "    for j in range(K):\n",
    "        if (j!=i):\n",
    "            encoder=np.zeros((d1,d1))\n",
    "            J1=np.vstack([J1,encoder])\n",
    "        else:\n",
    "            encoder=np.identity(d1)\n",
    "            J1=np.vstack([J1,encoder])\n",
    "    J1 = np.delete(J1,np.s_[0:d1], axis=0)\n",
    "    encoder1[i]=J1\n",
    "    #encoder2[i]=num2.reshape(K*d2,d2)\n",
    "    #divide the vector into 15 parts of size 10000 each and one part of size 9010\n",
    "#define encoder[0],encoder[1],encoder[2],encoder[3]\n",
    "K=args.cluster\n",
    "print(encoder1[1].shape)\n",
    "#N_R=5#no of receiver antenna\n",
    "for i in range(K): # K= no. of clusters\n",
    "    J1=np.random.rand(K*d1,d1)\n",
    "    #J2=np.random.rand(K*d2,d2)\n",
    "    #print(type(J2))\n",
    "    print(type(encoder1[i]))\n",
    "    for j in range(args.cluster):\n",
    "        if(j!=i):\n",
    "            J1=np.hstack([J1,encoder1[j]]) #concatenating different encoder matrices\n",
    "            #J1.append(encoder1[j])\n",
    "            #J2=np.hstack([J2,encoder2[j]])\n",
    "    print(J1.shape)\n",
    "    J1 = np.delete(J1,np.s_[0:d1], axis=1)# delete the initial d rows in J\n",
    "    print(J1.shape)\n",
    "    #J2 = np.delete(J2,np.s_[0:d2], axis=1)\n",
    "    #print(J2.shape)\n",
    "    A_bar1[i]=J1\n",
    "    #A_bar2[i]=J2\n",
    "    print(A_bar1[i].shape)\n",
    "    #print(A_bar2[i].shape)\n",
    "    print(\"printing type of A_bar\")\n",
    "    print(type(A_bar1[i]))\n",
    "    U1[i]=null_space((A_bar1[i].transpose()))\n",
    "    print(U1[i].shape)\n",
    "    #U2[i]=null_space((A_bar2[i].transpose()))\n",
    "    #print(U2[i].shape)\n",
    "    decoder1[i]=(np.linalg.inv((U1[i].transpose())@(encoder1[i])) @(U1[i].transpose()))\n",
    "    #decoder2[i]=(np.linalg.inv((U2[i].transpose())@(encoder2[i])) @(U2[i].transpose()))\n",
    "    #decoder1[i]=\n",
    "    print(\"printing size of decoder1\")\n",
    "    print(decoder1[0].shape)\n",
    "    print(type(decoder1))\n",
    "    #print(\"printing size of decoder2\")\n",
    "    #print(decoder2[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n",
      "5\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# load dataset and split users\n",
    "u=[4,5,8,12,15]\n",
    "u=[3,4,4,4,10]\n",
    "u=[10,4,4,4,3]\n",
    "#u=[15,3,3,2,2]\n",
    "#u=[5,5,5,5,5]\n",
    "#u=[2,5,5,6,7]\n",
    "#u=[2,3,4,4,12]\n",
    "u=[5,5,5,5,5]\n",
    "#u=[2,2,3,3,15]\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    count=0\n",
    "    #print(len(dataset_train))\n",
    "    dict_users=[] #2D array in each row, users of a particular cluster\n",
    "    train_data=[]\n",
    "    test_data=[]\n",
    "    for j in range(args.cluster):\n",
    "        train_data.append([])\n",
    "        test_data.append([])\n",
    "        dict_users.append([])\n",
    "    for j in range(len(dataset_train)):\n",
    "        data,label=j\n",
    "        if (label==0) | (label==1):\n",
    "            train_data[0].append(dataset_train[j])\n",
    "        elif (label==2) | (label==3):\n",
    "            train_data[1].append(dataset_train[j])\n",
    "        elif (label==4) | (label==5):\n",
    "            train_data[2].append(dataset_train[j])\n",
    "        elif (label==6) | (label==7):\n",
    "            train_data[3].append(dataset_train[j])\n",
    "        elif (label==8) | (label==9):\n",
    "            train_data[4].append(dataset_train[j])\n",
    "    for j in range(len(dataset_test)):\n",
    "        data,label=j\n",
    "        if (label==0) | (label==1):\n",
    "            test_data[0].append(dataset_test[j])\n",
    "        elif (label==2) | (label==3):\n",
    "            test_data[1].append(dataset_test[j])\n",
    "        elif (label==4) | (label==5):\n",
    "            test_data[2].append(dataset_test[j])\n",
    "        elif (label==6) | (label==7):\n",
    "            test_data[3].append(dataset_test[j])\n",
    "        elif (label==8) | (label==9):\n",
    "            test_data[4].append(dataset_test[j])\n",
    "    \n",
    "#defining 5 different types of datasets for 5 different clusters\n",
    "    \n",
    "    if args.iid:\n",
    "        for cluster_no in range(args.cluster):\n",
    "            dict_users[cluster_no] = mnist_iid_cluster(train_data[cluster_no], args.num_users)\n",
    "    else:\n",
    "        for cluster_no in range(args.cluster):\n",
    "            dict_users[cluster_no] = mnist_noniid_cluster(train_data[cluster_no], args.num_users)\n",
    "elif args.dataset == 'cifar':\n",
    "    trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    dataset_train = datasets.CIFAR10('./data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "    dataset_test = datasets.CIFAR10('./data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "#defining 5 different types of datasets for 5 different clusters\n",
    "    count=0\n",
    "    #print(len(dataset_train))\n",
    "    dict_users=[] #2D array in each row, users of a particular cluster\n",
    "    train_data=[]\n",
    "    test_data=[]\n",
    "    for j in range(args.cluster):\n",
    "        train_data.append([])\n",
    "        test_data.append([])\n",
    "        dict_users.append([])\n",
    "    for j in range(len(dataset_train)):\n",
    "        data,label=dataset_train[j]\n",
    "        if (label==0) | (label==1):\n",
    "            train_data[0].append(dataset_train[j])\n",
    "        elif (label==2) | (label==3):\n",
    "            train_data[1].append(dataset_train[j])\n",
    "        elif (label==4) | (label==5):\n",
    "            train_data[2].append(dataset_train[j])\n",
    "        elif (label==6) | (label==7):\n",
    "            train_data[3].append(dataset_train[j])\n",
    "        elif (label==8) | (label==9):\n",
    "            train_data[4].append(dataset_train[j])\n",
    "    for j in range(len(dataset_test)):\n",
    "        data,label=dataset_test[j]\n",
    "        if (label==0) | (label==1):\n",
    "            test_data[0].append(dataset_test[j])\n",
    "        elif (label==2) | (label==3):\n",
    "            test_data[1].append(dataset_test[j])\n",
    "        elif (label==4) | (label==5):\n",
    "            test_data[2].append(dataset_test[j])\n",
    "        elif (label==6) | (label==7):\n",
    "            test_data[3].append(dataset_test[j])\n",
    "        elif (label==8) | (label==9):\n",
    "            test_data[4].append(dataset_test[j])\n",
    "\n",
    "    if args.iid:\n",
    "        for cluster_no in range(args.cluster):\n",
    "            dict_users[cluster_no] = cifar_iid_cluster(train_data[cluster_no], u[cluster_no])\n",
    "    else:\n",
    "        for cluster_no in range(args.cluster):\n",
    "            dict_users[cluster_no] = cifar_noniid_cluster_varying_users(train_data[cluster_no], u[cluster_no])\n",
    "else:\n",
    "    exit('Error: dataset not found')\n",
    "img_size = dataset_train[0][0].shape\n",
    "#print(dict_users[0])\n",
    "#print((dict_users[0][4]))\n",
    "print(len(dataset_train))\n",
    "print(len(dict_users))\n",
    "print(len(train_data[0]))\n",
    "#print(train_data[0])\n",
    "#idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "#print(idxs_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3747 8142 1802 ... 4528 9021 5676]\n"
     ]
    }
   ],
   "source": [
    "print(dict_users[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "#print(use_cuda)\n",
    "args.device = torch.device(\"cuda:3\" if use_cuda else \"cpu\")\n",
    "#args.device=torch.device(\"cpu\")\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[], [], [], [], []], [[], [], [], [], []], [[], [], [], [], []], [[], [], [], [], []], [[], [], [], [], []]]\n"
     ]
    }
   ],
   "source": [
    "acc_test=[]\n",
    "acc_test_arr=[]\n",
    "loss_test=[]\n",
    "loss_test_arr=[]\n",
    "for cluster_no in range(args.cluster):\n",
    "    acc_test.append([])\n",
    "    loss_test.append([])\n",
    "    acc_test_arr.append([])\n",
    "    loss_test_arr.append([])\n",
    "for cluster_no in range(args.cluster):\n",
    "    for i in range(args.cluster):\n",
    "        acc_test_arr[cluster_no].append([])\n",
    "        loss_test_arr[cluster_no].append([])\n",
    "print(acc_test_arr)\n",
    "noise_acc=[]\n",
    "for user in range(25):\n",
    "    noise_acc.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        seed=123\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        nn.init.xavier_uniform(m.weight.data, nn.init.calculate_gain('relu'))\n",
    "        #nn.init.xavier_uniform(m.bias.data)\n",
    "        torch.nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        seed=123\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        #torch.nn.init.xavier_uniform_(m.bias.data)\n",
    "        torch.nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        seed=123\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        #torch.nn.init.xavier_uniform_(m.bias.data)\n",
    "        torch.nn.init.zeros_(m.bias.data)\n",
    "        #conv1.bias.data.fill_(0.01)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62006\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "from models_v4.Fed import weight_vectorization,FedSubstract,FedAvg_gradient,FedAdd\n",
    "from models_v4.Fed import FedAdd,FedSubstract,weight_vectorization_gen,FedAvg_gradient\n",
    "import numpy as np\n",
    "import copy\n",
    "if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "    net_glob=[]\n",
    "    for i in range(args.cluster):\n",
    "        net_glob.append(CNNCifar(args=args).to(args.device))\n",
    "elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "    net_glob=[]\n",
    "    for i in range(args.cluster):\n",
    "        net_glob.append(CNNMnist2(args=args).to(args.device))\n",
    "\n",
    "else:\n",
    "    exit('Error:model not found')\n",
    "#print(net_glob)\n",
    "w_glob=[]\n",
    "# copy weights\n",
    "for i in range(args.cluster):\n",
    "    net_glob[i].train()\n",
    "    w_glob.append(net_glob[i].state_dict())\n",
    "abs_vect,layer_size=weight_vectorization_gen(w_glob[2])\n",
    "net_glob_in=copy.deepcopy(net_glob)\n",
    "w_glob_in=copy.deepcopy(w_glob)\n",
    "print(len(abs_vect))\n",
    "#print(\"weights\")\n",
    "#print(w_glob)\n",
    "#print(w_glob.shape)\n",
    "#print(w_glob[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_glob_in[0]=torch.load(\"net1_2.pt\")\n",
    "w_glob_in[1]=torch.load(\"net2_2.pt\")\n",
    "w_glob_in[2]=torch.load(\"net3_2.pt\")\n",
    "w_glob_in[3]=torch.load(\"net4_2.pt\")\n",
    "w_glob_in[4]=torch.load(\"net5_2.pt\")\n",
    "net_glob_in[0].load_state_dict(w_glob_in[0])\n",
    "net_glob_in[1].load_state_dict(w_glob_in[1])\n",
    "net_glob_in[2].load_state_dict(w_glob_in[2])\n",
    "net_glob_in[3].load_state_dict(w_glob_in[3])\n",
    "net_glob_in[4].load_state_dict(w_glob_in[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4, 14]\n",
      "[10, 11, 12, 13]\n",
      "[20, 21, 22, 23, 24]\n",
      "11638.141627123827\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "tensor(51.4500)\n",
      "iteration number 1\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "13834.795976226453\n",
      "tensor(50.)\n",
      "tensor(50.)\n",
      "tensor(52.2500)\n",
      "tensor(47.6000)\n",
      "tensor(50.)\n",
      "iteration number 2\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "13966.005267244767\n",
      "tensor(44.6000)\n",
      "tensor(50.)\n",
      "tensor(66.8500)\n",
      "tensor(49.1000)\n",
      "tensor(50.)\n",
      "iteration number 3\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "12685.070291901979\n",
      "tensor(48.0500)\n",
      "tensor(50.)\n",
      "tensor(58.1500)\n",
      "tensor(50.)\n",
      "tensor(53.8500)\n",
      "iteration number 4\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "8124.527990111592\n",
      "tensor(50.)\n",
      "tensor(61.8000)\n",
      "tensor(70.2000)\n",
      "tensor(56.1000)\n",
      "tensor(54.3000)\n",
      "iteration number 5\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "5480.822821685537\n",
      "tensor(50.)\n",
      "tensor(63.3500)\n",
      "tensor(77.6500)\n",
      "tensor(58.9000)\n",
      "tensor(69.6500)\n",
      "iteration number 6\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "4806.93799270556\n",
      "tensor(50.)\n",
      "tensor(66.1000)\n",
      "tensor(75.)\n",
      "tensor(63.6000)\n",
      "tensor(60.1500)\n",
      "iteration number 7\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "4432.06369621749\n",
      "tensor(55.5000)\n",
      "tensor(69.7000)\n",
      "tensor(75.7500)\n",
      "tensor(64.4500)\n",
      "tensor(58.8000)\n",
      "iteration number 8\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "4137.965182803209\n",
      "tensor(59.1000)\n",
      "tensor(71.1000)\n",
      "tensor(70.6500)\n",
      "tensor(50.)\n",
      "tensor(64.9500)\n",
      "iteration number 9\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "3835.103712127552\n",
      "tensor(57.3000)\n",
      "tensor(71.6500)\n",
      "tensor(75.9000)\n",
      "tensor(61.6500)\n",
      "tensor(69.4500)\n",
      "iteration number 10\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "3687.1893178839655\n",
      "tensor(54.9000)\n",
      "tensor(71.2500)\n",
      "tensor(79.6500)\n",
      "tensor(58.9000)\n",
      "tensor(67.6000)\n",
      "iteration number 11\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "3484.9622969809006\n",
      "tensor(59.7500)\n",
      "tensor(63.5500)\n",
      "tensor(78.3000)\n",
      "tensor(64.8500)\n",
      "tensor(66.1000)\n",
      "iteration number 12\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "3613.1411730765203\n",
      "tensor(60.7500)\n",
      "tensor(51.4500)\n",
      "tensor(78.5500)\n",
      "tensor(67.4500)\n",
      "tensor(73.1000)\n",
      "iteration number 13\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "3543.870181636491\n",
      "tensor(60.5000)\n",
      "tensor(76.3500)\n",
      "tensor(77.3000)\n",
      "tensor(66.4000)\n",
      "tensor(74.9500)\n",
      "iteration number 14\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "3427.1841126600966\n",
      "tensor(60.6500)\n",
      "tensor(75.8000)\n",
      "tensor(60.5000)\n",
      "tensor(63.1500)\n",
      "tensor(68.9000)\n",
      "iteration number 15\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "3060.8638079354932\n",
      "tensor(58.2500)\n",
      "tensor(75.7500)\n",
      "tensor(70.5500)\n",
      "tensor(65.6000)\n",
      "tensor(75.3500)\n",
      "iteration number 16\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "3280.279225536526\n",
      "tensor(59.9000)\n",
      "tensor(74.9500)\n",
      "tensor(80.9500)\n",
      "tensor(67.5000)\n",
      "tensor(74.5500)\n",
      "iteration number 17\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2979.127374644977\n",
      "tensor(61.5000)\n",
      "tensor(78.8000)\n",
      "tensor(82.3000)\n",
      "tensor(67.9000)\n",
      "tensor(70.4000)\n",
      "iteration number 18\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2991.8239252148187\n",
      "tensor(63.5500)\n",
      "tensor(78.0500)\n",
      "tensor(80.)\n",
      "tensor(61.3500)\n",
      "tensor(69.2000)\n",
      "iteration number 19\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2907.2881590487614\n",
      "tensor(63.0500)\n",
      "tensor(79.9000)\n",
      "tensor(82.4500)\n",
      "tensor(67.4500)\n",
      "tensor(74.4000)\n",
      "iteration number 20\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2731.489282815909\n",
      "tensor(64.6000)\n",
      "tensor(72.9500)\n",
      "tensor(83.3000)\n",
      "tensor(68.6500)\n",
      "tensor(75.)\n",
      "iteration number 21\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2680.1811808958073\n",
      "tensor(63.0500)\n",
      "tensor(69.5500)\n",
      "tensor(80.2500)\n",
      "tensor(54.4000)\n",
      "tensor(75.3000)\n",
      "iteration number 22\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2598.1706368131854\n",
      "tensor(57.2500)\n",
      "tensor(80.8500)\n",
      "tensor(82.1500)\n",
      "tensor(69.6500)\n",
      "tensor(76.)\n",
      "iteration number 23\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2708.0775463468663\n",
      "tensor(66.)\n",
      "tensor(81.7000)\n",
      "tensor(79.)\n",
      "tensor(69.4500)\n",
      "tensor(70.1500)\n",
      "iteration number 24\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2627.7009109952296\n",
      "tensor(68.5000)\n",
      "tensor(80.7500)\n",
      "tensor(83.1000)\n",
      "tensor(71.1500)\n",
      "tensor(73.6000)\n",
      "iteration number 25\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2591.66241285449\n",
      "tensor(67.)\n",
      "tensor(76.6500)\n",
      "tensor(83.0500)\n",
      "tensor(70.2000)\n",
      "tensor(76.3500)\n",
      "iteration number 26\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2353.3214887136587\n",
      "tensor(60.2500)\n",
      "tensor(78.6000)\n",
      "tensor(82.4000)\n",
      "tensor(66.7500)\n",
      "tensor(75.1500)\n",
      "iteration number 27\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2326.3443330442155\n",
      "tensor(69.9500)\n",
      "tensor(80.2000)\n",
      "tensor(81.9000)\n",
      "tensor(64.2000)\n",
      "tensor(76.8500)\n",
      "iteration number 28\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2187.8655777520335\n",
      "tensor(65.3500)\n",
      "tensor(82.9000)\n",
      "tensor(82.8500)\n",
      "tensor(68.5000)\n",
      "tensor(75.5500)\n",
      "iteration number 29\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2307.3463236566013\n",
      "tensor(55.)\n",
      "tensor(83.9000)\n",
      "tensor(80.9500)\n",
      "tensor(58.)\n",
      "tensor(77.0500)\n",
      "iteration number 30\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2379.0907019969\n",
      "tensor(69.5500)\n",
      "tensor(83.2500)\n",
      "tensor(81.7500)\n",
      "tensor(69.1000)\n",
      "tensor(73.4000)\n",
      "iteration number 31\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2163.3526240422334\n",
      "tensor(63.7000)\n",
      "tensor(83.2500)\n",
      "tensor(81.4000)\n",
      "tensor(71.8500)\n",
      "tensor(77.2500)\n",
      "iteration number 32\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2298.2147429804427\n",
      "tensor(68.9500)\n",
      "tensor(84.3000)\n",
      "tensor(83.0500)\n",
      "tensor(71.4000)\n",
      "tensor(72.3500)\n",
      "iteration number 33\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2082.148047029419\n",
      "tensor(68.0500)\n",
      "tensor(74.3000)\n",
      "tensor(82.8500)\n",
      "tensor(70.2000)\n",
      "tensor(77.6000)\n",
      "iteration number 34\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "2117.0924113796996\n",
      "tensor(67.8000)\n",
      "tensor(80.8000)\n",
      "tensor(83.0500)\n",
      "tensor(70.4500)\n",
      "tensor(72.6000)\n",
      "iteration number 35\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1937.2942604213604\n",
      "tensor(58.9500)\n",
      "tensor(84.2000)\n",
      "tensor(82.5000)\n",
      "tensor(72.8500)\n",
      "tensor(73.6000)\n",
      "iteration number 36\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1939.1510192291064\n",
      "tensor(69.1000)\n",
      "tensor(83.1000)\n",
      "tensor(81.4500)\n",
      "tensor(66.)\n",
      "tensor(76.4500)\n",
      "iteration number 37\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1893.2647090711062\n",
      "tensor(71.2500)\n",
      "tensor(78.)\n",
      "tensor(83.)\n",
      "tensor(69.3500)\n",
      "tensor(73.3000)\n",
      "iteration number 38\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1890.4073466371296\n",
      "tensor(70.4000)\n",
      "tensor(79.8500)\n",
      "tensor(81.7500)\n",
      "tensor(75.3500)\n",
      "tensor(77.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 39\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1797.2576138788659\n",
      "tensor(71.4500)\n",
      "tensor(84.9000)\n",
      "tensor(83.5500)\n",
      "tensor(74.4000)\n",
      "tensor(76.9500)\n",
      "iteration number 40\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1817.9812945496003\n",
      "tensor(61.1000)\n",
      "tensor(86.1500)\n",
      "tensor(81.7000)\n",
      "tensor(75.9500)\n",
      "tensor(76.0500)\n",
      "iteration number 41\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1663.0386185670527\n",
      "tensor(71.6000)\n",
      "tensor(78.8500)\n",
      "tensor(80.7000)\n",
      "tensor(72.5000)\n",
      "tensor(73.9500)\n",
      "iteration number 42\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1765.1641323979275\n",
      "tensor(69.4500)\n",
      "tensor(85.4000)\n",
      "tensor(78.9000)\n",
      "tensor(71.0500)\n",
      "tensor(77.6000)\n",
      "iteration number 43\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1627.9673525185221\n",
      "tensor(68.8000)\n",
      "tensor(73.4000)\n",
      "tensor(82.2500)\n",
      "tensor(72.5500)\n",
      "tensor(69.1000)\n",
      "iteration number 44\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1826.198396413211\n",
      "tensor(71.8000)\n",
      "tensor(81.2000)\n",
      "tensor(83.9500)\n",
      "tensor(69.7500)\n",
      "tensor(78.3000)\n",
      "iteration number 45\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1623.4384151638642\n",
      "tensor(70.3000)\n",
      "tensor(85.4500)\n",
      "tensor(84.4000)\n",
      "tensor(74.3500)\n",
      "tensor(76.5500)\n",
      "iteration number 46\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1515.6901224400535\n",
      "tensor(70.4500)\n",
      "tensor(83.7000)\n",
      "tensor(80.4500)\n",
      "tensor(72.1000)\n",
      "tensor(74.9000)\n",
      "iteration number 47\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1591.4262995066938\n",
      "tensor(72.)\n",
      "tensor(86.8500)\n",
      "tensor(80.)\n",
      "tensor(68.2500)\n",
      "tensor(78.8500)\n",
      "iteration number 48\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1520.7201420175772\n",
      "tensor(66.2000)\n",
      "tensor(87.2500)\n",
      "tensor(79.4500)\n",
      "tensor(72.4500)\n",
      "tensor(74.8000)\n",
      "iteration number 49\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1559.1031736037594\n",
      "tensor(71.8000)\n",
      "tensor(80.8500)\n",
      "tensor(82.5000)\n",
      "tensor(70.5000)\n",
      "tensor(68.7000)\n",
      "iteration number 50\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1539.2638083656032\n",
      "tensor(72.7500)\n",
      "tensor(86.)\n",
      "tensor(83.4500)\n",
      "tensor(72.4000)\n",
      "tensor(69.8000)\n",
      "iteration number 51\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1624.5170552915456\n",
      "tensor(71.8000)\n",
      "tensor(85.6500)\n",
      "tensor(81.7500)\n",
      "tensor(75.8500)\n",
      "tensor(79.2000)\n",
      "iteration number 52\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1535.60296250415\n",
      "tensor(62.4000)\n",
      "tensor(86.2500)\n",
      "tensor(79.6500)\n",
      "tensor(68.9500)\n",
      "tensor(79.3000)\n",
      "iteration number 53\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1574.6000077540552\n",
      "tensor(64.6500)\n",
      "tensor(86.)\n",
      "tensor(82.2000)\n",
      "tensor(75.7500)\n",
      "tensor(77.6000)\n",
      "iteration number 54\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1566.5912648790004\n",
      "tensor(70.1500)\n",
      "tensor(85.8500)\n",
      "tensor(84.7500)\n",
      "tensor(72.5500)\n",
      "tensor(78.9500)\n",
      "iteration number 55\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1474.5720278873441\n",
      "tensor(71.4000)\n",
      "tensor(79.9000)\n",
      "tensor(83.1000)\n",
      "tensor(74.1000)\n",
      "tensor(78.2500)\n",
      "iteration number 56\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1560.6142474819444\n",
      "tensor(68.4000)\n",
      "tensor(80.7500)\n",
      "tensor(85.2000)\n",
      "tensor(67.1500)\n",
      "tensor(78.2000)\n",
      "iteration number 57\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1539.7873625328723\n",
      "tensor(65.8000)\n",
      "tensor(85.2000)\n",
      "tensor(84.2000)\n",
      "tensor(72.4500)\n",
      "tensor(75.5000)\n",
      "iteration number 58\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1525.7036965355826\n",
      "tensor(68.4500)\n",
      "tensor(86.3000)\n",
      "tensor(84.6000)\n",
      "tensor(75.2500)\n",
      "tensor(78.6000)\n",
      "iteration number 59\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1442.2461090578608\n",
      "tensor(72.3000)\n",
      "tensor(84.9500)\n",
      "tensor(81.6000)\n",
      "tensor(74.9500)\n",
      "tensor(75.2000)\n",
      "iteration number 60\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1502.4712049085604\n",
      "tensor(72.6500)\n",
      "tensor(84.4000)\n",
      "tensor(85.6500)\n",
      "tensor(76.6500)\n",
      "tensor(79.7000)\n",
      "iteration number 61\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n",
      "1441.094579109137\n",
      "tensor(74.8000)\n",
      "tensor(83.4500)\n",
      "tensor(83.7500)\n",
      "tensor(75.7000)\n",
      "tensor(79.8000)\n",
      "iteration number 62\n",
      "[5, 6, 7, 8, 9]\n",
      "[15, 16, 17, 18, 19]\n",
      "[0, 1, 2, 3, 4]\n",
      "[10, 11, 12, 13, 14]\n",
      "[20, 21, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "d=62006\n",
    "from models_v4.Fed import weight_vectorization_cifar,FedAdd,FedSubstract,FedAvg_gradient\n",
    "import numpy as np\n",
    "import copy\n",
    "slot=math.ceil(d/d1) # the no. of slots we need to send vectors\n",
    "w_glob=copy.deepcopy(w_glob_in)\n",
    "net_glob=copy.deepcopy(net_glob_in)\n",
    "#u=[5,8,10,12,15]\n",
    "#u=range(0,50)\n",
    "idx_users=range(0,25)\n",
    "# training\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "num=[]\n",
    "import random\n",
    "D=part #dimension of noise vector = Nr= Kd\n",
    "mu=0\n",
    "sigma=1\n",
    "K=args.cluster\n",
    "\n",
    "#hist_ = np.zeros(10,dtype=int)\n",
    "sample=0 # fro the purpose of using fresh samples in each iteration\n",
    "args.lr=0.0001\n",
    "for iter in range(500): #args.epochs\n",
    "    print(\"iteration number\",iter)\n",
    "    if(iter%4==0):\n",
    "        sample=0\n",
    "    encoded=[]\n",
    "    decoded=[]\n",
    "    num=[]\n",
    "    noise=[] #noise vector\n",
    "  \n",
    "    num=np.random.normal(mu, sigma, K*D)\n",
    "    num=np.transpose(num.reshape(1,len(num)))\n",
    "    #m = 10\n",
    "    loss_train=[]\n",
    "    cluster_block=[]\n",
    "    noise_dec=[] #decoded noise vector\n",
    "    for cluster_no in range(args.cluster):\n",
    "        loss_train.append([])\n",
    "        cluster_block.append([])\n",
    "        decoded.append([])\n",
    "        noise_dec.append([])\n",
    "    Power=[]\n",
    "    Power2=[]\n",
    "    alpha=[]\n",
    "    feature_vector2=[]\n",
    "    for i in range(args.cluster):\n",
    "        Power.append([])\n",
    "        Power2.append([])\n",
    "        alpha.append([])\n",
    "        feature_vector2.append(np.zeros((d,1)))\n",
    "                \n",
    "    #idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    idx_users=[]\n",
    "    sorted_train_data=[]\n",
    "    sorted_test_data=[]\n",
    "    for cluster_no in range(args.cluster):\n",
    "        for index in dict_users[cluster_no]:\n",
    "            idx_users.append(index) # putting the data indices of users in this list\n",
    "            sorted_train_data.append(train_data[cluster_no])#putting the corresponding training data in this array\n",
    "            sorted_test_data.append(test_data[cluster_no])\n",
    "            \n",
    "    for user in range(len(idx_users)): # no. of loop= no. of users\n",
    "        cluster_loss=[]\n",
    "        L=len(idx_users[user])\n",
    "        #L=1000\n",
    "        #L=600\n",
    "        sample_size=int(L/8)\n",
    "        #sample_size=int(L/2)\n",
    "        #sample_size=250\n",
    "        for i in range(args.cluster):\n",
    "             # each type of dataset belong to 5 users\n",
    "            local2 = ClusterDetect(args=args, dataset= sorted_train_data[user], idxs=idx_users[user][(sample*sample_size):(sample+1)*sample_size])\n",
    "            #local2 = ClusterDetect(args=args, dataset= sorted_train_data[user], idxs=idx_users[user][0:599])\n",
    "            # using 1st 600 data\n",
    "            w2, loss2 = local2.train2(net=copy.deepcopy(net_glob[i]).to(args.device))\n",
    "            cluster_loss.append(loss2)\n",
    "        #print(\"printing user\")\n",
    "        #print(user)\n",
    "        #print(cluster_loss)\n",
    "        minimum=min(cluster_loss)\n",
    "        index_of_min=cluster_loss.index(minimum)\n",
    "        cluster_block[index_of_min].append(user)\n",
    "    print(cluster_block[0])\n",
    "    print(cluster_block[1])\n",
    "    print(cluster_block[2])\n",
    "    print(cluster_block[3])\n",
    "    print(cluster_block[4])\n",
    "    K_global=part\n",
    "    m_global=np.zeros((d,1))\n",
    "    location_global=np.random.choice(range(d),K_global,replace=False)\n",
    "    for i in location_global:\n",
    "        m_global[i]=1\n",
    "    updated=[]\n",
    "    model_diff=[]\n",
    "    grad_vect=[]\n",
    "    prev=[]\n",
    "    error=[]\n",
    "    grad_vect_send=[]\n",
    "    store_grad=[]\n",
    "    for i in range(50):\n",
    "        updated.append([])\n",
    "        model_diff.append([])\n",
    "        grad_vect.append([])\n",
    "        prev.append([])\n",
    "        error.append(np.zeros((d,1)))\n",
    "        grad_vect_send.append([])\n",
    "        store_grad.append([])\n",
    "    b=part  \n",
    "    sigma_R=1/math.sqrt(b)\n",
    "    mu_R=0\n",
    "    R=np.random.normal(mu_R, sigma_R, (b,total_size))\n",
    "    superposition=0\n",
    "    for cluster_no in range(args.cluster):\n",
    "        w_locals, loss_locals,grad_locals,diff_locals= [],[],[],[]\n",
    "        if(cluster_block[cluster_no]==[]):\n",
    "            continue\n",
    "        for user2 in cluster_block[cluster_no]:\n",
    "            trans=np.array([])\n",
    "            #local = LocalUpdate(args=args, dataset=sorted_train_data[user], idxs=idx_users[user][(sample+1)*600:(sample+2)*600])\n",
    "            #local = LocalUpdate(args=args, dataset=sorted_train_data[user], idxs=idx_users[user][600:1199])\n",
    "            #print(user2)\n",
    "            total_P=0\n",
    "            updated[user2]=copy.deepcopy(w_glob[cluster_no])\n",
    "            L=len(idx_users[user2])\n",
    "#             L=1000\n",
    "#             L=600\n",
    "            sample_size=int(L/8)\n",
    "            #sample_size=250\n",
    "            local = LocalTrain(args=args, dataset=sorted_train_data[user2], idxs=idx_users[user2][(sample+1)*sample_size:(sample+2)*sample_size])\n",
    "            #using 2nd half data\n",
    "            w, loss = local.train(net=copy.deepcopy(net_glob[cluster_no]).to(args.device))\n",
    "            w_locals.append(copy.deepcopy(w))\n",
    "            loss_locals.append(copy.deepcopy(loss))\n",
    "            prev[user2]=updated[user2]\n",
    "            model_diff[user2]=FedSubstract(w,prev[user2])\n",
    "            grad_vect[user2],layer_size=weight_vectorization_gen(model_diff[user2]) # vectorizing the gradient\n",
    "            #grad_vect[user2]=grad_vect[user2]+error[user2] # error feedback\n",
    "            array_one=np.ones((d,1))\n",
    "            model_vector,layer_size=weight_vectorization_gen(w)\n",
    "            M=max(abs(model_vector))\n",
    "            mask=m_global\n",
    "            count=0\n",
    "            #calculating the modified gradient to be sent to server\n",
    "            #grad_vect_send[user2]=np.multiply(mask,grad_vect[user2])\n",
    "            grad_vect_send[user2]= R@grad_vect[user2]\n",
    "            #error[user2]=grad_vect[user2]-grad_vect_send[user2]\n",
    "            grad_locals.append(grad_vect_send[user2])\n",
    "            store_grad[user2]=grad_vect_send[user2]\n",
    "            diff_locals.append(copy.deepcopy(model_diff[user2]))\n",
    "            #print(M)\n",
    "            SNR=math.log10(M**2)\n",
    "            #print(SNR)\n",
    "            #n=K*K*d1*d1\n",
    "            #H = np.random.normal(loc=0, scale=np.sqrt(2)/2, size=(n, 2)).view(np.complex128)\n",
    "            #H=H.reshape(K*d1,K*d1)\n",
    "            #U, D, VT = np.linalg.svd(H)\n",
    "            #HT=np.matrix(H).getH()\n",
    "            #left_inv=np.linalg.inv(HT@H)@HT\n",
    "            #E1= left_inv@(encoder1[cluster_no])\n",
    "            #for i in range(slot1):\n",
    "                #z=(abs(np.array(E1 @ model_vector[i*d1:(i+1)*d1])))**2\n",
    "                #trans=np.concatenate((trans,z[:,0]))\n",
    "                #total_P=total_P+sum(z)\n",
    "            d2=total_size%d1\n",
    "            n=K*K*d2*d2\n",
    "            #H = np.random.normal(loc=0, scale=np.sqrt(2)/2, size=(n, 2)).view(np.complex128)\n",
    "            #H=H.reshape(K*d2,K*d2)\n",
    "            #U, D, VT = np.linalg.svd(H)\n",
    "            #HT=np.matrix(H).getH()\n",
    "            #left_inv=np.linalg.inv(HT@H)@HT\n",
    "#print(left_inv.shape)\n",
    "            #E2= left_inv@(encoder2[cluster_no])\n",
    "            #z=(abs(np.array(E2 @ model_vector[slot1*d1:slot1*d1+d2])))**2\n",
    "            #trans=np.concatenate((trans,z[:,0]))\n",
    "            #M=max(abs(trans))\n",
    "            #print(M)\n",
    "            #total_P=total_P + sum(z)\n",
    "            #print(total_P)\n",
    "            Power[cluster_no].append(total_P)\n",
    "        Power2[cluster_no]=(sum(Power[cluster_no]))/len(cluster_block[cluster_no])\n",
    "        P_limit=2000000000\n",
    "        #alpha[cluster_no]=math.sqrt(P_limit/Power2[cluster_no])\n",
    "        alpha[cluster_no]=1*100\n",
    "        grad_avg=FedAvg_gradient(grad_locals)\n",
    "        loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "        loss_train[cluster_no].append(loss_avg)\n",
    "        #print(len(w_locals))\n",
    "        j=w_locals[0]\n",
    "        feature_vector=grad_avg\n",
    "        #print(len(feature_vector))\n",
    "        #print(feature_vector)\n",
    "        #fecture vector is dx1 vector after flattening all types of weight matrices and concatenaitng them\n",
    "        increment=0 #keeps track of feature_vactor index\n",
    "        superposition=superposition+alpha[cluster_no]*encoder1[cluster_no]@feature_vector\n",
    "    received=superposition\n",
    "    superposition=superposition+num\n",
    "    #print(len(superposition))\n",
    "    #superposition=superposition+num\n",
    "    #print(\"total received Power\")\n",
    "    print(sum((received)**2))\n",
    "    #print(\"received min max\")\n",
    "    #print(max(abs(received)))\n",
    "    #print(min(abs(received)))\n",
    "    m_global=np.zeros((d,1))\n",
    "        \n",
    "    for cluster_no in range(args.cluster):\n",
    "        #print(w_glob[cluster_no])\n",
    "        if(cluster_block[cluster_no]==[]):\n",
    "            continue\n",
    "        flat=[]\n",
    "        for i in range(16): # 4 layers in parameter\n",
    "            flat.append([])\n",
    "        increment=0\n",
    "        decoded[cluster_no]=(1/alpha[cluster_no])*decoder1[cluster_no]@superposition\n",
    "        count=0\n",
    "    \n",
    "        decoded[cluster_no]=R.transpose()@decoded[cluster_no]\n",
    "        count=0\n",
    "        w_glob_prev=copy.deepcopy(w_glob[cluster_no])\n",
    "        for i in range(len(w_glob[cluster_no].keys())): # 4 layers in parameter\n",
    "            flat.append([])\n",
    "\n",
    "        for h in w_glob_prev.keys():\n",
    "            s=list(w_glob[cluster_no][h].shape)\n",
    "            if (len(s)==0):\n",
    "                new=np.array(0)\n",
    "                decoded[cluster_no]=np.delete(decoded[cluster_no],np.s_[0])\n",
    "            else:\n",
    "                z=np.prod(list(w_glob[cluster_no][h].shape))\n",
    "                flat[count]=decoded[cluster_no][0:z] # taking out the vector for the specified layer\n",
    "                decoded[cluster_no]=np.delete(decoded[cluster_no],np.s_[0:z])# deleting that vector from decoded after taking out\n",
    "             \n",
    "                new=flat[count].reshape(list(w_glob[cluster_no][h].shape)) #reshaping back to the marix\n",
    "              \n",
    "            w_glob[cluster_no][h]=torch.from_numpy(new) #converting the matrix to a tensor\n",
    "            #print(w_glob[cluster_no][h].shape)\n",
    "            count=count+1\n",
    "    # update global weights\n",
    "        \n",
    "        global_diff = w_glob[cluster_no]\n",
    "        w_glob[cluster_no]=FedAdd(w_glob_prev,global_diff)\n",
    "        net_glob[cluster_no].load_state_dict(w_glob[cluster_no])\n",
    "        #printing loss in each iteration\n",
    "        for i in range(args.cluster):\n",
    "            acc_test[i], loss_test[i] = test_acc(net_glob[cluster_no], test_data[i], args)\n",
    "            acc_test_arr[i][cluster_no].append(acc_test[i])\n",
    "            loss_test_arr[i][cluster_no].append(loss_test[i])\n",
    "        for user in cluster_block[cluster_no]:\n",
    "            acc,loss= test_acc(net_glob[cluster_no], sorted_test_data[user], args)\n",
    "            noise_acc[user].append(acc)\n",
    "            if(user==0)|(user==5)|(user==10)|(user==15)|(user==20):\n",
    "                print(acc)\n",
    "        #print(acc_test[cluster_no])\n",
    "        #print(loss_test[cluster_no])\n",
    "        #if iter % 1 ==0:\n",
    "            #print('Round {:3d}, Average loss {:.3f} Test accuracy {:.3f}'.format(iter, loss_avg[cluster_no],acc_test[cluster_no]))\n",
    "        #print(hist_)\n",
    "        #print(\"users in cluster\",cluster_no)\n",
    "        #print(cluster_block[cluster_no])\n",
    "        #print(\"Test accuracy of cluster\",cluster_no)\n",
    "        #print(acc_test[cluster_no])\n",
    "    #print(loss_train)\n",
    "    sample=sample+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(idx_users[1]))\n",
    "print(user2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum((received)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_acc=[]\n",
    "for i in range(25):\n",
    "    user_acc.append([])\n",
    "for i in range(25):\n",
    "    for j in noise_acc[i]:\n",
    "        user_acc[i].append(float(j))\n",
    "print(user_acc[19][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Avg_acc_cluster0=[]\n",
    "Avg_acc_cluster1=[]\n",
    "Avg_acc_cluster2=[]\n",
    "Avg_acc_cluster3=[]\n",
    "Avg_acc_cluster4=[]\n",
    "for i in range(200):\n",
    "    Avg_acc_cluster0.append((user_acc[0][i]+user_acc[1][i]+user_acc[2][i]+user_acc[3][i]+user_acc[4][i])/5)\n",
    "                            #+user_acc[10][i]+user_acc[11][i]+user_acc[12][i]+user_acc[13][i]+user_acc[14][i])/15)\n",
    "    #Avg_acc_cluster1.append((user_acc[15][i]+user_acc[16][i]+user_acc[17][i])/3)\n",
    "    Avg_acc_cluster1.append((user_acc[5][i]+user_acc[6][i]\n",
    "                            +user_acc[7][i]+user_acc[8][i]+user_acc[9][i])/5)\n",
    "    Avg_acc_cluster2.append((user_acc[10][i]+user_acc[11][i]+user_acc[12][i]+user_acc[13][i]+user_acc[14][i])/5)\n",
    "    #Avg_acc_cluster2.append((user_acc[18][i]+user_acc[19][i]+user_acc[20][i])/3)\n",
    "    Avg_acc_cluster3.append((user_acc[15][i]+user_acc[16][i]+user_acc[17][i]+user_acc[18][i]+user_acc[19][i])/5)\n",
    "    #Avg_acc_cluster3.append((user_acc[18][i]+user_acc[19][i]+user_acc[20][i]+user_acc[21][i])/4)\n",
    "    #Avg_acc_cluster3.append((user_acc[21][i]+user_acc[22][i])/2)\n",
    "    #Avg_acc_cluster4.append((user_acc[23][i]+user_acc[24][i])/2)\n",
    "    Avg_acc_cluster4.append((user_acc[20][i]+user_acc[21][i]+user_acc[22][i]+user_acc[23][i]\n",
    "                            +user_acc[24][i])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(Avg_acc_cluster0)), Avg_acc_cluster0)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 0 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()\n",
    "# plt.savefig('./save/fed_{}_{}_{}_C{}_iid{}.png'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))\n",
    "plt.plot(range(len(Avg_acc_cluster1)), Avg_acc_cluster1)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 1 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()\n",
    "plt.plot(range(len(Avg_acc_cluster2)), Avg_acc_cluster2)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 2 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()\n",
    "plt.plot(range(len(Avg_acc_cluster3)), Avg_acc_cluster3)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 3 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()\n",
    "plt.plot(range(len(Avg_acc_cluster4)), Avg_acc_cluster4)\n",
    "#plt.ylabel('cluster 0 accuracy(%)')\n",
    "plt.ylabel('cluster 4 test accuracy')\n",
    "plt.xlabel('iteration no.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acc=0\n",
    "#print(sum(noise_acc[20]))\n",
    "for i in range(25):\n",
    "    avg_acc=avg_acc+sum(noise_acc[i])/200\n",
    "#avg_acc=(sum(Avg_acc_cluster0)+sum(Avg_acc_cluster3)+sum(Avg_acc_cluster4))/(216*3)\n",
    "avg_acc=avg_acc/25\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Cluster 0 accuracy\")\n",
    "print(Avg_acc_cluster0)\n",
    "print(\"gap\")\n",
    "print(\"Cluster 1 accuracy\")\n",
    "print(Avg_acc_cluster1)\n",
    "print(\"gap\")\n",
    "print(\"Cluster 2 accuracy\")\n",
    "print(Avg_acc_cluster2)\n",
    "print(\"gap\")\n",
    "print(\"Cluster 3 accuracy\")\n",
    "print(Avg_acc_cluster3)\n",
    "print(\"gap\")\n",
    "print(\"Cluster 4 accuracy\")\n",
    "print(Avg_acc_cluster4)\n",
    "print(\"gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
